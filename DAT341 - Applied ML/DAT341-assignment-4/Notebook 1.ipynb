{"cells":[{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"3258d9d60db64cf08866f8b741078157","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# Assignment 5  ","block_group":"3258d9d60db64cf08866f8b741078157"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"91800876b74a4a54a2b193f0b9a4a226","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"The names of the people in the group: Jonas RÃ¶st & Kailash de Jesus Hornig \nYour answer to the exercise question.\nThe accuracies you get for the SVC and LR classifiers, or any other classifiers you've implemented.\nAny information needed to run the code.\nAny clarification of steps in your code that could be hard to understand for someone who didn't write that code.\nAny other topic you'd like to discuss.","block_group":"91800876b74a4a54a2b193f0b9a4a226"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"36d0c912b91b4fca83b63bf93378eaab","source_hash":"ef861ecf","execution_start":1676913990616,"execution_millis":19,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#experimenting \nimport pandas as pd\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import make_pipeline\n\nX1 = [{'city':'Gothenburg', 'month':'July'},\n      {'city':'Gothenburg', 'month':'December'},\n      {'city':'Paris', 'month':'July'},\n      {'city':'Paris', 'month':'December'}]\nY1 = ['rain', 'rain', 'sun', 'rain']\n\nX2 = [{'city':'Sydney', 'month':'July'},\n      {'city':'Sydney', 'month':'December'},\n      {'city':'Paris', 'month':'July'},\n      {'city':'Paris', 'month':'December'}]\nY2 = ['rain', 'sun', 'sun', 'rain']\n\nclassifier1 = make_pipeline(DictVectorizer(), Perceptron(max_iter=10))\nclassifier1.fit(X1, Y1)\nguesses1 = classifier1.predict(X1)\nprint(accuracy_score(Y1, guesses1))\n\nclassifier2 = make_pipeline(DictVectorizer(), Perceptron(max_iter=10))\n#classifier2 = make_pipeline(DictVectorizer(), LinearSVC())\nclassifier2.fit(X2, Y2)\nguesses2 = classifier2.predict(X2)\nprint(accuracy_score(Y2, guesses2))\n","block_group":"36d0c912b91b4fca83b63bf93378eaab","execution_count":6,"outputs":[{"name":"stdout","text":"1.0\n0.5\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"5ab04235cb644f63b7365d05a4e76e91","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"The theory is that the data is not linearly separable for the second data set ","block_group":"5ab04235cb644f63b7365d05a4e76e91"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"d2d6537243614c0aaac7ba00b103040f","source_hash":"26646042","execution_start":1676914074596,"execution_millis":17,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"vect = DictVectorizer()\nX = vect.fit_transform(X1)\n\ndocs_df = pd.DataFrame(data = X.A, columns=vect.get_feature_names())\ndocs_df","block_group":"d2d6537243614c0aaac7ba00b103040f","execution_count":8,"outputs":[{"name":"stderr","text":"/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n  warnings.warn(msg, category=FutureWarning)\n","output_type":"stream"},{"output_type":"execute_result","execution_count":8,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":4,"row_count":4,"columns":[{"name":"city=Gothenburg","dtype":"float64","stats":{"unique_count":2,"nan_count":0,"min":"0.0","max":"1.0","histogram":[{"bin_start":0,"bin_end":0.1,"count":2},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":2}]}},{"name":"city=Paris","dtype":"float64","stats":{"unique_count":2,"nan_count":0,"min":"0.0","max":"1.0","histogram":[{"bin_start":0,"bin_end":0.1,"count":2},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":2}]}},{"name":"month=December","dtype":"float64","stats":{"unique_count":2,"nan_count":0,"min":"0.0","max":"1.0","histogram":[{"bin_start":0,"bin_end":0.1,"count":2},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":2}]}},{"name":"month=July","dtype":"float64","stats":{"unique_count":2,"nan_count":0,"min":"0.0","max":"1.0","histogram":[{"bin_start":0,"bin_end":0.1,"count":2},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":2}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"city=Gothenburg":"1.0","city=Paris":"0.0","month=December":"0.0","month=July":"1.0","_deepnote_index_column":"0"},{"city=Gothenburg":"1.0","city=Paris":"0.0","month=December":"1.0","month=July":"0.0","_deepnote_index_column":"1"},{"city=Gothenburg":"0.0","city=Paris":"1.0","month=December":"0.0","month=July":"1.0","_deepnote_index_column":"2"},{"city=Gothenburg":"0.0","city=Paris":"1.0","month=December":"1.0","month=July":"0.0","_deepnote_index_column":"3"}]},"text/plain":"   city=Gothenburg  city=Paris  month=December  month=July\n0              1.0         0.0             0.0         1.0\n1              1.0         0.0             1.0         0.0\n2              0.0         1.0             0.0         1.0\n3              0.0         1.0             1.0         0.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>city=Gothenburg</th>\n      <th>city=Paris</th>\n      <th>month=December</th>\n      <th>month=July</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"4894b9c8f25147cf8c2aee9aeb22bfa3","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## Data preprocessing","block_group":"4894b9c8f25147cf8c2aee9aeb22bfa3"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"e6ddf5355cd84fa79ddcae2df867e15a","deepnote_cell_type":"code"},"source":"","block_group":"e6ddf5355cd84fa79ddcae2df867e15a","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"2dd2b22f29ed4a4c922e057c93fe5bc3","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# The Pegasos algorithm","block_group":"2dd2b22f29ed4a4c922e057c93fe5bc3"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"3c8515c340d348de9110c39705929b88","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Brief description: ","block_group":"3c8515c340d348de9110c39705929b88"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"d78ccf9e-8acf-4c01-b515-99258b257452","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"n and d are the number of samples and the number of features in the training data, respectively.\nWe initialize the weight vector w to all zeros.\nWe loop over the specified number of epochs.\nWithin each epoch, we loop over each training sample.\nWe calculate the learning rate eta based on the current time step t and the regularization parameter lmbda.\nIf the current sample is misclassified (i.e., y[i]*np.dot(X[i], w) < 1), we update the weight vector using the sub-gradient of the hinge loss function.\nIf the current sample is correctly classified, we only update the weight vector to account for regularization.\nWe increment the time step t at each iteration.","block_group":"d78ccf9e-8acf-4c01-b515-99258b257452"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"e7ca16e5-0fd4-4f6b-9ef7-df2f09b9174f","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Note that this implementation assumes that the data is already preprocessed (e.g., normalized). If your data is not preprocessed, you may want to do this before calling this function. Also, note that the output w is not a binary classifier, so you may need to threshold it appropriately to make predictions.","block_group":"e7ca16e5-0fd4-4f6b-9ef7-df2f09b9174f"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"108aa8890dbd4363b19cd5ec010b3ec8","deepnote_cell_type":"code"},"source":"import numpy as np\n\ndef pegasos_svm(X, y, lmbda, epochs):\n    n, d = X.shape\n    w = np.zeros(d)\n    t = 1\n    for epoch in range(epochs):\n        for i in range(n):\n            eta = 1.0 / (lmbda * t)\n            if y[i] * np.dot(X[i], w) < 1:\n                w = (1 - eta*lmbda) * w + eta*y[i]*X[i]\n            else:\n                w = (1 - eta*lmbda) * w\n            t += 1\n    return w\n","block_group":"108aa8890dbd4363b19cd5ec010b3ec8","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"0626f6caf26b412784e2a9d44b6a6526","source_hash":"2c73b647","execution_start":1677140414031,"execution_millis":2374,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"\n\"\"\"This file shows a couple of implementations of the perceptron learning\nalgorithm. It is based on the code from Lecture 3, but using the slightly\nmore compact perceptron formulation that we saw in Lecture 6.\n\nThere are two versions: Perceptron, which uses normal NumPy vectors and\nmatrices, and SparsePerceptron, which uses sparse vectors and matrices.\nThe latter may be faster when we have high-dimensional feature representations\nwith a lot of zeros, such as when we are using a \"bag of words\" representation\nof documents.\n\"\"\"\n\nimport numpy as np\nfrom sklearn.base import BaseEstimator\n\nclass LinearClassifier(BaseEstimator):\n    \"\"\"\n    General class for binary linear classifiers. Implements the predict\n    function, which is the same for all binary linear classifiers. There are\n    also two utility functions.\n    \"\"\"\n\n    def decision_function(self, X):\n        \"\"\"\n        Computes the decision function for the inputs X. The inputs are assumed to be\n        stored in a matrix, where each row contains the features for one\n        instance.\n        \"\"\"\n        return X.dot(self.w)\n\n    def predict(self, X):\n        \"\"\"\n        Predicts the outputs for the inputs X. The inputs are assumed to be\n        stored in a matrix, where each row contains the features for one\n        instance.\n        \"\"\"\n\n        # First compute the output scores\n        scores = self.decision_function(X)\n\n        # Select the positive or negative class label, depending on whether\n        # the score was positive or negative.\n        out = np.select([scores >= 0.0, scores < 0.0],\n                        [self.positive_class,\n                         self.negative_class])\n        return out\n\n    def find_classes(self, Y):\n        \"\"\"\n        Finds the set of output classes in the output part Y of the training set.\n        If there are exactly two classes, one of them is associated to positive\n        classifier scores, the other one to negative scores. If the number of\n        classes is not 2, an error is raised.\n        \"\"\"\n        classes = sorted(set(Y))\n        if len(classes) != 2:\n            raise Exception(\"this does not seem to be a 2-class problem\")\n        self.positive_class = classes[1]\n        self.negative_class = classes[0]\n\n    def encode_outputs(self, Y):\n        \"\"\"\n        A helper function that converts all outputs to +1 or -1.\n        \"\"\"\n        return np.array([1 if y == self.positive_class else -1 for y in Y])\n\n\nclass Perceptron(LinearClassifier):\n    \"\"\"\n    A straightforward implementation of the perceptron learning algorithm.\n    \"\"\"\n\n    def __init__(self, n_iter=20):\n        \"\"\"\n        The constructor can optionally take a parameter n_iter specifying how\n        many times we want to iterate through the training set.\n        \"\"\"\n        self.n_iter = n_iter\n\n    def fit(self, X, Y):\n        \"\"\"\n        Train a linear classifier using the perceptron learning algorithm.\n        \"\"\"\n\n        # First determine which output class will be associated with positive\n        # and negative scores, respectively.\n        self.find_classes(Y)\n\n        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n        Ye = self.encode_outputs(Y)\n\n        # If necessary, convert the sparse matrix returned by a vectorizer\n        # into a normal NumPy matrix.\n        if not isinstance(X, np.ndarray):\n            X = X.toarray()\n\n        # Initialize the weight vector to all zeros.\n        n_features = X.shape[1]\n        self.w = np.zeros(n_features)\n\n        # Perceptron algorithm:\n        for i in range(self.n_iter):\n            for x, y in zip(X, Ye):\n\n                # Compute the output score for this instance.\n                score = x.dot(self.w)\n\n                # If there was an error, update the weights.\n                if y*score <= 0:\n                    self.w += y*x\n\n\n##### The following part is for the optional task.\n\n### Sparse and dense vectors don't collaborate very well in NumPy/SciPy.\n### Here are two utility functions that help us carry out some vector\n### operations that we'll need.\n\ndef add_sparse_to_dense(x, w, factor):\n    \"\"\"\n    Adds a sparse vector x, scaled by some factor, to a dense vector.\n    This can be seen as the equivalent of w += factor * x when x is a dense\n    vector.\n    \"\"\"\n    w[x.indices] += factor * x.data\n\ndef sparse_dense_dot(x, w):\n    \"\"\"\n    Computes the dot product between a sparse vector x and a dense vector w.\n    \"\"\"\n    return np.dot(w[x.indices], x.data)\n\n\nclass SparsePerceptron(LinearClassifier):\n    \"\"\"\n    A straightforward implementation of the perceptron learning algorithm,\n    assuming that the input feature matrix X is sparse.\n    \"\"\"\n\n    def __init__(self, n_iter=20):\n        \"\"\"\n        The constructor can optionally take a parameter n_iter specifying how\n        many times we want to iterate through the training set.\n        \"\"\"\n        self.n_iter = n_iter\n\n    def fit(self, X, Y):\n        \"\"\"\n        Train a linear classifier using the perceptron learning algorithm.\n\n        Note that this will only work if X is a sparse matrix, such as the\n        output of a scikit-learn vectorizer.\n        \"\"\"\n        self.find_classes(Y)\n\n        # First determine which output class will be associated with positive\n        # and negative scores, respectively.\n        Ye = self.encode_outputs(Y)\n\n        # Initialize the weight vector to all zeros.\n        self.w = np.zeros(X.shape[1])\n\n        # Iteration through sparse matrices can be a bit slow, so we first\n        # prepare this list to speed up iteration.\n        XY = list(zip(X, Ye))\n\n        for i in range(self.n_iter):\n            for x, y in XY:\n\n                # Compute the output score for this instance.\n                # (This corresponds to score = x.dot(self.w) above.)\n                score = sparse_dense_dot(x, self.w)\n\n                # If there was an error, update the weights.\n                if y*score <= 0:\n                    # (This corresponds to self.w += y*x above.)\n                    add_sparse_to_dense(x, self.w, y)","block_group":"0626f6caf26b412784e2a9d44b6a6526","execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"536f6adb55ce4031a9a653f25664609b","source_hash":"c6d91d21","execution_start":1677142692496,"execution_millis":4216,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"\nimport time\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# This function reads the corpus, returns a list of documents, and a list\n# of their corresponding polarity labels. \ndef read_data(corpus_file):\n    X = []\n    Y = []\n    with open(corpus_file, encoding='utf-8') as f:\n        for line in f:\n            _, y, _, x = line.split(maxsplit=3)\n            X.append(x.strip())\n            Y.append(y)\n    return X, Y\n\n\nif __name__ == '__main__':\n    \n    # Read all the documents.\n    X, Y = read_data('data/all_sentiment_shuffled.txt')\n    \n    # Split into training and test parts.\n    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n                                                    random_state=0)\n\n    # Set up the preprocessing steps and the classifier.\n    pipeline = make_pipeline(\n        TfidfVectorizer(),\n        SelectKBest(k=1000),\n        Normalizer(),\n\n        # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n        SparsePerceptron()  \n    )\n\n    # Train the classifier.\n    t0 = time.time()\n    pipeline.fit(Xtrain, Ytrain)\n    t1 = time.time()\n    print('Training time: {:.2f} sec.'.format(t1-t0))\n\n    # Evaluate on the test set.\n    Yguess = pipeline.predict(Xtest)\n    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n\n","block_group":"536f6adb55ce4031a9a653f25664609b","execution_count":12,"outputs":[{"name":"stdout","text":"Training time: 3.67 sec.\nAccuracy: 0.7919.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=91294706-5412-4c53-b64a-ecb74941a413' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"820a18da3ad44eecaefa309c032bc6b6","deepnote_execution_queue":[]}}
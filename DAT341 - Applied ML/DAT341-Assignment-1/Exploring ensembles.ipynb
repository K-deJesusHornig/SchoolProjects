{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"22668c31ada04494b8dea4e3523ccdeb","deepnote_cell_type":"markdown"},"source":"# Ensemble learning in scikit-learn\n\nThis notebook shows how to use a number of different types of [ensembles](https://scikit-learn.org/stable/modules/ensemble.html) in scikit-learn. We use the Adult dataset to exemplify.","block_group":"22668c31ada04494b8dea4e3523ccdeb"},{"cell_type":"markdown","metadata":{"cell_id":"04ff8412d3a64fd49d7e7cd31b7b181d","deepnote_cell_type":"markdown"},"source":"### Reading data and preprocessing\n\nWhat we do here is probably going to be a bit more obvious after the next lecture, where we discuss preprocessing.\n\nWe use the same dataset as we'll use elsewhere in the course (among other places, in Programming assignment 2). The task here is a binary classification task, where we want to predict whether someone earns more than 50K dollars a year or not, given a set of demographic features. The dataset comes with a pre-defined train/test split and you can download the [training set](http://www.cse.chalmers.se/~richajo/dit866/data/adult_train.csv) and the [test set](http://www.cse.chalmers.se/~richajo/dit866/data/adult_test.csv) as separate files.\n\nAs in PA 2, we convert the rows of the Pandas dataframe into dictionaries, which works nicely with the `DictVectorizer`.","block_group":"04ff8412d3a64fd49d7e7cd31b7b181d"},{"cell_type":"code","metadata":{"cell_id":"fef541ccd5bf430f8cab19755b0aacdc","deepnote_cell_type":"code"},"source":"import pandas as pd\n\ntrain_data = pd.read_csv('data/adult_train.csv')\n\nn_cols = len(train_data.columns)\nXtrain_dicts = train_data.iloc[:, :n_cols-1].to_dict('records')\nYtrain = train_data.iloc[:, n_cols-1]\n\ntest_data = pd.read_csv('data/adult_test.csv')\nXtest_dicts = test_data.iloc[:, :n_cols-1].to_dict('records')\nYtest = test_data.iloc[:, n_cols-1]\n","block_group":"fef541ccd5bf430f8cab19755b0aacdc","execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"b68bdb49dae2435081805f8e8e1f3166","deepnote_cell_type":"markdown"},"source":"To give you a feel for the dataset, here are the first five rows. We want to predict the `target` column, given the other columns.","block_group":"b68bdb49dae2435081805f8e8e1f3166"},{"cell_type":"code","metadata":{"cell_id":"558ef9877acc43aca8a2c0a79c9e26e2","deepnote_cell_type":"code"},"source":"train_data.head()","block_group":"558ef9877acc43aca8a2c0a79c9e26e2","execution_count":2,"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>workclass</th>\n      <th>education</th>\n      <th>education-num</th>\n      <th>marital-status</th>\n      <th>occupation</th>\n      <th>relationship</th>\n      <th>race</th>\n      <th>sex</th>\n      <th>capital-gain</th>\n      <th>capital-loss</th>\n      <th>hours-per-week</th>\n      <th>native-country</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>27</td>\n      <td>Private</td>\n      <td>Some-college</td>\n      <td>10</td>\n      <td>Divorced</td>\n      <td>Adm-clerical</td>\n      <td>Unmarried</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>0</td>\n      <td>44</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>27</td>\n      <td>Private</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Never-married</td>\n      <td>Prof-specialty</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>25</td>\n      <td>Private</td>\n      <td>Assoc-acdm</td>\n      <td>12</td>\n      <td>Married-civ-spouse</td>\n      <td>Sales</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>46</td>\n      <td>Private</td>\n      <td>5th-6th</td>\n      <td>3</td>\n      <td>Married-civ-spouse</td>\n      <td>Transport-moving</td>\n      <td>Husband</td>\n      <td>Amer-Indian-Eskimo</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>1902</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>45</td>\n      <td>Private</td>\n      <td>11th</td>\n      <td>7</td>\n      <td>Divorced</td>\n      <td>Transport-moving</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>2824</td>\n      <td>76</td>\n      <td>United-States</td>\n      <td>&gt;50K</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   age workclass     education  education-num      marital-status  \\\n0   27   Private  Some-college             10            Divorced   \n1   27   Private     Bachelors             13       Never-married   \n2   25   Private    Assoc-acdm             12  Married-civ-spouse   \n3   46   Private       5th-6th              3  Married-civ-spouse   \n4   45   Private          11th              7            Divorced   \n\n         occupation   relationship                race     sex  capital-gain  \\\n0      Adm-clerical      Unmarried               White  Female             0   \n1    Prof-specialty  Not-in-family               White  Female             0   \n2             Sales        Husband               White    Male             0   \n3  Transport-moving        Husband  Amer-Indian-Eskimo    Male             0   \n4  Transport-moving  Not-in-family               White    Male             0   \n\n   capital-loss  hours-per-week native-country target  \n0             0              44  United-States  <=50K  \n1             0              40  United-States  <=50K  \n2             0              40  United-States  <=50K  \n3          1902              40  United-States  <=50K  \n4          2824              76  United-States   >50K  "},"execution_count":2,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"cell_id":"1364da8c0a99407fa5fe9b32b48574ce","deepnote_cell_type":"markdown"},"source":"Here is the representation of the first individual:","block_group":"1364da8c0a99407fa5fe9b32b48574ce"},{"cell_type":"code","metadata":{"cell_id":"f49528b1947d41c193dc5d417994a27b","deepnote_cell_type":"code"},"source":"Xtrain_dicts[0]","block_group":"f49528b1947d41c193dc5d417994a27b","execution_count":3,"outputs":[{"data":{"text/plain":"{'age': 27,\n 'workclass': 'Private',\n 'education': 'Some-college',\n 'education-num': 10,\n 'marital-status': 'Divorced',\n 'occupation': 'Adm-clerical',\n 'relationship': 'Unmarried',\n 'race': 'White',\n 'sex': 'Female',\n 'capital-gain': 0,\n 'capital-loss': 0,\n 'hours-per-week': 44,\n 'native-country': 'United-States'}"},"execution_count":3,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"cell_id":"0d9c9dd7529d41d7b8c010625ab928ac","deepnote_cell_type":"markdown"},"source":"To work with scikit-learn, we need to convert the symbolic features into a numerical matrix. Here is how we do this. Again, this is going to be clearer after the next lecture!","block_group":"0d9c9dd7529d41d7b8c010625ab928ac"},{"cell_type":"code","metadata":{"cell_id":"69b3399c180047f4b39bfc06158c6c51","deepnote_cell_type":"code"},"source":"# basic preprocessing stuff\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction import DictVectorizer\n\n\npreprocessing_pipeline = make_pipeline(DictVectorizer(), StandardScaler(with_mean=False))\n\nXtrain = preprocessing_pipeline.fit_transform(Xtrain_dicts)\nXtest = preprocessing_pipeline.transform(Xtest_dicts)","block_group":"69b3399c180047f4b39bfc06158c6c51","execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"410ddf4ae6c44096ac4e4d964c326f8a","collapsed":true,"deepnote_cell_type":"markdown"},"source":"### Building an ensemble of any set of classifiers\n\nThe following example shows how to combine a set of classifiers into an ensemble. This can be done simply in scikit-learn by using a [`VotingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html). Here, we combine two types of logistic regression, a decision tree, and a neural network.\n\nBy default, the `VotingClassifier` will use voting to compute the final prediction. By using the option `voting='soft'`, the `VotingClassifier` will use averaging of probabilities instead. Note that this requires a probability-aware classifier: it needs to have a method called `predict_proba`.\n\nThe option `n_jobs=-1` is for efficiency and simply means that we use all available processors on the machine and run the training of the submodels in parallel.","block_group":"410ddf4ae6c44096ac4e4d964c326f8a"},{"cell_type":"code","metadata":{"cell_id":"3f8e8c4218a94ae3a664d49665c2409a","deepnote_cell_type":"code"},"source":"# for evaluation\nfrom sklearn.metrics import accuracy_score\n\n# a few different types of classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\n# turn off annoying warnings\nimport warnings; warnings.simplefilter('ignore')\n\n# and the VotingClassifier\nfrom sklearn.ensemble import VotingClassifier","block_group":"3f8e8c4218a94ae3a664d49665c2409a","execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"201062ed8c324086b408dceb15745323","deepnote_cell_type":"code"},"source":"ensemble = [\n            ('lr', LogisticRegression()),\n            ('dt', DecisionTreeClassifier(max_depth=5)),\n            ('lr1', LogisticRegression(penalty='l1', solver='liblinear')),\n            ('mlp', MLPClassifier(hidden_layer_sizes=(8), max_iter=10000))\n           ]\n\nvoting = VotingClassifier(ensemble)\n#voting = VotingClassifier(ensemble, voting='soft')\n\nvoting.fit(Xtrain, Ytrain)\n\naccuracy_score(Ytest, voting.predict(Xtest))","block_group":"201062ed8c324086b408dceb15745323","execution_count":6,"outputs":[{"data":{"text/plain":"0.8540630182421227"},"execution_count":6,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"cell_id":"62da263143f64b85a6e1353adc6aebcf","deepnote_cell_type":"markdown"},"source":"### Stacking\n\nWe can create an ensemble using stacking in more or less the same way. This will take a bit of time, because cross-validation is used during training. (Why?)","block_group":"62da263143f64b85a6e1353adc6aebcf"},{"cell_type":"code","metadata":{"cell_id":"09d5705728834e979eac44c43223dea9","deepnote_cell_type":"code"},"source":"from sklearn.ensemble import StackingClassifier\n\nensemble = [\n            ('lr', LogisticRegression()),\n            ('dt', DecisionTreeClassifier(max_depth=5)),\n            ('lr1', LogisticRegression(penalty='l1', solver='liblinear')),\n            ('mlp', MLPClassifier(hidden_layer_sizes=(8), max_iter=10000))\n           ]\n\nstacking = StackingClassifier(ensemble)\n\nstacking.fit(Xtrain, Ytrain)\n\naccuracy_score(Ytest, stacking.predict(Xtest))","block_group":"09d5705728834e979eac44c43223dea9","execution_count":7,"outputs":[{"data":{"text/plain":"0.8560899207665377"},"execution_count":7,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"cell_id":"cc509604b9274e23bc91d88ebedee341","deepnote_cell_type":"markdown"},"source":"### Creating an ensemble using bagging and random subspace learning\n\nIn contrast to the example above, where we just combined a few different classifiers, we will now see how an ensemble can be created in a way where we more systematically try to achieve a diversity among the classifiers. We will use decision trees in this example.\n\nBefore we do that, let's see what kind of accuracy we get when we use a single decision tree with this dataset.","block_group":"cc509604b9274e23bc91d88ebedee341"},{"cell_type":"code","metadata":{"cell_id":"733a082cb875488cb84f33c6cb67c2e1","deepnote_cell_type":"code"},"source":"tree = DecisionTreeClassifier()\n\ntree.fit(Xtrain, Ytrain)\naccuracy_score(Ytest, tree.predict(Xtest))","block_group":"733a082cb875488cb84f33c6cb67c2e1","execution_count":8,"outputs":[{"data":{"text/plain":"0.8170259812050856"},"execution_count":8,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"cell_id":"0b6b181895da400687f00b612836fb92","deepnote_cell_type":"markdown"},"source":"The [`BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) creates an ensemble using the [*bagging*](https://en.wikipedia.org/wiki/Bootstrap_aggregating) method and/or [*random subspace learning*](https://en.wikipedia.org/wiki/Random_subspace_method) (\"feature bagging\"). In bagging, diversity of sub-classifiers is achieved by selecting new training sets from the original set by drawing *instances* with replacement. In random subspace learning, the different sub-classifiers instead use different subsets of *features*.\n\nBy setting `bootstrap=True` (this is true by default), bagging is enabled, and random subspace learning is turned on by setting `bootstrap_feature=True`. As you can see, by turning on both options, we can get an accuracy in the 0.85-0.86 range by turning on these features when using an ensemble. The exact accuracy you get will depend on how random sampling of instances and features is done; you can get reproducible results by setting the `random_state`. In general, you will get a higher accuracy when using a larger number of sub-classifiers (`n_estimators`) and there is no risk of overfitting by increasing this value, but this will of course make the ensemble slower.","block_group":"0b6b181895da400687f00b612836fb92"},{"cell_type":"code","metadata":{"cell_id":"4ec3c7d378bc44e2b6e2ba89bae55195","deepnote_cell_type":"code"},"source":"from sklearn.ensemble import BaggingClassifier\n\nfor bootstrap_instances in [False, True]:\n    for bootstrap_features in [False, True]:\n        bagging = BaggingClassifier(DecisionTreeClassifier(), \n                                    n_estimators=10, \n                                    bootstrap=bootstrap_instances, bootstrap_features=bootstrap_features, \n                                    random_state=0, n_jobs=-1)\n        \n\n        bagging.fit(Xtrain, Ytrain)\n\n        acc = accuracy_score(Ytest, bagging.predict(Xtest))\n\n        print(f'Instance bootstrapping: {bootstrap_instances}; feature bootstrapping: {bootstrap_features}; accuracy: {acc:.3f}')\n","block_group":"4ec3c7d378bc44e2b6e2ba89bae55195","execution_count":9,"outputs":[{"name":"stdout","output_type":"stream","text":"Instance bootstrapping: False; feature bootstrapping: False; accuracy: 0.819\nInstance bootstrapping: False; feature bootstrapping: True; accuracy: 0.847\nInstance bootstrapping: True; feature bootstrapping: False; accuracy: 0.844\nInstance bootstrapping: True; feature bootstrapping: True; accuracy: 0.851\n"}]},{"cell_type":"markdown","metadata":{"cell_id":"380800057b1344d29c6105f6286b2caf","deepnote_cell_type":"markdown"},"source":"### Random forests\n\nThe [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) uses the [*random forest*](https://en.wikipedia.org/wiki/Random_forest) method to build ensembles of decision trees. This ensemble training method uses training set bagging as well as random subspace learning each time a feature is selected when building the decision trees. This is usually a high-quality model for \"tabular\" data: that is, a set of named columns, such as what we get if we load a CSV or Excel file using Pandas. This is also the situation we have here.\n\nAs in the `BaggingClassifier`, the main hyperparameter to adjust when constructing the ensemble is the number of sub-trees used in the ensemble (`n_estimators`). Apart from that, the `RandomForestClassifier` (and the equivalent model for regression, `RandomForestRegression`) has a number of hyperparameter controlling the tree building, similar to a `DecisionTreeClassifier`.","block_group":"380800057b1344d29c6105f6286b2caf"},{"cell_type":"code","metadata":{"cell_id":"1fb960d2462d459cb73f2a85d092fdac","deepnote_cell_type":"code"},"source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=100, max_depth=20, random_state=0, n_jobs=-1)\n\nrf.fit(Xtrain, Ytrain)\naccuracy_score(Ytest, rf.predict(Xtest))","block_group":"1fb960d2462d459cb73f2a85d092fdac","execution_count":10,"outputs":[{"data":{"text/plain":"0.8624777347828757"},"execution_count":10,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"cell_id":"03b4a3013f2f4eab8550bea5c6a0fb28","deepnote_cell_type":"markdown"},"source":"Important hyperparameters for random forests:\n\n- `n_estimators` controls the size of the ensemble\n- `max_features`: how many features to consider when splitting; by default, sqrt(n_features)\n- tree-related hyperparameters including `max_depth`\n- `n_jobs` for how many CPU cores to use\n- `random_state` for reproducibility","block_group":"03b4a3013f2f4eab8550bea5c6a0fb28"},{"cell_type":"code","metadata":{"cell_id":"7ef564fd09ef4df492022cf600519bd8","deepnote_cell_type":"code"},"source":"","block_group":"7ef564fd09ef4df492022cf600519bd8","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=727b6f02-a414-4cc8-9dec-71ed60bb1da5' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","version":"3.8.5","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"deepnote_notebook_id":"3bd3487447f84d959187e2a32129ec8c","deepnote_execution_queue":[]}}
{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"bacac0efed514e129b417ba678117dd1","formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# Converting CSV data --> RDF for grahpDB","block_group":"bacac0efed514e129b417ba678117dd1"},{"cell_type":"markdown","metadata":{"cell_id":"93fe001c85a34df7874e95eef1baeb7b","formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Martin Toremark & Kailash de Jesus Hornig \nGroup 43 \nDAT475 advanced databases @ Chalmers","block_group":"93fe001c85a34df7874e95eef1baeb7b"},{"cell_type":"code","metadata":{"cell_id":"113c29b68b6341e6a769937daa728dc3","source_hash":"237f57ed","execution_start":1683876398479,"execution_millis":5003,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"!pip install rdflib==6.3.2","block_group":"113c29b68b6341e6a769937daa728dc3","execution_count":1,"outputs":[{"name":"stdout","text":"Collecting rdflib==6.3.2\n  Downloading rdflib-6.3.2-py3-none-any.whl (528 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m528.1/528.1 KB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: isodate<0.7.0,>=0.6.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from rdflib==6.3.2) (0.6.1)\nRequirement already satisfied: pyparsing<4,>=2.1.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from rdflib==6.3.2) (3.0.9)\nRequirement already satisfied: six in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from isodate<0.7.0,>=0.6.0->rdflib==6.3.2) (1.16.0)\nInstalling collected packages: rdflib\nSuccessfully installed rdflib-6.3.2\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"1102100957db4027a21838c956e665a1","source_hash":"bff679b3","execution_start":1683879204273,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# Imports\nimport pandas as pd\nimport csv\nfrom rdflib import URIRef, Literal, Namespace, Graph\nfrom rdflib.namespace import RDF, RDFS","block_group":"1102100957db4027a21838c956e665a1","execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"3ad37a7156f14f44974edc5beefca879","source_hash":"7a54d25a","execution_start":1683879209417,"execution_millis":202,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# create an RDF graph and define namespaces\ng = Graph()\n\n# Load your ontology into the RDF graph\n#g.parse(\"assignment2_export.ttl\", format=\"turtle\")\n\n# Define namespaces\nMY_ONTOLOGY_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/\")\nMY_DATA_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/Course\")\n\n\n# Open the CSV file\nwith open('Courses.csv', 'r') as csvfile:\n    csvreader = csv.DictReader(csvfile)\n\n    # Loop through each row in the CSV file\n    for row in csvreader:\n        # Create URIs for the course and its properties\n        course_uri = MY_DATA_NS[row['Course_code']] # creates unique names (course+course_code)\n        course_name_uri = MY_ONTOLOGY_NS['courseName']\n        credits_uri = MY_ONTOLOGY_NS['credits']\n        level_uri = MY_ONTOLOGY_NS['level']\n        owned_by_uri = MY_ONTOLOGY_NS['OwnedBy']\n\n        # Add triples for the course\n        g.add((course_uri, RDF.type, MY_ONTOLOGY_NS['Course'])) #instanceID\n        g.add((course_uri, course_name_uri, Literal(row['Course_name']))) #data props\n        g.add((course_uri, credits_uri, Literal(row['Credits'])))\n        g.add((course_uri, level_uri, Literal(row['Level'])))\n        g.add((course_uri, owned_by_uri, Literal(row['Owned_By']))) #obj props\n\n\"\"\" Skriv objs på format g.add((course_uri, owned_by_uri, MY_ONTOLOGY_NS['Programme']))\n\"\"\"\n\n# serialize the graph in Turtle format\ngraph_data_k = g.serialize(format='turtle')\n#print(graph_data_k)\n\n# save file\n#with open(\"with_courses_kai.ttl\", \"w\") as turtlefile:\n#    turtlefile.write(g.serialize(format='turtle'))\n","block_group":"3ad37a7156f14f44974edc5beefca879","execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"3970085e3f10479ebd689c592f657dd3","source_hash":"3d67aedb","execution_start":1683661242824,"execution_millis":146,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# Same process for adding programmes to graph\n# Define namespaces\nMY_ONTOLOGY_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/\")\nMY_DATA_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/Programmme\")\n\n# Open the CSV file\nwith open('Programmes.csv', 'r') as csvfile:\n    csvreader = csv.DictReader(csvfile)\n\n    # Loop through each row in the CSV file\n    for row in csvreader:\n        # Create URIs for the programme and its properties\n        programme_uri = MY_DATA_NS[row['Programme_code']]\n        programme_name_uri = MY_ONTOLOGY_NS['programme_name']\n        programme_code_uri = MY_ONTOLOGY_NS['programme_code']\n        director_uri = MY_ONTOLOGY_NS['director']\n\n        # Add triples for the programme\n        g.add((programme_uri, RDF.type, MY_ONTOLOGY_NS['Programme']))\n        g.add((programme_uri, programme_name_uri, Literal(row['Programme_name'])))\n        g.add((programme_uri, programme_code_uri, Literal(row['Programme_code'])))\n        g.add((programme_uri, director_uri, Literal(row['Director'])))\n\n# serialize the graph in Turtle format\ngraph_data_k = g.serialize(format='turtle')\n#print(graph_data_k)\n\n\n# save file\nwith open(\"with_programmes_kai.ttl\", \"w\") as turtlefile:\n    turtlefile.write(g.serialize(format='turtle'))\n\n","block_group":"3970085e3f10479ebd689c592f657dd3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"946eaf53925b46a28032ae29739ab6c3","source_hash":"b4d3eaf7","execution_start":1683661245961,"execution_millis":319,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# Same process for adding students to graph\n# Define namespaces\nMY_ONTOLOGY_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/\")\nMY_DATA_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/Student\")\n\n\n# read csv file and create triples\nwith open(\"Students.csv\", \"r\") as csvfile:\n    #loop each row\n    for row in csv.DictReader(csvfile):\n        # create student URI\n        student_uri = MY_DATA_NS[row[\"Student id\"]]\n        # create student triples\n        g.add((student_uri, RDF.type, MY_ONTOLOGY_NS.Student))\n        g.add((student_uri, MY_ONTOLOGY_NS['Student_name'], Literal(row[\"Student name\"]))) \n        g.add((student_uri, MY_ONTOLOGY_NS['Programme'], Literal(row[\"Programme\"])))\n        g.add((student_uri, MY_ONTOLOGY_NS['Year'], Literal(row[\"Year\"])))\n        g.add((student_uri, MY_ONTOLOGY_NS['Graduated'], Literal(row[\"Graduated\"])))\n\n\n# serialize the graph in Turtle format\ngraph_data_k = g.serialize(format='turtle')\n#print(graph_data_k)\n\n\n# save file\nwith open(\"with_students_kai.ttl\", \"w\") as turtlefile:\n    turtlefile.write(g.serialize(format='turtle'))\n\n","block_group":"946eaf53925b46a28032ae29739ab6c3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"dc7f97997c7c4ec086033307454fc57c","source_hash":"2735dad4","execution_start":1683661217755,"execution_millis":11,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# Same process for adding program courses to graph\n# Define namespaces\nMY_ONTOLOGY_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/\")\nMY_DATA_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/ProgrammeCourse\")\n\n\n# read csv file and create triples\nwith open(\"Programme_Courses.csv\", \"r\") as csvfile:\n    #loop each row\n    for row in csv.DictReader(csvfile):\n        # create  URI\n        programmeCourse_uri = MY_DATA_NS[row[\"Program course id\"]]\n        # create  triples\n        g.add((programmeCourse_uri, RDF.type, MY_ONTOLOGY_NS.ProgrammeCourse))\n        g.add((programmeCourse_uri, MY_ONTOLOGY_NS['studyYear'], Literal(row[\"Study Year\"]))) \n        g.add((programmeCourse_uri, MY_ONTOLOGY_NS['courseType'], Literal(row[\"Course Type\"])))\n        g.add((programmeCourse_uri, MY_ONTOLOGY_NS['HoldsA'], Literal(row[\"Course\"])))\n        g.add((programmeCourse_uri, MY_ONTOLOGY_NS['HasA'], Literal(row[\"Programme code\"])))\n\n\n# serialize the graph in Turtle format\ngraph_data_k = g.serialize(format='turtle')\n#print(graph_data_k)\n\n\n# save file\nwith open(\"with_ProgrammeCourse_kai.ttl\", \"w\") as turtlefile:\n    turtlefile.write(g.serialize(format='turtle'))\n\n","block_group":"dc7f97997c7c4ec086033307454fc57c","execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'Namespace' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Same process for adding program courses to graph\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Define namespaces\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m MY_ONTOLOGY_NS \u001b[38;5;241m=\u001b[39m \u001b[43mNamespace\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m MY_DATA_NS \u001b[38;5;241m=\u001b[39m Namespace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/ProgramCourse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# read csv file and create triples\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'Namespace' is not defined"]}]},{"cell_type":"markdown","metadata":{"cell_id":"512dc6cd4219487281e15ccf1e6cf06a","formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## Merging Course planning and course instances","block_group":"512dc6cd4219487281e15ccf1e6cf06a"},{"cell_type":"code","metadata":{"cell_id":"12d689cb02e448c6af1245edea402776","source_hash":"cfb4699d","execution_start":1683879224255,"execution_millis":2949,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#merge course planning and instances on  id and then split into Teacher and teacherassistant\n\n# read in the first CSV file\nwith open('Course_plannings.csv', 'r') as f1:\n    reader1 = csv.DictReader(f1)\n    data1 = [row for row in reader1]\n\n# read in the second CSV file\nwith open('Course_Instances.csv', 'r') as f2:\n    reader2 = csv.DictReader(f2)\n    data2 = [row for row in reader2]\n\n# merge the two datasets on the \"Teacher Id\" column\nmerged_data = []\nfor row1 in data1:\n    for row2 in data2:\n        if row1['Course'] == row2['Instance_id']:\n            merged_row = {**row1, **row2}\n            merged_data.append(merged_row)\n\n# write the merged data to a new CSV file\nfieldnames = list(merged_data[0].keys())\nwith open('merged_course_data.csv', 'w', newline='') as outfile:\n    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n    writer.writeheader()\n    for row in merged_data:\n        writer.writerow(row)\n","block_group":"12d689cb02e448c6af1245edea402776","execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"9511a6e1dc7a4692bcd9bfad0b411a6a","deepnote_cell_type":"code"},"source":"# Load merged course data to graph\n# Define namespaces\nMY_ONTOLOGY_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/\")\nMY_DATA_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/Courseinstance\")\n\n\n# read csv file and create triples\nwith open(\"merged_course_data.csv\", \"r\") as csvfile:\n    #loop each row\n    for row in csv.DictReader(csvfile):\n        # create  URI\n        courseinstance_uri = MY_DATA_NS[row[\"Instance_id\"]]\n        # create  triples\n        g.add((courseinstance_uri, RDF.type, MY_ONTOLOGY_NS.Courseinstance))\n        g.add((courseinstance_uri, MY_ONTOLOGY_NS['studyPeriod'], Literal(row[\"Study period\"]))) \n        g.add((courseinstance_uri, MY_ONTOLOGY_NS['academicYear'], Literal(row[\"Academic year\"])))\n        g.add((courseinstance_uri, MY_ONTOLOGY_NS['numberOfStudents'], Literal(row[\"Planned number of Students\"])))\n        g.add((courseinstance_uri, MY_ONTOLOGY_NS['teacherHours'], Literal(row[\"Teacher Hours\"])))\n        g.add((courseinstance_uri, MY_ONTOLOGY_NS['assistantHours'], Literal(row[\"Assistant Hours\"])))\n\n# serialize the graph in Turtle format\ngraph_data_k = g.serialize(format='turtle')\n#print(graph_data_k)\n\n# save file\nwith open(\"with_mergedCourseinstance_kai.ttl\", \"w\") as turtlefile:\n    turtlefile.write(g.serialize(format='turtle'))\n","block_group":"9511a6e1dc7a4692bcd9bfad0b411a6a","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"7a3edf6f-1ea0-41f7-9523-190b4ca8d71d","formattedRanges":[],"deepnote_cell_type":"text-cell-h3"},"source":"### ","block_group":"7a3edf6f-1ea0-41f7-9523-190b4ca8d71d"},{"cell_type":"code","metadata":{"cell_id":"24fd033f0c05404190a76a5511c638cd","source_hash":"8fe95646","execution_start":1683880572555,"execution_millis":2607,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# Same process for adding Registrations to graph\n# Define namespaces\nMY_ONTOLOGY_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/\")\nMY_DATA_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/CourseRegistration\")\n\n\n# read csv file and create triples\nwith open(\"Registrations.csv\", \"r\") as csvfile:\n    #loop each row\n    for row in csv.DictReader(csvfile):\n        # create  URI\n        registration_uri = MY_DATA_NS[row[\"Registration id\"]]\n        # create  triples\n        g.add((registration_uri, RDF.type, MY_ONTOLOGY_NS.CourseRegistration))\n        g.add((registration_uri, MY_ONTOLOGY_NS['Course_Instance'], Literal(row[\"Course Instance\"]))) \n        g.add((registration_uri, MY_ONTOLOGY_NS['Student_id'], Literal(row[\"Student id\"])))\n        g.add((registration_uri, MY_ONTOLOGY_NS['Status'], Literal(row[\"Status\"])))\n        g.add((registration_uri, MY_ONTOLOGY_NS['Grade'], Literal(row[\"Grade\"])))\n\n\n# serialize the graph in Turtle format\ngraph_data_k = g.serialize(format='turtle')\n#print(graph_data_k)\n\n\n# save file\nwith open(\"with_CourseRegistration_kai.ttl\", \"w\") as turtlefile:\n    turtlefile.write(g.serialize(format='turtle'))\n","block_group":"24fd033f0c05404190a76a5511c638cd","execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"12ff2483e8074f6e911eedb9d07d891a","formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## Merging assigned and reported hours for different assignments","block_group":"12ff2483e8074f6e911eedb9d07d891a"},{"cell_type":"code","metadata":{"cell_id":"b9cd9e0e49984469b2ab1220823a58ae","source_hash":"fb245be3","execution_start":1683663371105,"execution_millis":26216,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#merge assigned and reported hours on teacher id and then split into Teacher and teacherassistant\n\n# read in the first CSV file\nwith open('Assigned_Hours.csv', 'r') as f1:\n    reader1 = csv.DictReader(f1)\n    data1 = [row for row in reader1]\n\n# read in the second CSV file\nwith open('Reported_Hours.csv', 'r') as f2:\n    reader2 = csv.DictReader(f2)\n    data2 = [row for row in reader2]\n\n# merge the two datasets on the \"Teacher Id\" column\nmerged_data = []\nfor row1 in data1:\n    for row2 in data2:\n        if row1['Teacher Id'] == row2['Teacher Id']:\n            merged_row = {**row1, **row2}\n            merged_data.append(merged_row)\n\n# write the merged data to a new CSV file\nfieldnames = list(merged_data[0].keys())\nwith open('merged_data.csv', 'w', newline='') as outfile:\n    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n    writer.writeheader()\n    for row in merged_data:\n        writer.writerow(row)\n","block_group":"b9cd9e0e49984469b2ab1220823a58ae","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"83fd03c33e344ca28d6d3e44ceda2efb","source_hash":"f43dafd9","execution_start":1683663952861,"execution_millis":11410,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# then the split \n\n# Load the Teaching_Assistants.csv file into a set\nwith open('Teaching_Assistants.csv', 'r') as f:\n    reader = csv.DictReader(f)\n    teacher_assistants = set([row['Teacher id'] for row in reader])\n\n# Separate the rows based on teacher assistants and teachers\nteacher_assistants_rows = []\nteachers_rows = []\nwith open('merged_data.csv', 'r') as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        if row['Teacher Id'] in teacher_assistants:\n            teacher_assistants_rows.append(row)\n        else:\n            teachers_rows.append(row)\n\n# Write the teacherassistants_extracted.csv file\nwith open('teacherassistants_extracted.csv', 'w', newline='') as f:\n    writer = csv.DictWriter(f, fieldnames=reader.fieldnames)\n    writer.writeheader()\n    for row in teacher_assistants_rows:\n        writer.writerow(row)\n\n# Write the teachers_extracted.csv file\nwith open('teachers_extracted.csv', 'w', newline='') as f:\n    writer = csv.DictWriter(f, fieldnames=reader.fieldnames)\n    writer.writeheader()\n    for row in teachers_rows:\n        writer.writerow(row)\n\n# some random controls were carried out to validate the operations, it looked good. ","block_group":"83fd03c33e344ca28d6d3e44ceda2efb","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"03a2896bf6364ddbb3fc9b8d751faad2","formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## Loading the extracted files of TA and T-assignments (needs extra computational resources)","block_group":"03a2896bf6364ddbb3fc9b8d751faad2"},{"cell_type":"markdown","metadata":{"cell_id":"526a027e57994ee5970bb320573be239","formattedRanges":[],"deepnote_cell_type":"text-cell-h3"},"source":"### The extracted files also have added synthetical IDs for unique rows (Teacher Id, Hours, Course Instance)","block_group":"526a027e57994ee5970bb320573be239"},{"cell_type":"code","metadata":{"cell_id":"1c71bffc14f04e80b857f0d70c294ea6","source_hash":"39cfbb51","execution_start":1683665539860,"execution_millis":517,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# Same process for adding TA assignment to graph\n# Define namespaces\nMY_ONTOLOGY_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/\")\nMY_DATA_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/TA_Assignment\")\n\n\n# read csv file and create triples\nwith open(\"teacherassistants_extracted.csv\", \"r\") as csvfile:\n    #loop each row\n    for row in csv.DictReader(csvfile):\n        # create  URI\n        ta_assignment_uri = MY_DATA_NS[row[\"TA assignment id\"]]\n        # create  triples\n        g.add((ta_assignment_uri, RDF.type, MY_ONTOLOGY_NS.TA_Assignment))\n        g.add((ta_assignment_uri, MY_ONTOLOGY_NS['Course_code'], Literal(row[\"Course code\"]))) \n        g.add((ta_assignment_uri, MY_ONTOLOGY_NS['Study_Period'], Literal(row[\"Study Period\"])))\n        g.add((ta_assignment_uri, MY_ONTOLOGY_NS['Academic_Year'], Literal(row[\"Academic Year\"])))\n        g.add((ta_assignment_uri, MY_ONTOLOGY_NS['Teacher_Id'], Literal(row[\"Teacher Id\"])))\n        g.add((ta_assignment_uri, MY_ONTOLOGY_NS['Hours'], Literal(row[\"Hours\"])))\n        g.add((ta_assignment_uri, MY_ONTOLOGY_NS['Course_Instance'], Literal(row[\"Course Instance\"])))\n\n\n# serialize the graph in Turtle format\ngraph_data_k = g.serialize(format='turtle')\n#print(graph_data_k)\n\n# save file\nwith open(\"with_TA_Assignment_kai.ttl\", \"w\") as turtlefile:\n    turtlefile.write(g.serialize(format='turtle'))\n","block_group":"1c71bffc14f04e80b857f0d70c294ea6","execution_count":1,"outputs":[{"output_type":"error","ename":"KernelInterrupted","evalue":"Execution interrupted by the Jupyter kernel.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKernelInterrupted\u001b[0m: Execution interrupted by the Jupyter kernel."]}]},{"cell_type":"code","metadata":{"cell_id":"e4e9cfc4287a4eec9b558f1196a00772","source_hash":"25504dd6","execution_start":1683666118596,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# Same process for adding T assignment to graph\n# Define namespaces\nMY_ONTOLOGY_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/\")\nMY_DATA_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/T_Assignment\")\n\n\n# read csv file and create triples\nwith open(\"teachers_extracted.csv\", \"r\") as csvfile:\n    #loop each row\n    for row in csv.DictReader(csvfile):\n        # create  URI\n        t_assignment_uri = MY_DATA_NS[row[\"T assignment id\"]]\n        # create  triples\n        g.add((t_assignment_uri, RDF.type, MY_ONTOLOGY_NS.T_Assignment))\n        g.add((t_assignment_uri, MY_ONTOLOGY_NS['Course_code'], Literal(row[\"Course code\"]))) \n        g.add((t_assignment_uri, MY_ONTOLOGY_NS['Study_Period'], Literal(row[\"Study Period\"])))\n        g.add((t_assignment_uri, MY_ONTOLOGY_NS['Academic_Year'], Literal(row[\"Academic Year\"])))\n        g.add((t_assignment_uri, MY_ONTOLOGY_NS['Teacher_Id'], Literal(row[\"Teacher Id\"])))\n        g.add((t_assignment_uri, MY_ONTOLOGY_NS['Hours'], Literal(row[\"Hours\"])))\n        g.add((t_assignment_uri, MY_ONTOLOGY_NS['Course_Instance'], Literal(row[\"Course Instance\"])))\n\n\n# serialize the graph in Turtle format\ngraph_data_k = g.serialize(format='turtle')\n#print(graph_data_k)\n\n# save file\nwith open(\"with_T_Assignment_kai.ttl\", \"w\") as turtlefile:\n    turtlefile.write(g.serialize(format='turtle'))\n","block_group":"e4e9cfc4287a4eec9b558f1196a00772","execution_count":null,"outputs":[{"output_type":"error","ename":"KernelInterrupted","evalue":"Execution interrupted by the Jupyter kernel.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKernelInterrupted\u001b[0m: Execution interrupted by the Jupyter kernel."]}]},{"cell_type":"code","metadata":{"cell_id":"e9778178c2784f64b2429a31b1c607f8","source_hash":"a910b2f2","execution_start":1683880597162,"execution_millis":2363,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# Same process for adding (sr) teachers to graph\n# Define namespaces\nMY_ONTOLOGY_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/\")\nMY_DATA_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/Teacher\")\n\n\n# read csv file and create triples\nwith open(\"(sr)_Teachers.csv\", \"r\") as csvfile:\n    #loop each row\n    for row in csv.DictReader(csvfile):\n        # create  URI\n        teacher_uri = MY_DATA_NS[row[\"Teacher id\"]]\n        # create  triples\n        g.add((teacher_uri, RDF.type, MY_ONTOLOGY_NS.Teacher))\n        g.add((teacher_uri, MY_ONTOLOGY_NS['Teacher_name'], Literal(row[\"Teacher name\"]))) \n        g.add((teacher_uri, MY_ONTOLOGY_NS['Department_name'], Literal(row[\"Department name\"])))\n        g.add((teacher_uri, MY_ONTOLOGY_NS['Division_name'], Literal(row[\"Division name\"])))\n        \n\n\n# serialize the graph in Turtle format\ngraph_data_k = g.serialize(format='turtle')\n#print(graph_data_k)\n\n# save file\nwith open(\"with_Teacher_kai.ttl\", \"w\") as turtlefile:\n    turtlefile.write(g.serialize(format='turtle'))","block_group":"e9778178c2784f64b2429a31b1c607f8","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"fd93f54617c64f1ca968d222011d2e60","deepnote_cell_type":"code"},"source":"# Same process for adding teaching assistants to graph\n# Define namespaces\nMY_ONTOLOGY_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/\")\nMY_DATA_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/Teacherassistant\")\n\n\n# read csv file and create triples\nwith open(\"Teaching_Assistants.csv\", \"r\") as csvfile:\n    #loop each row\n    for row in csv.DictReader(csvfile):\n        # create  URI\n        teachingassistant_uri = MY_DATA_NS[row[\"Teacher id\"]]\n        # create  triples\n        g.add((teachingassistant_uri, RDF.type, MY_ONTOLOGY_NS.Teacherassistant))\n        g.add((teachingassistant_uri, MY_ONTOLOGY_NS['Teacher_name'], Literal(row[\"Teacher name\"]))) \n        g.add((teachingassistant_uri, MY_ONTOLOGY_NS['Department_name'], Literal(row[\"Department name\"])))\n        g.add((teachingassistant_uri, MY_ONTOLOGY_NS['Division_name'], Literal(row[\"Division name\"])))\n        \n\n\n# serialize the graph in Turtle format\ngraph_data_k = g.serialize(format='turtle')\n#print(graph_data_k)\n\n# save file\nwith open(\"with_TeachingAssistans_kai.ttl\", \"w\") as turtlefile:\n    turtlefile.write(g.serialize(format='turtle'))","block_group":"fd93f54617c64f1ca968d222011d2e60","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"8618f200c6c34fe4bf51844c96a58dde","deepnote_cell_type":"code"},"source":"# Same process for adding departments to graph\n# Define namespaces\nMY_ONTOLOGY_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/\")\nMY_DATA_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/Department\")\n\n\n# read csv file and create triples\nwith open(\"Departments.csv\", \"r\") as csvfile:\n    #loop each row\n    for row in csv.DictReader(csvfile):\n        # create  URI\n        department_uri = MY_DATA_NS[row[\"Department name\"]]\n        # create  triples\n        g.add((department_uri, RDF.type, MY_ONTOLOGY_NS.Department))\n        g.add((department_uri, MY_ONTOLOGY_NS['Department_name'], Literal(row[\"Department name\"]))) \n\n# potential for improvement, add courses of department via mapping\n\n# serialize the graph in Turtle format\ngraph_data_k = g.serialize(format='turtle')\n#print(graph_data_k)\n\n# save file\nwith open(\"with_department_kai.ttl\", \"w\") as turtlefile:\n    turtlefile.write(g.serialize(format='turtle'))","block_group":"8618f200c6c34fe4bf51844c96a58dde","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"12edae26b97d435c9d8034f355e00bd1","deepnote_cell_type":"code"},"source":"# Same process for adding divisions to graph\n# Define namespaces\nMY_ONTOLOGY_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/\")\nMY_DATA_NS = Namespace(\"http://www.semanticweb.org/kailashdejesushornig/ontologies/2023/3/assignment2_v1/Division\")\n\n\n# read csv file and create triples\nwith open(\"Divisions.csv\", \"r\") as csvfile:\n    #loop each row\n    for row in csv.DictReader(csvfile):\n        # create  URI\n        division_uri = MY_DATA_NS[row[\"Division name\"]]\n        # create  triples\n        g.add((division_uri, RDF.type, MY_ONTOLOGY_NS.Division))\n        g.add((division_uri, MY_ONTOLOGY_NS['Department_name'], Literal(row[\"Department name\"]))) \n        g.add((division_uri, MY_ONTOLOGY_NS['Division_name'], Literal(row[\"Division name\"])))\n\n# serialize the graph in Turtle format\ngraph_data_k = g.serialize(format='turtle')\n#print(graph_data_k)\n\n# save file\nwith open(\"with_division_kai.ttl\", \"w\") as turtlefile:\n    turtlefile.write(g.serialize(format='turtle'))","block_group":"12edae26b97d435c9d8034f355e00bd1","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"655fd3975e2d479983c7eba09011272d","formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"\n","block_group":"655fd3975e2d479983c7eba09011272d"},{"cell_type":"markdown","metadata":{"cell_id":"69d073c14de2490191ed229f5732a238","formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"","block_group":"69d073c14de2490191ed229f5732a238"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=b9a9ca49-e6a7-40ba-a823-657cfa73e980' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"5173c7521fb34c3b82a4c58be0ac6cdf","deepnote_execution_queue":[]}}
{"cells":[{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"09963bcc5d9740ffa83c8f68e4da751f","deepnote_cell_type":"markdown"},"source":"#### Authors and Group members\nTime spent:\nNima Hansen: 14 hours \nKailash de Jesus Hornig: 14 hours\n","block_group":"09963bcc5d9740ffa83c8f68e4da751f"},{"cell_type":"markdown","metadata":{"id":"R3D82waLqItO","cell_id":"5d0c85e4808e4acab1cd4e70a9d41bc1","deepnote_cell_type":"markdown"},"source":"#DAT405 Introduction to Data Science and AI \n##2022-2023, Reading Period 1\n## Assignment 5: Reinforcement learning and classification\nThere will be an overall grade for this assignment. To get a pass grade (grade 3), you need to pass items 1-3 below. To receive higher grades, finish items 4 and 5 as well.\n\nThe exercise takes place in a notebook environment where you can chose to use Jupyter or Google Colabs. We recommend you use Google Colabs as it will facilitate remote group-work and makes the assignment less technical. \nHints:\nYou can execute certain linux shell commands by prefixing the command with `!`. You can insert Markdown cells and code cells. The first you can use for documenting and explaining your results the second you can use writing code snippets that execute the tasks required.  \n\nThis assignment is about **sequential decision making** under uncertainty (Reinforcement learning). In a sequential decision process, the process jumps between different states (the environment), and in each state the decision maker, or agent, chooses among a set of actions. Given the state and the chosen action, the process jumps to a new state. At each jump the decision maker receives a reward, and the objective is to find a sequence of decisions (or an optimal policy) that maximizes the accumulated rewards.\n\nWe will use **Markov decision processes** (MDPs) to model the environment, and below is a primer on the relevant background theory. The assignment can be divided in two parts:\n","block_group":"5d0c85e4808e4acab1cd4e70a9d41bc1"},{"cell_type":"markdown","metadata":{"id":"8jEcC9NKqItQ","cell_id":"b1ee33b68c504b6881bfee22daa56213","deepnote_cell_type":"markdown"},"source":"\n* To make things concrete, we will first focus on decision making under **no** uncertainity, i.e, given we have a world model, we can calculate the exact and optimal actions to take in it. We will first introduce **Markov Decision Process (MDP)** as the world model. Then we give one algorithm (out of many) to solve it.\n\n\n* Next, we will work through one type of reinforcement learning algorithm called Q-learning. Q-learning is an algorithm for making decisions under uncertainity, where uncertainity is over the possible world model (here MDP). It will find the optimal policy for the **unknown** MDP, assuming we do infinite exploration.","block_group":"b1ee33b68c504b6881bfee22daa56213"},{"cell_type":"markdown","metadata":{"id":"uGtknnUVqItP","cell_id":"e03f6ed7b5b943529dbaa381f2e7065e","deepnote_cell_type":"markdown"},"source":"## Primer\n### Decision Making\nThe problem of **decision making under uncertainty** (commonly known as **reinforcement learning**) can be broken down into\ntwo parts. First, how do we learn about the world? This involves both the problem of modeling our initial uncertainty about the world, and that of drawing conclusions from evidence and our initial belief. Secondly, given what we currently know about the world, how should we decide what to do, taking into account future events and observations that may change our conclusions?\nTypically, this will involve creating long-term plans covering possible future eventualities. That is, when planning under uncertainty, we also need to take into account what possible future knowledge could be generated when implementing our plans. Intuitively, executing plans which involve trying out new things should give more information, but it is hard to tell whether this information will be beneficial. The choice between doing something which is already known to produce good results and experiment with something new is known as the **exploration-exploitation dilemma**.\n\n### The exploration-exploitation trade-off\n\nConsider the problem of selecting a restaurant to go to during a vacation. Lets say the\nbest restaurant you have found so far was **Les Epinards**. The food there is\nusually to your taste and satisfactory. However, a well-known recommendations\nwebsite suggests that **King’s Arm** is really good! It is tempting to try it out. But\nthere is a risk involved. It may turn out to be much worse than **Les Epinards**,\nin which case you will regret going there. On the other hand, it could also be\nmuch better. What should you do?\nIt all depends on how much information you have about either restaurant,\nand how many more days you’ll stay in town. If this is your last day, then it’s\nprobably a better idea to go to **Les Epinards**, unless you are expecting **King’s\nArm** to be significantly better. However, if you are going to stay there longer,\ntrying out **King’s Arm** is a good bet. If you are lucky, you will be getting much\nbetter food for the remaining time, while otherwise you will have missed only\none good meal out of many, making the potential risk quite small.","block_group":"e03f6ed7b5b943529dbaa381f2e7065e"},{"cell_type":"markdown","metadata":{"id":"h9WIePUCqItR","cell_id":"b3dd74db96524786b809893520de0444","deepnote_cell_type":"markdown"},"source":"### Markov Decision Processes\nMarkov Decision Processes (MDPs) provide a mathematical framework for modeling sequential decision making under uncertainty. An *agent* moves between *states* in a *state space* choosing *actions* that affects the transition probabilities between states, and the subsequent *rewards* recieved after a jump. This is then repeated a finite or infinite number of epochs. The objective, or the *solution* of the MDP, is to optimize the accumulated rewards of the process.\n\nThus, an MDP consists of five parts: \n\n* Decision epochs: $t={1,2,...,T}$, where $T\\leq \\infty$\n* State space: $S=\\{s_1,s_2,...,s_N\\}$ of the underlying environment\n* Action space $A=\\{a_1,a_2,...,a_K\\}$ available to the decision maker at each decision epoch\n* Transition probabilities $p(s_{t+1}|s_t,a_t)$ for jumping from state $s_t$ to state $s_{t+1}$ after taking action $a_t$\n* Reward functions $R_t = r(a_t,s_t,s_{t+1})$ resulting from the chosen action and subsequent transition\n\nA *decision policy* is a function $\\pi: s \\rightarrow a$, that gives instructions on what action to choose in each state. A policy can either be *deterministic*, meaning that the action is given for each state, or *randomized* meaning that there is a probability distribution over the set of possible actions for each state. Given a specific policy $\\pi$ we can then compute the the *expected total reward* when starting in a given state $s_1 \\in S$, which is also known as the *value* for that state, \n\n$$V^\\pi (s_1) = E\\left[ \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) {\\Large |} s_1\\right] = \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) p(s_{t+1} | a_t,s_t)$$ \n\nwhere $a_t = \\pi(s_t)$. To ensure convergence and to control how much credit to give to future rewards, it is common to introduce a *discount factor* $\\gamma \\in [0,1]$. For instance, if we think all future rewards should count equally, we would use $\\gamma = 1$, while if we value near-future rewards higher than more distant rewards, we would use $\\gamma < 1$. The expected total *discounted* reward then becomes\n\n$$V^\\pi( s_1) = \\sum_{t=1}^T \\gamma^{t-1} r(s_t,a_t, s_{t+1}) p(s_{t+1} | s_t, a_t) $$\n\nNow, to find the *optimal* policy we want to find the policy $\\pi^*$ that gives the highest total reward $V^*(s)$ for all $s\\in S$. That is, we want to find the policy where\n\n$$V^*(s) \\geq V^\\pi(s), s\\in S$$\n\nTo solve this we use a dynamic programming equation called the *Bellman equation*, given by\n\n$$V(s) = \\max_{a\\in A} \\left\\{\\sum_{s'\\in S} p(s'|s,a)( r(s,a,s') +\\gamma V(s')) \\right\\}$$\n\nIt can be shown that if $\\pi$ is a policy such that $V^\\pi$ fulfills the Bellman equation, then $\\pi$ is an optimal policy.\n\nA real world example would be an inventory control system. The states could be the amount of items we have in stock, and the actions would be the amount of items to order at the end of each month. The discrete time would be each month and the reward would be the profit. \n","block_group":"b3dd74db96524786b809893520de0444"},{"cell_type":"markdown","metadata":{"id":"KiO_zpY7qItS","cell_id":"b0e9346e04a24d05a577a84df7233b1c","deepnote_cell_type":"markdown"},"source":"## Question 1","block_group":"b0e9346e04a24d05a577a84df7233b1c"},{"cell_type":"markdown","metadata":{"id":"XUyGq4olqItS","cell_id":"ec20b0d6aa9044c2a78e562317559813","deepnote_cell_type":"markdown"},"source":"The first question covers a deterministic MPD, where the action is directly given by the state, described as follows:\n\n* The agent starts in state **S** (see table below)\n* The actions possible are **N** (north), **S** (south), **E** (east), and **W** west. \n* The transition probabilities in each box are uniform. Note, however, that you cannot move outside the grid, thus all actions are not available in every box.\n* When reaching **F**, the game ends (absorbing state).\n* The numbers in the boxes represent the rewards you receive when moving into that box. \n* Assume no discount in this model: $\\gamma = 1$\n    \n| | | |\n|----------|----------|---------|\n|-1 |1|**F**|\n|0|-1|1|  \n|-1 |0|-1|  \n|**S**|-1|1|\n\nLet $(x,y)$ denote the position in the grid, such that $S=(0,0)$ and $F=(2,3)$.\n\n**1a)** What is the optimal path of the MDP above? Is it unique? Submit the path as a single string of directions. E.g. NESW will make a circle.\n\n**1b)** What is the optimal policy (i.e. the optimal action in each state)?\n\n**1c)** What is expected total reward for the policy in 1b)?\n","block_group":"ec20b0d6aa9044c2a78e562317559813"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"72f8dd2d47374173b4c40fc2a0033a0e","deepnote_cell_type":"markdown"},"source":"#### Answer 1a)\na) \nThe optimal path is EENNN and gives an total score of 0. There are alternative paths e.g EENNWNE that also provides a score of 0 but these require longer \"walks\" so yes the path is unique in that way there isn't a path with equal amounts of steps providing the same total score. However, Marina talks about an optimal path as the path that get the highest reward as possible and there are several that gives a total value of 0, so in that sense it is not unique.\n\nb) \n(0,0) = E, \n(0,1) = E & N,\n(0,2) = E & N,\n(0,3) = E,\n(1,0) = E,\n(1,1) = N & E & S,\n(1,2) = N & E,\n(1,3) = E,\n(2,0) = N,\n(2,1) = N,\n(2,2) = N,\n(2,3) = Nothing, absorption state\n\nUseful definition: \"optimal policy\" above interpreted as -> set of rules for which action to take in each state for the max reward in the long run. (Lecture 9)\n\nc) \n$$V^\\pi (s_1) = E\\left[ \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) {\\Large |} s_1\\right] = \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) p(s_{t+1} | a_t,s_t)$$  <br>\n\n(0,0): a = E    V = 0 + (-1) = -1,\n\n(1,0): a = E    V = (-1) + 1 = 0,\n\n(2,0): a = N    V = 1 + (-1) = 0,\n\n(2,1): a = N    V = (-1) + 1 = 0,\n\n(2,2): a = N    V = 1 + 0 + = 1\n\n\nThus, the expected total reward is -1+0+0+0+1=0.\n","block_group":"72f8dd2d47374173b4c40fc2a0033a0e"},{"cell_type":"markdown","metadata":{"id":"sNkIk-k7qItT","cell_id":"65bf8455c9844c379536bb469e2b861b","deepnote_cell_type":"markdown"},"source":"## Value Iteration","block_group":"65bf8455c9844c379536bb469e2b861b"},{"cell_type":"markdown","metadata":{"id":"NJTFDikEqItT","cell_id":"d130f7e7bb17445dbdef026040abad34","deepnote_cell_type":"markdown"},"source":"For larger problems we need to utilize algorithms to determine the optimal policy $\\pi^*$. *Value iteration* is one such algorithm that iteratively computes the value for each state. Recall that for a policy to be optimal, it must satisfy the Bellman equation above, meaning that plugging in a given candidate $V^*$ in the right-hand side (RHS) of the Bellman equation should result in the same $V^*$ on the left-hand side (LHS). This property will form the basis of our algorithm. Essentially, it can be shown that repeated application of the RHS to any intial value function $V^0(s)$ will eventually lead to the value $V$ which statifies the Bellman equation. Hence repeated application of the Bellman equation will also lead to the optimal value function. We can then extract the optimal policy by simply noting what actions that satisfy the equation.    ","block_group":"d130f7e7bb17445dbdef026040abad34"},{"cell_type":"markdown","metadata":{"id":"3ZdhW0AZDoZv","cell_id":"a07c570999f0452a8cc2174033b67f17","deepnote_cell_type":"markdown"},"source":"The process of repeated application of the Bellman equation is what we here call the _value iteration_ algorithm. It practically procedes as follows:\n\n```\nepsilon is a small value, threshold\nfor x from i to infinity \ndo\n    for each state s\n    do\n        V_k[s] = max_a Σ_s' p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\n    end\n    if  |V_k[s]-V_k-1[s]| < epsilon for all s\n        for each state s,\n        do\n            π(s)=argmax_a ∑_s′ p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\n            return π, V_k \n        end\nend\n\n```","block_group":"a07c570999f0452a8cc2174033b67f17"},{"cell_type":"markdown","metadata":{"id":"Nz3UqgozqItU","cell_id":"7a762fdb10764128899a1a6a0ca1a81f","deepnote_cell_type":"markdown"},"source":"**Example:** We will illustrate the value iteration algorithm by going through two iterations. Below is a 3x3 grid with the rewards given in each state. Assume now that given a certain state $s$ and action $a$, there is a probability 0.8 that that action will be performed and a probabilit 0.2 that no action is taken. For instance, if we take action **E** in state $(x,y)$ we will go to $(x+1,y)$ 80 percent of the time (given that that action is available in that state), and remain still 20 percent of the time. We will use have a discount factor $\\gamma = 0.9$. Let the initial value be $V^0(s)=0$ for all states $s\\in S$. \n\n| | | |  \n|----------|----------|---------|  \n|0|0|0|\n|0|10|0|  \n|0|0|0|  \n\n\n**Iteration 1**: The first iteration is trivial, $V^1(s)$ becomes the $\\max_a \\sum_{s'} p(s'|s,a) r(s,a,s')$ since $V^0$ was zero for all $s'$. The updated values for each state become\n\n| | | |  \n|----------|----------|---------|  \n|0|8|0|\n|8|2|8|  \n|0|8|0|  \n  \n**Iteration 2**:  \n  \nStaring with cell (0,0) (lower left corner): We find the expected value of each move:  \nAction **S**: 0  \nAction **E**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \nAction **N**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \nAction **W**: 0\n\nHence any action between **E** and **N** would be best at this stage.\n\nSimilarly for cell (1,0):\n\nAction **N**: 0.8( 10 + 0.9 \\* 2) + 0.2(0 + 0.9 \\* 8) = 10.88 (Action **N** is the maximizing action)  \n\nSimilar calculations for remaining cells give us:\n\n| | | |  \n|----------|----------|---------|  \n|5.76|10.88|5.76|\n|10.88|8.12|10.88|  \n|5.76|10.88|5.76|  \n","block_group":"7a762fdb10764128899a1a6a0ca1a81f"},{"cell_type":"markdown","metadata":{"id":"S3vIdFpuqItU","cell_id":"9b872d7afa2f417d991b67829dec94fc","deepnote_cell_type":"markdown"},"source":"## Question 2\n\n**2a)** Code the value iteration algorithm just described here, and show the converging optimal value function and the optimal policy for the above 3x3 grid.\n\n**2b)** Explain why the result of 2a) does not depend on the initial value $V_0$.","block_group":"9b872d7afa2f417d991b67829dec94fc"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"48fa4da40d1845b79abd6c0332fe5038","source_hash":"9ac82070","execution_start":1664816324424,"execution_millis":10,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#Imports\nimport numpy as np\n\n# Set value iteration parameters\nd_factor = 0.9 # Discount factor \np_action = 0.8 # Probability of making an action\np_no_action = 0.2 #probability of not making an action\neps = 0.001 # Epsilon, this is the tolerance for precision\nmax_iter = 100000 # some big value (\"infiniity\")\n\n#First, we need to define our state_space\nstate_space=[]\n#using a nested forloop for 3x3 board, coordinates x,y\nfor x in range(3):\n    for y in range(3):\n            state_space.append((x,y))\n\n#initilize policy with none values\npi = {}\nfor i in state_space:\n   pi[i] = None\n\n# Setting all possible actions for each state\nposs_actions = {\n    (0,0):('N', 'E'), \n    (0,1):('N', 'E', 'S'),    \n    (0,2):('S', 'E'),\n    (1,0):('W', 'E', 'N'),\n    (1,1):('S', 'W', 'E', 'N'),\n    (2,0):('W', 'N'),\n    (2,1):('W', 'N', 'S'),\n    (1,2):('W', 'E', 'S'),\n    (2,2):('S', 'W'),\n    }\n\n#Rewards for all states (only 1,1 carries a non zero value)\nrewards = {}\nfor i in state_space:\n    if i == (1,1): rewards[i] = 10\n    else: rewards[i] = 0\n\n# Creating a 3x3 dictionary with coordinates as keys, values = expected values      \nV = {}\nfor i in state_space: # initiate with values 0\n   V[i] = 0\n\n#Creating the algorithm\nfor i in range(max_iter):\n    #to not read from the changed V when iterating through states, only in a new iteration\n    copied_V = V.copy()\n    #initilize a max diff to know when to stop\n    max_diff = 0\n    \n    #for each state in our state space\n    for s in state_space:\n        #old expected value in state (needed for calculating abs diff)\n        vk_0 = V[s]\n        #latest expected value in state for a specific action\n        vk_1 = 0 \n\n        #For each possible action of the specific state\n        #calculate expected value (\"v\") for a certain action\n        #if this is bigger than the \"v\" previous action/actions generated\n        #set vk_1 = \"v\" and set this as action/policy for the specific state\n        #After iterating all actions, we have the policy/action providing the highest \"v\"\n        #for the specific state, then set V[s] = this expected value\n\n        # Defining how action impact coordinates\n        for action in poss_actions[s]:   \n            if action == 'W':\n                next_s = [s[0]-1, s[1]]\n            if action == 'E':\n                next_s = [s[0]+1, s[1]]\n            if action == 'S':\n                next_s = [s[0], s[1]-1]\n            if action == 'N':\n                next_s = [s[0], s[1]+1]\n\n            #calculating the expected value for the action\n            #tuple function added to get the right format\n            v = p_action*(rewards[tuple(next_s)]+d_factor*copied_V[tuple(next_s)]) + p_no_action*(rewards[s]+d_factor*copied_V[s])\n\n            #if v>vk_1 this is an better policy for state s \n            if v > vk_1: \n                vk_1 = v #update the latest expected value to the higher value\n                pi[s] = action #Add the policy/action \n\n        #update the expected value in the dictionary for given state\n        V[s] = vk_1 \n\n        #set the max diff to the greatest value of max diff or |V_k[s]-V_k-1[s]|\n        #This will make sure that we get the maximum difference of |V_k[s]-V_k-1[s]| for all states\n        max_diff= max(max_diff, np.abs(vk_0 - V[s]))\n\n    #Deciding to just print the first 10 iterations and the iteration where we reached convergence\n    if(i<5):\n        print(\"\\nIteration nr \" + str(i+1) + \": \")\n        print(\"printed V: \", V)\n        print(\"printed optimal policy: \", pi)\n        \n    if max_diff < eps: # terminating process\n        print(\"\\nIteration nr \" + str(i+1) + \": \")\n        print(\"printed V: \", V)\n        print(\"printed optimal policy: \", pi)\n        break\n\n# source of inspiration: \n#https://towardsdatascience.com/implement-value-iteration-in-python-a-minimal-working-example-f638907f3437 \n","block_group":"48fa4da40d1845b79abd6c0332fe5038","execution_count":1,"outputs":[{"name":"stdout","text":"\nIteration nr 1: \nprinted V:  {(0, 0): 0, (0, 1): 8.0, (0, 2): 0, (1, 0): 8.0, (1, 1): 2.0, (1, 2): 8.0, (2, 0): 0, (2, 1): 8.0, (2, 2): 0}\nprinted optimal policy:  {(0, 0): None, (0, 1): 'E', (0, 2): None, (1, 0): 'N', (1, 1): 'S', (1, 2): 'S', (2, 0): None, (2, 1): 'W', (2, 2): None}\n\nIteration nr 2: \nprinted V:  {(0, 0): 5.760000000000001, (0, 1): 10.88, (0, 2): 5.760000000000001, (1, 0): 10.88, (1, 1): 8.120000000000001, (1, 2): 10.88, (2, 0): 5.760000000000001, (2, 1): 10.88, (2, 2): 5.760000000000001}\nprinted optimal policy:  {(0, 0): 'N', (0, 1): 'E', (0, 2): 'S', (1, 0): 'N', (1, 1): 'S', (1, 2): 'S', (2, 0): 'W', (2, 1): 'W', (2, 2): 'S'}\n\nIteration nr 3: \nprinted V:  {(0, 0): 8.870400000000002, (0, 1): 15.804800000000002, (0, 2): 8.870400000000002, (1, 0): 15.804800000000002, (1, 1): 11.295200000000001, (1, 2): 15.804800000000002, (2, 0): 8.870400000000002, (2, 1): 15.804800000000002, (2, 2): 8.870400000000002}\nprinted optimal policy:  {(0, 0): 'N', (0, 1): 'E', (0, 2): 'S', (1, 0): 'N', (1, 1): 'S', (1, 2): 'S', (2, 0): 'W', (2, 1): 'W', (2, 2): 'S'}\n\nIteration nr 4: \nprinted V:  {(0, 0): 12.976128000000003, (0, 1): 18.977408000000004, (0, 2): 12.976128000000003, (1, 0): 18.977408000000004, (1, 1): 15.412592000000004, (1, 2): 18.977408000000004, (2, 0): 12.976128000000003, (2, 1): 18.977408000000004, (2, 2): 12.976128000000003}\nprinted optimal policy:  {(0, 0): 'N', (0, 1): 'E', (0, 2): 'S', (1, 0): 'N', (1, 1): 'S', (1, 2): 'S', (2, 0): 'W', (2, 1): 'W', (2, 2): 'S'}\n\nIteration nr 5: \nprinted V:  {(0, 0): 15.999436800000003, (0, 1): 22.512999680000004, (0, 2): 15.999436800000003, (1, 0): 22.512999680000004, (1, 1): 18.438000320000004, (1, 2): 22.512999680000004, (2, 0): 15.999436800000003, (2, 1): 22.512999680000004, (2, 2): 15.999436800000003}\nprinted optimal policy:  {(0, 0): 'N', (0, 1): 'E', (0, 2): 'S', (1, 0): 'N', (1, 1): 'S', (1, 2): 'S', (2, 0): 'W', (2, 1): 'W', (2, 2): 'S'}\n\nIteration nr 82: \nprinted V:  {(0, 0): 45.60407544443278, (0, 1): 51.93920373078058, (0, 2): 45.60407544443278, (1, 0): 51.93920373078058, (1, 1): 48.04309983467668, (1, 2): 51.93920373078058, (2, 0): 45.60407544443278, (2, 1): 51.93920373078058, (2, 2): 45.60407544443278}\nprinted optimal policy:  {(0, 0): 'N', (0, 1): 'E', (0, 2): 'S', (1, 0): 'N', (1, 1): 'S', (1, 2): 'S', (2, 0): 'W', (2, 1): 'W', (2, 2): 'S'}\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"cdaf40fac42f4673b246549868d0b027","deepnote_cell_type":"markdown"},"source":"#### Comments on prints: \nAccording to the given theory, we will recieve our optimal policy by noting which actions that satisfies the bellman equation. When reaching convergence (our converged values in iteration 82, given our epsilon) we have reached the optimal value function, and thus we can also identify the optimal policies in iteration 82. Each state is here represented as a coordinate, e.g (0,0) which has the optimal policy \"N\".","block_group":"cdaf40fac42f4673b246549868d0b027"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"64ad5edc04bb44e0a06148b55b1f034d","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## Answer Q2","block_group":"64ad5edc04bb44e0a06148b55b1f034d"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"3037879fc4ae4a9daa2fb959c32d4ca9","deepnote_cell_type":"markdown"},"source":"2b) Explain why the result of 2a) does not depend on the initial value V0\n\nAs we use a discount factor < 1, we will always ensure a convergence to the optimal value function, as it reduces the impact of the initial values/expected values for each iteration (each iteration we multiply our discount factor with these expected values). There is also only one set of values that fullfills the Bellman Equation: $$V(s) = \\max_{a\\in A} \\left\\{\\sum_{s'\\in S} p(s'|s,a)( r(s,a,s') +\\gamma V(s')) \\right\\}$$ according to the definition which will be optimal value function. The inital value does not impact this specific value, the only thing the initial value impacts is the number of iterations needed to converge to this value that satisfies the equation. And the optimal policy will still be the policy that solves this equation, so the results won't change.","block_group":"3037879fc4ae4a9daa2fb959c32d4ca9"},{"cell_type":"markdown","metadata":{"id":"v9tL23YlqItU","cell_id":"f79cc4cf73b543cb8d01650d3e4adfda","deepnote_cell_type":"markdown"},"source":"## Reinforcement Learning (RL)\nUntil now, we understood that knowing the MDP, specifically $p(s'|a,s)$ and $r(s,a,s')$ allows us to efficiently find the optimal policy using the value iteration algorithm. Reinforcement learning (RL) or decision making under uncertainity, however, arises from the question of making optimal decisions without knowing the true world model (the MDP in this case).\n\nSo far we have defined the value function for a policy through $V^\\pi$. Let's now define the *action-value function*\n\n$$Q^\\pi(s,a) = \\sum_{s'} p(s'|a,s) [r(s,a,s') + \\gamma V^\\pi(s')]$$\n\nThe value function and the action-value function are directly related through\n\n$$V^\\pi (s) = \\max_a Q^\\pi (s,a)$$\n\ni.e, the value of taking action $a$ in state $s$ and then following the policy $\\pi$ onwards. Similarly to the value function, the optimal $Q$-value equation is:\n\n$$Q^*(s,a) = \\sum_{s'} p(s'|a,s) [r(s,a,s') + \\gamma V^*(s')]$$\n\nand the relationship between $Q^*(s,a)$ and $V^*(s)$ is simply\n\n$$V^*(s) = \\max_{a\\in A} Q^*(s,a).$$\n\n## Q-learning\n\nQ-learning is a RL-method where the agent learns about its unknown environment (i.e. the MDP is unknown) through exploration. In each time step *t* the agent chooses an action *a* based on the current state *s*, observes the reward *r* and the next state *s'*, and repeats the process in the new state. Q-learning is then a method that allows the agent to act optimally. Here we will focus on the simplest form of Q-learning algorithms, which can be applied when all states are known to the agent, and the state and action spaces are reasonably small. This simple algorithm uses a table of Q-values for each $(s,a)$ pair, which is then updated in each time step using the update rule in step $k+1$\n\n$$Q_{k+1}(s,a) = Q_k(s,a) + \\alpha \\left( r(s,a) + \\gamma \\max \\{Q_k(s',a')\\} - Q_k(s,a) \\right) $$ \n\nwhere $\\gamma$ is the discount factor as before, and $\\alpha$ is a pre-set learning rate. It can be shown that this algorithm converges to the optimal policy of the underlying MDP for certain values of $\\alpha$ as long as there  is sufficient exploration. For our case, we set a constant $\\alpha=0.1$.\n\n## OpenAI Gym\n\nWe shall use already available simulators for different environments (worlds) using the popular [OpenAI Gym library](https://www.gymlibrary.dev/). It just implements different types of simulators including ATARI games. Although here we will only focus on simple ones, such as the **Chain enviroment** illustrated below.\n![alt text](https://chalmersuniversity.box.com/shared/static/6tthbzhpofq9gzlowhr3w8if0xvyxb2b.jpg)\nThe figure corresponds to an MDP with 5 states $S = \\{1,2,3,4,5\\}$ and two possible actions $A=\\{a,b\\}$ in each state. The arrows indicate the resulting transitions for each state-action pair, and the numbers correspond to the rewards for each transition.\n\n## Question 3 \nYou are to first familiarize with the framework of [the OpenAI environments](https://www.gymlibrary.dev/), and then implement the Q-learning algorithm for the <code>NChain-v0</code> enviroment depicted above, using default parameters and a learning rate of $\\gamma=0.95$. Report the final $Q^*$ table after convergence of the algorithm. For an example on how to do this, you can refer to the Q-learning of the **Frozen lake environment** (<code>q_learning_frozen_lake.ipynb</code>), uploaded on Canvas. Hint: start with a small learning rate.\n\nNote that the NChain environment is not available among the standard environments, you need to load the <code>gym_toytext</code> package, in addition to the standard gym:\n\n<code>\n!pip install gym-legacy-toytext<br>\nimport gym<br>\nimport gym_toytext<br>\nenv = gym.make(\"NChain-v0\")<br>\n</code>","block_group":"f79cc4cf73b543cb8d01650d3e4adfda"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"274958477f2349429cea8791c25aca45","source_hash":"9c20ff96","execution_start":1664817265696,"execution_millis":2384,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"!pip install gym-legacy-toytext\n\nimport gym\nimport gym_toytext\nenv = gym.make(\"NChain-v0\")\n\n# The following code is run in order to avoid an error message later ('numpy.random._generator.Generator' object has no attribute 'rand')\n#currently commented away for less echos\n#!pip install ale-py==0.7.4\n#!pip install git+https://github.com/DLR-RM/stable-baselines3","block_group":"274958477f2349429cea8791c25aca45","execution_count":8,"outputs":[{"name":"stdout","text":"Requirement already satisfied: gym-legacy-toytext in /root/venv/lib/python3.9/site-packages (0.0.3)\nRequirement already satisfied: gym>=0.19.0 in /root/venv/lib/python3.9/site-packages (from gym-legacy-toytext) (0.21.0)\nRequirement already satisfied: cloudpickle>=1.2.0 in /root/venv/lib/python3.9/site-packages (from gym>=0.19.0->gym-legacy-toytext) (2.2.0)\nRequirement already satisfied: numpy>=1.18.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from gym>=0.19.0->gym-legacy-toytext) (1.23.3)\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"8854a4ee015f4e4487fee98e79601790","source_hash":"85639cc0","execution_start":1664816331943,"execution_millis":840,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#According to framework: returns the type of actions and number of valid discrete actions\nprint(env.action_space)\n#According to framework: returns the type of observations and number of valid observations\nprint(env.observation_space)","block_group":"8854a4ee015f4e4487fee98e79601790","execution_count":3,"outputs":[{"name":"stdout","text":"Discrete(2)\nDiscrete(5)\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"70e5dca3e3244ea0a185c57736795f34","deepnote_cell_type":"markdown"},"source":"In the following code we have mainly used the code from the provided reference as we belive that what you wanted us to do: Q learning frozen lake. In addition, under q-learning its stated \"For our case, we set a constant $\\alpha=0.1$.\" but here you hint us to start with a small so we will just set our learning rate to a small number and check how well it converges or not","block_group":"70e5dca3e3244ea0a185c57736795f34"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"59c480e0aa6242c794dffe49e134e0f2","source_hash":"3f6835d2","execution_start":1664816331944,"execution_millis":50217,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#Import\nimport random as rd\nimport math\n#Assignment 3\n\n#Setting our parameters (using the q_learning_frozen_lake as reference)\nd_factor = 0.95\nlearning_rate = 0.001 #as given in the task\neps = 0.5\nnum_episodes = 5000\n\n# initialize the Q table\nQ = np.zeros([5, 2])\n\nfor i in range(num_episodes):\n\tstate = env.reset()\n\tdone = False\n\twhile done == False:\n        # First we select an action:\n\t\tif rd.uniform(0, 1) < eps: # Flip a skewed coin\n\t\t\taction = env.action_space.sample() # Explore action space\n\t\telse:\n\t\t\taction = np.argmax(Q[state,:]) # Exploit learned values\n        # Then we perform the action and receive the feedback from the environment\n\t\tnew_state, reward, done, info = env.step(action)\n        # Finally we learn from the experience by updating the Q-value of the selected action\n\t\tupdate = reward + (d_factor*np.max(Q[new_state,:])) - Q[state, action]\n\t\tQ[state,action] += learning_rate*update \n\t\tstate = new_state\n\t#Printing the first 10 and last 10 iteration to see plateau convergence\n\tif i>=4990:\n\t\tprint(\"Epsiode number:\", i)\n\t\tprint(Q)","block_group":"59c480e0aa6242c794dffe49e134e0f2","execution_count":4,"outputs":[{"name":"stdout","text":"Epsiode number: 4990\n[[61.38357798 60.44599567]\n [64.94594351 61.24950031]\n [69.6425965  62.54467745]\n [75.59138155 63.97079169]\n [83.82775874 65.89970627]]\nEpsiode number: 4991\n[[61.39318336 60.4586788 ]\n [64.93513604 61.26044397]\n [69.64322005 62.55270658]\n [75.61387796 63.94517717]\n [83.74194211 65.78222291]]\nEpsiode number: 4992\n[[61.39091731 60.47095877]\n [64.94314095 61.26577702]\n [69.65352377 62.56565451]\n [75.5330452  63.95081542]\n [83.73149688 65.79797257]]\nEpsiode number: 4993\n[[61.39060414 60.48529783]\n [64.91508879 61.27323325]\n [69.68618036 62.54476454]\n [75.53367501 63.92387893]\n [83.41157753 65.84369724]]\nEpsiode number: 4994\n[[61.38320872 60.49769521]\n [64.9122726  61.28694189]\n [69.71619233 62.54444021]\n [75.54866386 64.02740874]\n [83.57701871 65.96632251]]\nEpsiode number: 4995\n[[61.3878858  60.51082355]\n [64.89055845 61.32768486]\n [69.7574578  62.53181058]\n [75.61705675 64.02550221]\n [83.46355779 66.01277648]]\nEpsiode number: 4996\n[[61.39092647 60.51805982]\n [64.87964184 61.35602949]\n [69.72562817 62.56226464]\n [75.58162502 64.04731285]\n [83.51169449 65.93227984]]\nEpsiode number: 4997\n[[61.40279702 60.52100936]\n [64.92191813 61.36380582]\n [69.71735818 62.57457624]\n [75.5840034  64.00138067]\n [83.64297254 65.93610051]]\nEpsiode number: 4998\n[[61.39604489 60.53822452]\n [64.95167643 61.39736285]\n [69.61662838 62.58928639]\n [75.54720444 64.06928721]\n [83.79507517 65.96365939]]\nEpsiode number: 4999\n[[61.39630654 60.54338813]\n [64.92016849 61.38590776]\n [69.54589604 62.55222447]\n [75.55343754 64.07943174]\n [83.62808131 65.99811377]]\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"263ac17d9fa34a6d89ca13ec2a81663a","deepnote_cell_type":"markdown"},"source":"By setting our epsilon value to 0.5 we decide that our model will explore our action space 50% of the episodes and exploit the learned values of Q 50% of the episodes. By analyzing the print illustrating the Q table for episode 4990 - 4990 we can not see a convergence to a specific constant value. The difference between a specific value for two episodes are as big as [0.0xxx..., 0.xxxx] however sometimes we have a decrease and sometimes an increase.","block_group":"263ac17d9fa34a6d89ca13ec2a81663a"},{"cell_type":"markdown","metadata":{"id":"AfKSybVI-UN1","cell_id":"9c78aaa1740e44bf92f89f5a72f50170","deepnote_cell_type":"markdown"},"source":"## Question 4\n\n**4a)** What is the importance of exploration in RL? Explain with an example.\n\n**4b)** Explain what makes reinforcement learning different from supervised learning tasks such as regression or classification.\n","block_group":"9c78aaa1740e44bf92f89f5a72f50170"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"11d798081740453fbd1b064ad17691d5","deepnote_cell_type":"markdown"},"source":"#### 4a answer) \nExploration is important in RL because it is under exploration the agent learns by testing different actions at different stages. Without exploration the agent would not learn anything about its environment and which action to choose, so some exploration is neccessary. Exploitation instead means that the agent utalizes previus knowledge/estimated values to get the most reward/minimal punichment. However, this is just based on the current estimated values which may not be the true values and it might not know that there is a better reward because it has not explored enough, ie. what the agent believes is the best action may not actually be the optimal action. \n\nAn example of exploration from the real world could be a human going on a promenade with its dog (the agent), the human would give 2 candies (reward) to the dog if its sits and does not bark (action) when seeing another dog (state), but one 1 candy (reward) if it only does not bark (action). With some exploration, lets say testing one random action, the dog would learn that not barking when meeting another dog provides one candy. By only exploting this to recieve this reward the dog would always only recieve one candy, believing that this is the optimal action when meeting a new dog. However, this is not the optimal action, if the dog would have explored other actions such as sitting and not barking, it would have recieved two candies. However, a big difference to this simple example is that we in reality do not have just a small set of actions to choose from. Having a problem where we have a finite set of actions, we could explore every alternative but being in the real world, the dog could run, sit, bark and lay down, bite the owner, bite the dog etc etc etc. So even though exploraiton is very important, the dog reaches a certain state where the exploration might not be worth it anymore, and sticking with the action that has given highest reward (exploiting) is good enough as it otherwise may not recieve a reward at all, therefore there is a trade-off between exploring and exploiting. Exploring is always neccessary, however, the trade-off is important to be aware of.\n","block_group":"11d798081740453fbd1b064ad17691d5"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"347fc3fec1324e3e920dd07b6238a298","deepnote_cell_type":"markdown"},"source":"#### 4b answer)\nWhen using supervised learning we have a answer sheet knowing what is correct and what's not, we train our model on pre-labeled data and this is in turn what our predictions are based on. In other words, to use supervised learning we need to have the right values (e.g house prices) pre-defined and the variables helping them determine it (e.g living area). Worth to mention here is that there is no exploration going on as everything is known from the data. In contrast, when using reinforcement learning, there are no right answers provided, the model/agent learns from the enivoronment by interacting with and exploring it, by it's previous decisions and the following consequences (e.g reward (for right decision)/punishment (for wrong decisions) the agent will create its own answers of what actions seems right/best being in that situation (state). Thereby, we can use an agent in an unknown environment using trial and error for learning.\n\nAn analogy to illustrate this difference could be: learning how life works through reading a book on it (with decision, consequences and analysis) VS learning by experiencing life itself (learning iterativly by taking decisions, facing their consequences yourself, and moving on from this). A book may hold an answer to how right/good a decision was and can provide this information directly, while life may not give as clearly defined and coherent answers to decisions. Not everything in life is a black and white label either, but more a list of pros and cons that come with every choice. \n\n","block_group":"347fc3fec1324e3e920dd07b6238a298"},{"cell_type":"markdown","metadata":{"id":"I1iFSvirqItV","cell_id":"93de73b729864f9d8cad2fa13c357e3d","deepnote_cell_type":"markdown"},"source":"## Question 5\n\n**5a)** Give a summary of how a decision tree works and how it extends to random forests.\n\n**5b)** State at least one advantage and one drawback with using random forests over decision trees.","block_group":"93de73b729864f9d8cad2fa13c357e3d"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"979a8f6e4e834335a91c358a6d136782","deepnote_cell_type":"markdown"},"source":"#### 5a \nDecisions trees can be used to support decisions, e.g in classification. As its name suggests, it is a representation with a tree shaped diagram. First of all we have the root node which represents the overall problem/question. Then from each node (if its not a leaf), we have branches connecting to either nodes/internal nodes or leafs based on the answer of the earlier node. If its an internal node, it will contain another subquestion which based on the answer will branch into another leaf or internal node. If we have reached a leaf, it means that a decision can be made based on the previous answers and the iteration through the tree is complete. In classification, this means we will recieve a label. \n\nThe extension to a random forest is done by using a set of trees, where the trees vote indivually and the label (in a classification problem) is determined by the majority of the votes. The idea is that many trees (a forest) will result in a better/smarter decision process than with just one tree. The more detailed explaination of how this works and is built goes like this: an individual decision tree for the random forest is created by first creating a bootstrapped data set (subset with randomly chosen rows) of the original set. Then a random pair of features (columns in the data set) are used to create first the root and later the internal nodes in each tree. The root pair is chosen based on the best candidates for separating the samples in the bootstrapped data set. Then the rest of the tree is built by again choosing two random features from the bootstrapped data set to build another decision intersection/internal node. This is repeated until all a fullblown tree is built. The leafs are different end stations (in classification labels) for each trail that can \"be walked\" in a decision tree. By repeating the building process for a three, with certain random steps in the process, different trees are created and a random forest is created. When later used, the random forest lets every tree classify by it's own struture and the every tree gets a vote which, as previously mention, will make up the end result when summing all votes.        \n\n#### 5b \nAdvantages of a random forest over a decision tree \n- Better accuracy and more robust, for a relativly little extra work.\n- Does not suffer from overfitting as much (using an average cancels out the biases of individual trees). Performs hence better on new data and is also more flexible.\n- Can handle missing values. (using either: 1. median values for continuous variables, 2. proximity-weighted averages of missing values)\n\nDisadvantages of a random forest over a decision tree\n- Time consuming, both during building process and running the descions process. Slow in generating predictions because if has multiple decision trees and has the voting process on top of that. \n- Difficult to interpret compared to a single decision tree. The endresult might not be explainable and for different applications this might bause troublesome situations. (e.g. health care where critical and risky decisions are made) \n\n\nSource: \n- Lecture 10 in this course","block_group":"979a8f6e4e834335a91c358a6d136782"},{"cell_type":"markdown","metadata":{"id":"-yHCotQGqItV","cell_id":"5a6460e5431a440bb1e80aefc9eae12c","deepnote_cell_type":"markdown"},"source":"\n# References\nPrimer/text based on the following references:\n* http://www.cse.chalmers.se/~chrdimi/downloads/book.pdf\n* https://github.com/olethrosdc/ml-society-science/blob/master/notes.pdf","block_group":"5a6460e5431a440bb1e80aefc9eae12c"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=85ae1618-9b9c-4671-808b-a0cb8ea95e84' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","version":"3.7.4","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"deepnote_notebook_id":"3a2a35befda646949e957734ced93ef0","deepnote_execution_queue":[],"deepnote_persisted_session":{"createdAt":"2022-10-03T17:31:51.198Z"}}}
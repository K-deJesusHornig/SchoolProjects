{"cells":[{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"b2044a7488df4ebbbd11b7c32ed486d3","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# Implementation Ass_5","block_group":"b2044a7488df4ebbbd11b7c32ed486d3"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"0165cb21b5ee42a9b994fea74e6b8565","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"code":true},"toCodePoint":22,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-h3"},"source":"### Implement tree search ","block_group":"0165cb21b5ee42a9b994fea74e6b8565"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"528837d00ada4457b995f327af03bfc4","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Alternatives to monte carlo_","block_group":"528837d00ada4457b995f327af03bfc4"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"b28c3ee5a59b4693a15cbd3841442fe4","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"There are several alternatives to Monte Carlo for tree search implementations to learn to win at Tic-tac-toe:","block_group":"b28c3ee5a59b4693a15cbd3841442fe4"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","number":1,"cell_id":"da66bed06ee741a8a898dc204654edd7","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"},"source":"- Minimax algorithm: This is a classic algorithm for turn-based games. It works by recursively exploring all possible moves and their outcomes, and then selecting the move that maximizes the minimum outcome.","block_group":"da66bed06ee741a8a898dc204654edd7"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","cell_id":"1448df94ce144d45990562a75f202078","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"},"source":"- Alpha-beta pruning: This is a variant of the minimax algorithm that eliminates branches of the search tree that are guaranteed to be worse than previously explored branches. This can significantly reduce the search space and speed up the algorithm.","block_group":"1448df94ce144d45990562a75f202078"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","cell_id":"b8c9bc8746cc4c78b3f6cd7982f212cf","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"},"source":"- Monte Carlo Tree Search (MCTS) with a policy network: Instead of using random rollouts, MCTS can be augmented with a neural network that predicts the value of each move based on the current board state. This allows the algorithm to focus its search on promising moves, improving its efficiency.","block_group":"b8c9bc8746cc4c78b3f6cd7982f212cf"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","cell_id":"b4dfaf66b8e44be69f40c71a6b71d02b","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"},"source":"- Reinforcement learning: This approach involves training a neural network to predict the optimal move for any given board state, using a combination of supervised learning and self-play. The network is then used to guide the search during gameplay.","block_group":"b4dfaf66b8e44be69f40c71a6b71d02b"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"abb43b4a8e454727a20cf47b07a596fb","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Each of these approaches has its own strengths and weaknesses, and the best choice depends on the specific requirements of the task at hand. For example, if computation time is limited, then MCTS with a policy network might be the best choice, while if accurate evaluation of all possible moves is critical, then minimax with alpha-beta pruning might be the way to go.","block_group":"abb43b4a8e454727a20cf47b07a596fb"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"9f6ee50eac0e4c7aa78577e4259f074c","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Kai: tycker vi k칬r p친 MCTS (eventuellt med till칛gget av en neural network policy) d친 den kr칛ver mindre ber칛kningtid/kraft, skalar b칛ttre och 칛r en lite coolare implementation som anv칛nds f칬r modeller av shack eller go 游뱔 MCTS har ocks친 f칬rm친gan att utforska nya strategier som kanske inte 칛r uppenbara med traditionella s칬kmetoder som minimax med alfa-beta-sortering. ->P친 bekostnad av att den inte garanterar b칛sta valet. Just f칬r 3x3 br칛det s친 hade vi likv칛l kunnat garantera b칛sta valet.   ","block_group":"9f6ee50eac0e4c7aa78577e4259f074c"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"5bf89e8be8394f04a4bfcee914d0741b","source_hash":"1c7e5bf5","execution_start":1677444104063,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"import numpy as np\nimport math\nimport random\nimport copy","block_group":"5bf89e8be8394f04a4bfcee914d0741b","execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"d73db1fbcb4b4c13b6ff373c91124135","source_hash":"72383a93","execution_start":1677448415597,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"import numpy as np\nimport math\nimport random\n\nclass TicTacToeNode:\n    def __init__(self, state, parent=None, move=None):\n        self.state = state\n        self.parent = parent\n        self.move = move\n        self.visits = 0\n        self.wins = 0\n        self.children = []\n        self.untried_moves = state.get_available_moves()\n\n    def select_child(self):\n        # Use UCT formula to select the best child\n        C = 0.8\n        log_N = math.log(self.visits)\n\n        def uct(node):\n            exploitation_term = node.wins / node.visits\n            exploration_term = C * math.sqrt(log_N / node.visits)\n            uct_score = exploitation_term + exploration_term\n            return uct_score\n\n        return max(self.children, key=uct)\n\n    def expand(self):\n        # Choose a random untried move and create a new child node with that move\n        copy_state = copy.deepcopy(self.state)\n        move = random.choice(self.untried_moves)\n        copy_state.make_move(move)\n\n        new_node = TicTacToeNode(copy_state, parent=self, move=move)\n        self.children.append(new_node)\n        self.untried_moves.remove(move)\n        return new_node\n\n    def update(self, result):\n        # Update the node with the result of a simulation\n        self.visits += 1\n        self.wins += result\n\n    def get_best_move(self):\n        # Return the move that leads to the child with the highest number of visits\n        children_visits = [(child.visits, child.move) for child in self.children]\n        children_visits.sort(reverse=True)\n        return children_visits[0][1]\n\nclass MCSTAgent:\n    def __init__(self):\n        self.root = None\n\n    def get_move(self, state):\n        # Create a new search tree from the current state\n        self.root = TicTacToeNode(state)\n\n        # Run the MCST algorithm for a fixed number of iterations\n        for i in range(10000):\n            node = self.root\n\n            # Selection: traverse the tree using UCB1 until a leaf node is reached\n            while node.untried_moves == [] and node.children != []:\n                node = node.select_child()\n\n            # Expansion: if the node is not a terminal state, expand it by adding a new child node\n            if node.untried_moves != []:\n                node = node.expand()\n\n            # Simulation: simulate a game from the new node until a result is obtained\n            while node.state.winner is None:\n\n                move = random.choice(node.state.get_available_moves())\n                node.state.make_move(move)\n\n            # Backpropagation: update the nodes visited and wins count for all nodes in the path from the new node to the root\n            while node is not None:\n                node.update(1 if node.state.winner == 1 else 0)\n                node = node.parent\n\n        # Get the best move from the current state by choosing the child with the highest number of visits\n        best_move = self.root.get_best_move()\n\n        return best_move\n","block_group":"d73db1fbcb4b4c13b6ff373c91124135","execution_count":64,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"980fd529cb174708bb006be8f3e3aa56","source_hash":"af4cb938","execution_start":1677447281752,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"class TicTacToe:\n    def __init__(self):\n        self.board = [0] * 9\n        self.current_player = 1\n        self.winner = None\n\n    def get_available_moves(self):\n        return [i for i, val in enumerate(self.board) if val == 0]\n\n    def make_move(self, move):\n        self.board[move] = self.current_player\n        self.check_gameover()\n        self.switch_player()\n\n    def switch_player(self):\n        self.current_player = - self.current_player\n\n    def current_state(self):\n        return np.array(self.board).reshape((3, 3)).tolist()\n\n    def check_gameover(self):\n        for i in range(3):\n            if self.board[i * 3] == self.board[i * 3 + 1] == self.board[i * 3 + 2] != 0:\n                self.winner = self.board[i]\n            if self.board[i] == self.board[i + 3] == self.board[i + 6] != 0:\n                self.winner = self.board[i]\n        if self.board[0] == self.board[4] == self.board[8] != 0:\n            self.winner = self.board[0]\n        if self.board[2] == self.board[4] == self.board[6] != 0:\n            self.winner = self.board[2]\n        if all(val != 0 for val in self.board):\n            self.winner = 0\n\n    def display(self):\n        print(\"-------------\")\n        for i in range(3):\n            print(f\"| {self.board[i*3]} | {self.board[i*3+1]} | {self.board[i*3+2]} |\")\n            print(\"-------------\")","block_group":"980fd529cb174708bb006be8f3e3aa56","execution_count":61,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"72fb5eedea3e44098c65fc49b5d97834","source_hash":"4e7d9f95","execution_start":1677447283701,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"def play_game():\n    # Create a new TicTacToe game and an MCST agent\n    game = TicTacToe()\n    agent = MCSTAgent()\n\n    # Main game loop\n    while game.winner is None:\n        # Display the current state of the game\n        game.display()\n\n        # If it's the player's turn, prompt them for a move and make the move\n        if game.current_player == 1:\n            valid_move = False\n            while not valid_move:\n                move = int(input(\"Enter your move (0-8): \"))\n                if move in game.get_available_moves():\n                    valid_move = True\n                    game.make_move(move)\n                else:\n                    print(\"Invalid move. Try again.\")\n        # If it's the agent's turn, get the agent's move and make the move\n        else:\n            game_copy = copy.deepcopy(game)\n            move = agent.get_move(game_copy)\n            print(f\"Agent plays move {move}\")\n            game.make_move(move)\n\n    # Display the final state of the game and the winner\n    game.display()\n    if game.winner == 0:\n        print(\"Tie game!\")\n    elif game.winner == 1:\n        print(\"Player wins!\")\n    else:\n        print(\"Agent wins!\")\n","block_group":"72fb5eedea3e44098c65fc49b5d97834","execution_count":62,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"73a8e3419c834de78f37c33095be1b5f","source_hash":"8f3dde3d","execution_start":1677448421732,"execution_millis":18166,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"play_game()","block_group":"73a8e3419c834de78f37c33095be1b5f","execution_count":65,"outputs":[{"name":"stdout","text":"-------------\n| 0 | 0 | 0 |\n-------------\n| 0 | 0 | 0 |\n-------------\n| 0 | 0 | 0 |\n-------------\n-------------\n| 0 | 0 | 0 |\n-------------\n| 0 | 1 | 0 |\n-------------\n| 0 | 0 | 0 |\n-------------\nAgent plays move 7\n-------------\n| 0 | 0 | 0 |\n-------------\n| 0 | 1 | 0 |\n-------------\n| 0 | -1 | 0 |\n-------------\n-------------\n| 0 | 0 | 0 |\n-------------\n| 1 | 1 | 0 |\n-------------\n| 0 | -1 | 0 |\n-------------\nAgent plays move 1\n-------------\n| 0 | -1 | 0 |\n-------------\n| 1 | 1 | 0 |\n-------------\n| 0 | -1 | 0 |\n-------------\n","output_type":"stream"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplay_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn [62], line 15\u001b[0m, in \u001b[0;36mplay_game\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m valid_move \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid_move:\n\u001b[0;32m---> 15\u001b[0m     move \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter your move (0-8): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m move \u001b[38;5;129;01min\u001b[39;00m game\u001b[38;5;241m.\u001b[39mget_available_moves():\n\u001b[1;32m     17\u001b[0m         valid_move \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m/shared-libs/python3.9/py-core/lib/python3.9/site-packages/ipykernel/kernelbase.py:1177\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1176\u001b[0m     )\n\u001b[0;32m-> 1177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/shared-libs/python3.9/py-core/lib/python3.9/site-packages/ipykernel/kernelbase.py:1219\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"622c5e9f06d84d64863663ba3fb1e89c","source_hash":"8ddd1798","execution_start":1677445129756,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"play()","block_group":"622c5e9f06d84d64863663ba3fb1e89c","execution_count":32,"outputs":[{"name":"stdout","text":"[0, 0, 0]\n[0, 0, 0]\n[0, 0, 0]\nDraw.\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"5d0648a5043a4f3c8f12c091e52b3397","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"","block_group":"5d0648a5043a4f3c8f12c091e52b3397"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"fa1d76c5ccd64d28955adc26b99f1112","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## Experimentation with exploration constant","block_group":"fa1d76c5ccd64d28955adc26b99f1112"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"5fa2f80c8f894f268c51271c377b9490","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Try different exploration constants and maximum iterations to see how they affect the performance of the algorithm.","block_group":"5fa2f80c8f894f268c51271c377b9490"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"1be77f06dd904f609c6564335ff69707","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# Evaluation","block_group":"1be77f06dd904f609c6564335ff69707"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"ba95c35946ee403a900b95bd76191ec7","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Evaluate your algorithm and comment on its pros and cons. For example, is it fast? Is it sample efficient? Is the learned policy competitive? Does it lose? Would you, as a human, beat it? Would it scale well to larger grids such as 4x4 or 5x5?","block_group":"ba95c35946ee403a900b95bd76191ec7"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"9fd6f0ae4c8c48adaa776798ef20835a","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# Next gen TTT AI","block_group":"9fd6f0ae4c8c48adaa776798ef20835a"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"c8d021ab134340d08e74727758a0ee3d","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"We could try using a neural network to estimate the value of each state instead of simulating games to the end. This could speed up the search and improve the performance of the algorithm.","block_group":"c8d021ab134340d08e74727758a0ee3d"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"402f3e101c564d82bec5b8e698141ccd","deepnote_cell_type":"code"},"source":"#Assume square board\nBOARD_SIZE = 3\n\nclass State:\n    def __init__(self, p1, p2):\n        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n        self.p1 = p1\n        self.p2 = p2\n        self.isEnd = False\n        self.boardHash = None\n        # init p1 plays first\n        self.playerSymbol = 1\n\n\n\n","block_group":"402f3e101c564d82bec5b8e698141ccd","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"0aee28e43f9b467d9028055007d8561a","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# Reinforcement learning","block_group":"0aee28e43f9b467d9028055007d8561a"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=1262dda2-abb7-4af7-a1b6-72164064af5a' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"3f28ea2e56da473fa226e83a4f62a2a2","deepnote_persisted_session":{"createdAt":"2023-02-26T15:32:01.247Z"},"deepnote_execution_queue":[]}}
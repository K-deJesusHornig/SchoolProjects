{"cells":[{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"1c6c1ff38a034f94bd2aa0a736a11322","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# Implementation Ass_5","block_group":"1c6c1ff38a034f94bd2aa0a736a11322"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"7dd344d3c94c4ba89459be397ead1061","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"code":true},"toCodePoint":22,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-h3"},"source":"### Implement tree search ","block_group":"7dd344d3c94c4ba89459be397ead1061"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"d521b7424dfb4fab81af0c8620fd1117","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Tree search algorithm for board games like TTT: ","block_group":"d521b7424dfb4fab81af0c8620fd1117"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"462ba93b344d45e6b77ce3de7cec1262","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"The (theoretical) whole search space is 9! = 362 880 different states. But terminal nodes (winning combinations) end branches beforehand. So actual possible states is < 20 000.  --> feasible to make exhaustive searches. No limits in search tree other that hyperparameter max-iter (good for time aspect).  ","block_group":"462ba93b344d45e6b77ce3de7cec1262"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","number":0,"cell_id":"2b8602ef395c495c883622e6ffe489e5","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"},"source":"- MCTS: cycle of four steps. Select, expand, explore, and backup. Monte-carlo methods perform randomly selected of actions. Ie. make sure to not leave out any exploration opportunities. (something like Kai's first version) ","block_group":"2b8602ef395c495c883622e6ffe489e5"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","number":1,"cell_id":"8b01e2a55c3345bf995a74fa109f8335","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"},"source":"- Minimax algorithm: This is a classic algorithm for turn-based games. It works by recursively exploring all possible moves and their outcomes, and then selecting the move that maximizes the minimum outcome.","block_group":"8b01e2a55c3345bf995a74fa109f8335"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","cell_id":"c3c75ef22d8b42748e143227895d0328","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"},"source":"- Alpha-beta pruning: This is a variant of the minimax algorithm that eliminates branches of the search tree that are guaranteed to be worse than previously explored branches. This can significantly reduce the search space and speed up the algorithm.","block_group":"c3c75ef22d8b42748e143227895d0328"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","cell_id":"d0a7a75bd4e449daa0a1614a6e1a31e4","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"},"source":"- ","block_group":"d0a7a75bd4e449daa0a1614a6e1a31e4"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"264a71e25b7f405ca1c8105e3c14839c","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true,"underline":true},"toCodePoint":26,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-p"},"source":"Different design choices: ","block_group":"264a71e25b7f405ca1c8105e3c14839c"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","number":1,"cell_id":"f168ad0273eb47d0b9201a08c441db23","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"},"source":"- the roll-out policy(move selection) of your player: How does my player select moves?  ","block_group":"f168ad0273eb47d0b9201a08c441db23"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","cell_id":"1541ce4743524842a71aeb29a8587c8c","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"For actual moves we give input. But our move in the simulation of the program should be a move towards a possible row/diagonal. --> implement a method to find and return random \"liberties\" that are \"aggressive moves\"?   ","block_group":"1541ce4743524842a71aeb29a8587c8c"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","number":2,"cell_id":"14ae7f931434436a9132c1119aa64a73","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"},"source":"- the policy of your opponent: how are the moves of the program selected?","block_group":"14ae7f931434436a9132c1119aa64a73"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","cell_id":"dfb20de0cbf94afdba292d02748fbfe4","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Choose the highest ucb move which should be the simulated move with the most wins per visit. Assuming that the simulation has shed light on most (all) good moves and increased their UCB. ","block_group":"dfb20de0cbf94afdba292d02748fbfe4"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","number":3,"cell_id":"0ed659c6932d42b08f63c017adaa33ca","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"},"source":"- the selection policy in your search tree: what node is selected in the search tree? ","block_group":"0ed659c6932d42b08f63c017adaa33ca"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","cell_id":"1e01de42811042e184d742eb36ac1cb2","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"The simulation should cover enough of the search three to make a decision that maximizes the probability of winning (wins/visits). To ensure that no good moves are missed out due to finding good but not the best move too early selection of suboptimal moves is done randomly as part of the exploration. But before, a \"minimal exploration\" is done by controlling whether all nodes are fully expanded can be carried out to guarantee that all nodes are visited at least once (do with if visits =0 for all legal moves).   ","block_group":"1e01de42811042e184d742eb36ac1cb2"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","cell_id":"b600e634aef34407a6331041cce52649","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"},"source":"- the updates to your search tree (\"back-up\")","block_group":"b600e634aef34407a6331041cce52649"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","cell_id":"b70d36f020d54a37b93aeebd2e0608ab","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"For the updates, we make sure to increment the wins and visits all the way to the top. The new values will automatically adjust the ucb score over time. During simulation, the search three is expanded randomly until termination in each loop, not fully expanded nodes are progressivly expanded and when all children have been visited at least once it is again a matter of random choice.     ","block_group":"b70d36f020d54a37b93aeebd2e0608ab"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","cell_id":"10b88eb0038c41f5b136b6d5d179a342","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"note: if we simply want to find the optimal node we can loop the entire search space with DFS or BFS. But that is costly and ineffective. --> hence we randomly explore large parts, make sure to get some minimal degree of exploration, and otherwise uniformly expand the different parts without any respect to the number of wins. The question remains if we could increase the speed of finding and sedimenting the best moves by designing a policy that takes the highest ucb score but where the ucb score initially is artificially increased for not visited nodes, and later is equal to the actual rate of wins/visits.   ","block_group":"10b88eb0038c41f5b136b6d5d179a342"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","cell_id":"4e7754b2403049a2980f8186382ef0ad","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"note: innan varje move borde vår motståndare göra en granskning om detta state är ett state innan förlust. Isåfall blockerar den ","block_group":"4e7754b2403049a2980f8186382ef0ad"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"f2fa494ecff64372814f9c2fae50593d","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"","block_group":"f2fa494ecff64372814f9c2fae50593d"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"7c24b4f76c3d40e19d992c230bb0b315","source_hash":"cb3f5a","execution_start":1677529274072,"execution_millis":7430155,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# imports\nimport numpy as np ","block_group":"7c24b4f76c3d40e19d992c230bb0b315","execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"832f209e59ef4c6b920c0389ac2b2177","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## Classes Node & MCTS","block_group":"832f209e59ef4c6b920c0389ac2b2177"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"da2a939ea4fb442ab53d72a88d4e6bb7","source_hash":"b7131131","execution_start":1677529274082,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"\"\"\"\nOverall we need to define two classes: Node and MCTS. \nThe Node class represents a node in the MCTS tree, and contains \nthe state, parent node, child nodes, win count, and visit count. \n\nThe MCTS class represents the MCTS algorithm itself, and contains the exploration constant, \nmaximum number of iterations, and the methods for running the search and simulating games.\n\"\"\"\n\nclass Node:\n    def __init__(self, state, player):\n        self.state = state\n        self.player = player\n        self.parent = None\n        self.children = [] # list with children\n        self.wins = 0 # use for V = average reward? \n        self.visits = 0 # cannot divide by zero --> adjust the UCB expression with \"1+visists\" --> evalaute\n        #self.untried_moves = [] # pop from this... \n\n    \n\n\n    def select_child(self, exploration_constant = np.sqrt(2)):\n        #compute ucb score for all kids \n        ucb_values = [child.wins/(1+child.visits) + exploration_constant * np.sqrt(np.log(1+self.visits)/(1+child.visits)) \n        for child in self.children]\n        #exploration_constant *= 0.9 # TODO adjust the constant\n        #ucb_values.sort(reverse=True)\n        #print(\"Scores for top three: \")\n        #print(ucb_values[:3])\n        return self.children[np.argmax(ucb_values)]\n\n    # expand (but only if never expanded before)\n    def expand(self):\n        possible_moves = [(i, j) for i in range(3) for j in range(3) if self.state[i][j] == 0]\n               \n        # handle if there are no possible moves, ie. a tie\n        if not possible_moves: #if empty return self\n            #print('The list is empty, no possible moves!')\n            return self\n\n        # is the node fully explored? \n        for legal_child in possible_moves:\n            if legal_child.visits == 0:\n                return legal_child\n         \n\n        else:\n            # TODO implement a method that correctly pops and explores\n            if self.untried_moves: #if moves left untried \n                #print(\"Poped in expansion\")\n                next_move = self.untried_moves.pop()\n                new_state_list = np.copy(self.state)\n                new_state_list[next_move] = self.player # sets currect player on new state position\n                child_node = Node(new_state_list, -self.player)\n                child_node.parent = self\n                self.children.append(child_node)\n                return child_node\n\n            # create children from all possible moves and return the selected best for rollout step\n            for new_child_move in possible_moves:\n                new_state_list = np.copy(self.state)\n                new_state_list[new_child_move] = self.player # sets currect player on new state position\n                child_node = Node(new_state_list, -self.player)\n                child_node.parent = self\n                self.children.append(child_node)\n            \n            child_chosen_for_simulation = self.select_child() #exploration constant default\n            return child_chosen_for_simulation \n        \n\n    \"\"\"Implement better policy for opponent.\n    For example, use a Minimax algorithm to choose the opponent's moves.\n    If first move (middle), else if, the connecting positions\"\"\"\n\n    def update(self, result):\n        self.visits += 1\n        if result == self.player:\n            self.wins += 1       \n        #print(\"###############update completed!\")\n\n","block_group":"da2a939ea4fb442ab53d72a88d4e6bb7","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"f81c5a4dd56844e2808f7fbfe4e35d30","source_hash":"a3f2e2da","execution_start":1677529274113,"execution_millis":6,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"class MCTS:\n    def __init__(self, exploration_constant= np.sqrt(2), max_iterations=10):\n        self.exploration_constant = exploration_constant\n        self.max_iterations = max_iterations\n\n    def search(self, initial_state):\n        root_node = Node(initial_state, 1) \n        for i in range(self.max_iterations):\n            node = root_node # start every search from root\n            #select & expand accordingly\n            if not node.children: #if no children exists\n                #expand leaf for the most promissing child\n                most_promising_child_node = node.expand() \n                \n            else: # children exist\n                existing_child_node = node.select_child(self.exploration_constant) # select child with highest UCB score for rollout        \n                most_promising_child_node = existing_child_node.expand()    \n            #rollout\n            result = self.run_simulation(most_promising_child_node) # simulate form new child on this MCTS obj\n            #backup\n            while node: #stops when at root node\n                node.update(result) # update wins and visits\n                node = node.parent # move up the three\n            #print(\"Simulations Nr: \" + str(i))\n        #act in line with search results    \n        best_child = root_node.children[np.argmax([child.visits for child in root_node.children])] #action based on most visited\n        action = best_child.state - root_node.state\n        #return self.select_action(best_child) # promotes exploration\n        print( \"Best move found: \\n\" + str(action) + \" now the bord is: \")\n        return action\n        #return root_node.children[np.argmax([child.visits for child in root_node.children])].state - root_node.state\n    \"\"\"\n    def traverse(): # TODO, called once in every search, before rollout\n    # while current node is not terminal node --> if not fully expanded, then expand (with untried node).  Else, choose best child node. \n    # return:  \n    \"\"\"\n    def run_simulation(self, node): #aka rollout\n        this_state = node.state\n        this_player = node.player\n        \n        #print(\"New simulation with: \\n\" +str(this_state)) #visualize\n        c=0\n       \n        while True:  #until terminal state reached\n            #possible_moves = [(i, j) for i in range(3) for j in range(3) if state[i][j] == 0] # detected bug, it misses the certain values before in row?\n            c+=1\n            #return either 0 (for tie) or -1/1 (for the winning player) as a reward for this rollout \n            \n            #expand and choose best action until either tie or win \n            if self.termination_control(state=this_state, current_player=this_player): #wining condition\n                #print(\"simulation: we have a winner\")\n                return this_player\n    \n            if node is node.expand(): # tie \n                #print(\"Simulation: It's a tie!\")\n                return 0    \n            # update node\n            \n            next_node = node.expand()\n            node = next_node  # loop forever in case of a tie now. --> since expand resturn self, identify this here!\n            #print(c)\n            player = -player\n\n    \"\"\"\n    def select_action(self, node): \n        for child in node.children:\n            #Loop below is done to promote exploration and avoid always choosing the same child node\n            if child.visits == node.visits:\n                #action is represented as the difference between the state lists \n                #of the best child node and the state of the current node.\n                return child.state - node.state \n        # return the action that selects the most visited child\n        return node.children[np.argmax([child.visits for child in node.children])].state - node.state\n    \"\"\"\n    def termination_control(self, state, current_player): #only investigates wins\n        for i in range(3): # check every row and colum for \"straight wins\"\n            if state[i][0] == state[i][1] == state[i][2] == current_player:\n                return True\n            if state[0][i] == state[1][i] == state[2][i] == current_player:\n                return True\n        if state[0][0] == state[1][1] == state[2][2] == current_player: # \"diagonal wins\"\n            return True\n        if state[0][2] == state[1][1] == state[2][0] == current_player:\n            return True\n        return False","block_group":"f81c5a4dd56844e2808f7fbfe4e35d30","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"d433f76d1846425586b198eb4b4ba2fa","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## Run an example","block_group":"d433f76d1846425586b198eb4b4ba2fa"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"7636b916dcad4cf19c11b85ac844daef","source_hash":"ab807ff3","execution_start":1677529274167,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# create a new MCTS instance to learn\nmcts = MCTS(exploration_constant=1.0, max_iterations=100)\n","block_group":"7636b916dcad4cf19c11b85ac844daef","execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"06ebcc3093e5412cb2682ac11723ee3c","source_hash":"228b96a1","execution_start":1677529274167,"execution_millis":901041,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"111\n1\n11\n# initialize the game state\ngame_state = np.zeros((3, 3))\nprint(\"Current borad: \")\nprint(game_state)\n\n# play the game\nwhile True:\n    \n    \n    # human player's turn\n    print(\"###Human player's turn\")\n    row = int(input(\"Enter row: \"))-1\n    col = int(input(\"Enter col: \"))-1\n    game_state[row][col] = -1\n    print(game_state)\n    if mcts.termination_control(game_state, -1):\n        print(\"Human player wins!\")\n        break\n    \n    # computer player's turn\n    print(\"###Computer player's turn\")\n    # computer explores the search tree & takes best action found \n    copy_game_state = game_state.copy()\n    action = mcts.search(copy_game_state)\n    game_state += action \n    print(game_state)\n    if mcts.termination_control(game_state, 1):\n        print(\"Computer player wins!\")\n        break\n    if not np.any(game_state == 0):\n        print(\"It's a tie!\")\n        break\n","block_group":"06ebcc3093e5412cb2682ac11723ee3c","execution_count":5,"outputs":[{"name":"stdout","text":"Current borad: \n[[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\n###Human player's turn\n","output_type":"stream"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [5], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     \n\u001b[1;32m     12\u001b[0m     \n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# human player's turn\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m###Human player\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms turn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter row: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     16\u001b[0m     col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter col: \u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     17\u001b[0m     game_state[row][col] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n","File \u001b[0;32m/shared-libs/python3.9/py-core/lib/python3.9/site-packages/ipykernel/kernelbase.py:1177\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1176\u001b[0m     )\n\u001b[0;32m-> 1177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/shared-libs/python3.9/py-core/lib/python3.9/site-packages/ipykernel/kernelbase.py:1219\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"3a93eefb2553478a903762790f7273ac","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## Experimentation with exploration constant","block_group":"3a93eefb2553478a903762790f7273ac"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"e8d10e67488a48a4810d31e5c4b7c04a","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Try different exploration constants and maximum iterations to see how they affect the performance of the algorithm.","block_group":"e8d10e67488a48a4810d31e5c4b7c04a"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"804ca84332164943a5b0ddd88c15ce0e","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# Evaluation","block_group":"804ca84332164943a5b0ddd88c15ce0e"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"fa22d830c89b4f44ae9b78ffe4c0c2c4","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Evaluate your algorithm and comment on its pros and cons. For example, is it fast? Is it sample efficient? Is the learned policy competitive? Does it lose? Would you, as a human, beat it? Would it scale well to larger grids such as 4x4 or 5x5?","block_group":"fa22d830c89b4f44ae9b78ffe4c0c2c4"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"2a7aa03a59724e3e891fc78a794f922f","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# Next gen TTT AI","block_group":"2a7aa03a59724e3e891fc78a794f922f"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"0acc82fa2b60454bad90ffcbae8a10c2","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"We could try using a neural network to estimate the value of each state instead of simulating games to the end. This could speed up the search and improve the performance of the algorithm.","block_group":"0acc82fa2b60454bad90ffcbae8a10c2"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=1262dda2-abb7-4af7-a1b6-72164064af5a' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3 (ipykernel)"},"language_info":{"name":"python","version":"3.9.13","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"deepnote_notebook_id":"f3bf8b939f2c4e60a72dca723defa7bc","deepnote_execution_queue":[],"deepnote_persisted_session":{"createdAt":"2023-02-27T22:31:53.774Z"}}}
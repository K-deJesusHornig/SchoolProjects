{"cells":[{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"bfffdd3b21c64c79b0cbc1b03a1cd8c5","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# Implementation Ass_5","block_group":"bfffdd3b21c64c79b0cbc1b03a1cd8c5"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"92008874-9944-47be-bcd8-a986b9d6cf76","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"code":true},"toCodePoint":22,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-h3"},"source":"### Implement tree search ","block_group":"92008874-9944-47be-bcd8-a986b9d6cf76"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"38e9dedc-ddee-42c7-a01e-4e1043042555","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Alternatives to monte carlo_","block_group":"38e9dedc-ddee-42c7-a01e-4e1043042555"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"f0dd305d-e4a3-4b4b-a6a6-7cf64dab4737","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"There are several alternatives to Monte Carlo for tree search implementations to learn to win at Tic-tac-toe:","block_group":"f0dd305d-e4a3-4b4b-a6a6-7cf64dab4737"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","number":1,"cell_id":"442c7090-35e5-47d4-a60e-e72894714e16","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"},"source":"- Minimax algorithm: This is a classic algorithm for turn-based games. It works by recursively exploring all possible moves and their outcomes, and then selecting the move that maximizes the minimum outcome.","block_group":"442c7090-35e5-47d4-a60e-e72894714e16"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","cell_id":"5ec5ad8a-da77-44ec-870e-224e33d3ae6e","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"},"source":"- Alpha-beta pruning: This is a variant of the minimax algorithm that eliminates branches of the search tree that are guaranteed to be worse than previously explored branches. This can significantly reduce the search space and speed up the algorithm.","block_group":"5ec5ad8a-da77-44ec-870e-224e33d3ae6e"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","cell_id":"135ec636-6c91-4774-93af-8e0c7db278d9","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"},"source":"- Monte Carlo Tree Search (MCTS) with a policy network: Instead of using random rollouts, MCTS can be augmented with a neural network that predicts the value of each move based on the current board state. This allows the algorithm to focus its search on promising moves, improving its efficiency.","block_group":"135ec636-6c91-4774-93af-8e0c7db278d9"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","cell_id":"999a7b33-4bff-4d5d-a6f2-e2fea95f3e00","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"},"source":"- Reinforcement learning: This approach involves training a neural network to predict the optimal move for any given board state, using a combination of supervised learning and self-play. The network is then used to guide the search during gameplay.","block_group":"999a7b33-4bff-4d5d-a6f2-e2fea95f3e00"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"58169f73-644c-465c-bbe3-fc3a78bb3dd3","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Each of these approaches has its own strengths and weaknesses, and the best choice depends on the specific requirements of the task at hand. For example, if computation time is limited, then MCTS with a policy network might be the best choice, while if accurate evaluation of all possible moves is critical, then minimax with alpha-beta pruning might be the way to go.","block_group":"58169f73-644c-465c-bbe3-fc3a78bb3dd3"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"bdbc86c4ab1f47068f11042fc89e4653","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Kai: tycker vi kÃ¶r pÃ¥ MCTS (eventuellt med tillÃ¤gget av en neural network policy) dÃ¥ den krÃ¤ver mindre berÃ¤kningtid/kraft, skalar bÃ¤ttre och Ã¤r en lite coolare implementation som anvÃ¤nds fÃ¶r modeller av shack eller go ðŸ¤© MCTS har ocksÃ¥ fÃ¶rmÃ¥gan att utforska nya strategier som kanske inte Ã¤r uppenbara med traditionella sÃ¶kmetoder som minimax med alfa-beta-sortering. ->PÃ¥ bekostnad av att den inte garanterar bÃ¤sta valet. Just fÃ¶r 3x3 brÃ¤det sÃ¥ hade vi likvÃ¤l kunnat garantera bÃ¤sta valet.   ","block_group":"bdbc86c4ab1f47068f11042fc89e4653"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"2a788ab623644d06ae4bf6a1bc5bde8a","source_hash":"cb3f5a","execution_start":1677529011418,"execution_millis":102637196,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# imports\nimport numpy as np ","block_group":"2a788ab623644d06ae4bf6a1bc5bde8a","execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"e0db1be3ffad4acba630b58ae9c378d5","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## Classes Node & MCTS","block_group":"e0db1be3ffad4acba630b58ae9c378d5"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"001a602b97ea4796af4596e2483370e9","source_hash":"cecddf8d","execution_start":1677529011419,"execution_millis":8,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"\"\"\"\nOverall we need to define two classes: Node and MCTS. \nThe Node class represents a node in the MCTS tree, and contains \nthe state, parent node, child nodes, win count, and visit count. \n\nThe MCTS class represents the MCTS algorithm itself, and contains the exploration constant, \nmaximum number of iterations, and the methods for running the search and simulating games.\n\"\"\"\n\nclass Node:\n    def __init__(self, state, player):\n        self.state = state\n        self.player = player\n        self.parent = None\n        self.children = []\n        self.wins = 0 # use for V = average reward? \n        self.visits = 0 # cannot divide by zero --> adjust the UCB expression with \"1+visists\" --> evalaute\n\n    def select_child(self, exploration_constant = np.sqrt(2)):\n        # experiment with different exploration_constant should be \n        # wiki: c is the exploration parameterâ€”theoretically equal to âˆš2; in practice usually chosen empirically\n        # look over the first term  (V_i is the average reward/value of all nodes beneath this node)\n        ucb_values = [child.wins/(1+child.visits) + exploration_constant * np.sqrt(np.log(self.visits) / (1+child.visits)) for child in self.children]\n        return self.children[np.argmax(ucb_values)]\n\n    def expand(self):\n        possible_moves = [(i, j) for i in range(3) for j in range(3) if self.state[i][j] == 0]\n        #print(\"possible moves are: \")\n        #print(possible_moves)\n        new_state = np.copy(self.state)\n        move = possible_moves[np.random.randint(len(possible_moves))] # currently chooses the new state at random\n        new_state[move] = self.player # sets currect player on new state\n        child_node = Node(new_state, -self.player)\n        child_node.parent = self\n        self.children.append(child_node)\n        return child_node\n\n    \"\"\"Implement better policy for opponent.\n    For example, use a Minimax algorithm to choose the opponent's moves.\n    If first move (middle), else if, the connecting positions\"\"\"\n\n    def update(self, result):\n        self.visits += 1\n        if result == self.player:\n            self.wins += 1       \n\nclass MCTS:\n    def __init__(self, exploration_constant= np.sqrt(2), max_iterations=1000):\n        self.exploration_constant = exploration_constant\n        self.max_iterations = max_iterations\n\n    def search(self, initial_state):\n        root_node = Node(initial_state, 1)\n        for i in range(self.max_iterations):\n            node = root_node\n            #assumes that the children all will be explored in every expansion?  \n            while node.children: # stops when the current node has no more children, ie. in leaf node\n                node = node.select_child(self.exploration_constant) # select child with highest UCB score\n            child_node = node.expand() # expand leaf with new child\n            result = self.run_simulations(child_node) # roll out form new child\n            while node: #stops when at root node\n                node.update(result) # back prop\n                node = node.parent\n        best_child = root_node.children[np.argmax([child.visits for child in root_node.children])]\n        return self.select_action(best_child)\n\n    def run_simulations(self, node):\n        state = node.state\n        player = node.player\n        \n        print(\"New simulation with: \\n\" +str(state)) #just test\n        c =0\n        while True:\n            possible_moves = [(i, j) for i in range(3) for j in range(3) if state[i][j] == 0] # detected bug, it misses the certain values before in row?\n            c+=1\n            if not possible_moves: # if empty\n                print(\"Possible moves is empty!!!\")\n                print(c)\n                return 0 # no update, only on terminal states\n            print(\"Possible moves NOT empty, but:\" + str(possible_moves))\n            \n            move = possible_moves[np.random.randint(len(possible_moves))] # select random possible move --> best policy\n            state[move] = player\n            if self.check_winner(state, player):\n                return player\n            player = -player\n\n    def select_action(self, node):\n        for child in node.children:\n            #Loop below is done to promote exploration and avoid always choosing the same child node\n            # checks ofr \n            if child.visits == node.visits:\n                #action is represented as the difference between the state \n                #of the best child node and the state of the current node.\n                return child.state - node.state \n        # resturn the action that selects the most visited child\n        return node.children[np.argmax([child.visits for child in node.children])].state - node.state\n\n    def check_winner(self, state, current_player):\n        for i in range(3): # check every row and colum for \"straight wins\"\n            if state[i][0] == state[i][1] == state[i][2] == current_player:\n                return True\n            if state[0][i] == state[1][i] == state[2][i] == current_player:\n                return True\n        if state[0][0] == state[1][1] == state[2][2] == current_player: # \"diagonal wins\"\n            return True\n        if state[0][2] == state[1][1] == state[2][0] == current_player:\n            return True\n        return False","block_group":"001a602b97ea4796af4596e2483370e9","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"f684c4d37e2046749c0774544516df84","source_hash":"d6ec0679","execution_start":1677529011471,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"player = -1\nstate1 = np.zeros((3, 3))\n\npossible_moves = [(i, j) for i in range(3) for j in range(3) if state1[i][j] == 0]\nprint(possible_moves)\nmove = possible_moves[np.random.randint(len(possible_moves))]\nprint(move)\nstate1[move] = player\nprint(state1)","block_group":"f684c4d37e2046749c0774544516df84","execution_count":3,"outputs":[{"name":"stdout","text":"[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n(0, 0)\n[[-1.  0.  0.]\n [ 0.  0.  0.]\n [ 0.  0.  0.]]\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"cbb31b7d216b459987da386b1c406ab6","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## Run an example","block_group":"cbb31b7d216b459987da386b1c406ab6"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"f3591cb2fcef4ebb8c95b8a29d6692f3","source_hash":"963f7a9d","execution_start":1677529011471,"execution_millis":7904,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# create a new MCTS instance\nmcts = MCTS(exploration_constant=1.0, max_iterations=1000)\n\n# initialize the game state\ngame_state = np.zeros((3, 3))\n\n# play the game\nwhile True:\n    print(\"Current borad: \")\n    print(game_state)\n    \n    # human player's turn\n    print(\"###human player's turn\")\n    row = int(input(\"Enter row: \"))\n    col = int(input(\"Enter col: \"))\n    game_state[row][col] = -1\n    print(game_state)\n    if mcts.check_winner(game_state, -1):\n        print(\"Human player wins!\")\n        break\n    \n    # computer player's turn\n    print(\"###Computer player's turn\")\n    action = mcts.search(game_state)\n    game_state += action\n    print(game_state)\n    if mcts.check_winner(game_state, 1): # change nanme to termination control\n        print(\"Computer player wins!\")\n        break\n    if not np.any(game_state == 0):\n        print(\"It's a tie!\")\n        break\n","block_group":"f3591cb2fcef4ebb8c95b8a29d6692f3","execution_count":4,"outputs":[{"name":"stdout","text":"Current borad: \n[[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\n###human player's turn\n[[ 0.  0.  0.]\n [ 0. -1.  0.]\n [ 0.  0.  0.]]\n###Computer player's turn\nNew simulation with: \n[[ 0.  0.  0.]\n [ 0. -1.  0.]\n [ 0.  1.  0.]]\nPossible moves NOT empty, but:[(0, 0), (0, 1), (0, 2), (1, 0), (1, 2), (2, 0), (2, 2)]\nPossible moves NOT empty, but:[(0, 0), (0, 1), (0, 2), (1, 0), (2, 0), (2, 2)]\nPossible moves NOT empty, but:[(0, 1), (0, 2), (1, 0), (2, 0), (2, 2)]\nPossible moves NOT empty, but:[(0, 2), (1, 0), (2, 0), (2, 2)]\nPossible moves NOT empty, but:[(0, 2), (2, 0), (2, 2)]\nPossible moves NOT empty, but:[(2, 0), (2, 2)]\nNew simulation with: \n[[ 1. -1. -1.]\n [ 1. -1. -1.]\n [ 1.  1. -1.]]\nPossible moves is empty!!!\n1\n","output_type":"stream"},{"output_type":"error","ename":"ValueError","evalue":"high <= 0","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn [4], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# computer player's turn\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m###Computer player\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms turn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mmcts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m game_state \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m action\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(game_state)\n","Cell \u001b[0;32mIn [2], line 58\u001b[0m, in \u001b[0;36mMCTS.search\u001b[0;34m(self, initial_state)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchildren: \u001b[38;5;66;03m# stops when the current node has no more children, ie. in leaf node\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     node \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mselect_child(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_constant) \u001b[38;5;66;03m# select child with highest UCB score\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m child_node \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# expand leaf with new child\u001b[39;00m\n\u001b[1;32m     59\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_simulations(child_node) \u001b[38;5;66;03m# roll out form new child\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m node: \u001b[38;5;66;03m#stops when at root node\u001b[39;00m\n","Cell \u001b[0;32mIn [2], line 31\u001b[0m, in \u001b[0;36mNode.expand\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#print(\"possible moves are: \")\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#print(possible_moves)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m new_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate)\n\u001b[0;32m---> 31\u001b[0m move \u001b[38;5;241m=\u001b[39m possible_moves[\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpossible_moves\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;66;03m# currently chooses the new state at random\u001b[39;00m\n\u001b[1;32m     32\u001b[0m new_state[move] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplayer \u001b[38;5;66;03m# sets currect player on new state\u001b[39;00m\n\u001b[1;32m     33\u001b[0m child_node \u001b[38;5;241m=\u001b[39m Node(new_state, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplayer)\n","File \u001b[0;32mmtrand.pyx:748\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m_bounded_integers.pyx:1247\u001b[0m, in \u001b[0;36mnumpy.random._bounded_integers._rand_int64\u001b[0;34m()\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: high <= 0"]}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"c3ff5110d9a543b882481270b9321495","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## Experimentation with exploration constant","block_group":"c3ff5110d9a543b882481270b9321495"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"73c0b50d383e4f0f89f70fb1aa6b6564","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Try different exploration constants and maximum iterations to see how they affect the performance of the algorithm.","block_group":"73c0b50d383e4f0f89f70fb1aa6b6564"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"73c0e994ffdf486a8a4daabe8789f241","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# Evaluation","block_group":"73c0e994ffdf486a8a4daabe8789f241"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"f613cd01-27bf-41b2-8d03-41d38e580b8f","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Evaluate your algorithm and comment on its pros and cons. For example, is it fast? Is it sample efficient? Is the learned policy competitive? Does it lose? Would you, as a human, beat it? Would it scale well to larger grids such as 4x4 or 5x5?","block_group":"f613cd01-27bf-41b2-8d03-41d38e580b8f"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"36f76753af6447eaa9ae946ac21a769a","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# Next gen TTT AI","block_group":"36f76753af6447eaa9ae946ac21a769a"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"f2cfcc90-bf77-443d-83cf-a93cd2698ef3","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"We could try using a neural network to estimate the value of each state instead of simulating games to the end. This could speed up the search and improve the performance of the algorithm.","block_group":"f2cfcc90-bf77-443d-83cf-a93cd2698ef3"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=1262dda2-abb7-4af7-a1b6-72164064af5a' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"d0846e4907114eb1bed06cf07cacdd35","deepnote_persisted_session":{"createdAt":"2023-02-26T23:02:00.945Z"},"deepnote_execution_queue":[]}}
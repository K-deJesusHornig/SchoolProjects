{"cells":[{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"98a250856edf439fb62aeef5d2f0b5d6","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# Implementation Ass_5","block_group":"98a250856edf439fb62aeef5d2f0b5d6"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"812af90b7dea49d1a41ec2b7c2479df7","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"code":true},"toCodePoint":22,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-h3"},"source":"### Implement tree search ","block_group":"812af90b7dea49d1a41ec2b7c2479df7"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"d2db6022b7fd48d28a09f37d2eb854fa","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Alternatives to monte carlo_","block_group":"d2db6022b7fd48d28a09f37d2eb854fa"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"0e4eed3ab07741fe91c60188c512d87e","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"There are several alternatives to Monte Carlo for tree search implementations to learn to win at Tic-tac-toe:","block_group":"0e4eed3ab07741fe91c60188c512d87e"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","number":1,"cell_id":"98949eaad0fa433da64a890d2b3f05d4","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"},"source":"- Minimax algorithm: This is a classic algorithm for turn-based games. It works by recursively exploring all possible moves and their outcomes, and then selecting the move that maximizes the minimum outcome.","block_group":"98949eaad0fa433da64a890d2b3f05d4"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","cell_id":"129e44e451e54cfab8dfe3f4d44e16fd","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"},"source":"- Alpha-beta pruning: This is a variant of the minimax algorithm that eliminates branches of the search tree that are guaranteed to be worse than previously explored branches. This can significantly reduce the search space and speed up the algorithm.","block_group":"129e44e451e54cfab8dfe3f4d44e16fd"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","cell_id":"e652bc714ea84722bfc59d62cfaad069","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"},"source":"- Monte Carlo Tree Search (MCTS) with a policy network: Instead of using random rollouts, MCTS can be augmented with a neural network that predicts the value of each move based on the current board state. This allows the algorithm to focus its search on promising moves, improving its efficiency.","block_group":"e652bc714ea84722bfc59d62cfaad069"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","cell_id":"c6034400c0f04709911ec899d95341cd","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"},"source":"- Reinforcement learning: This approach involves training a neural network to predict the optimal move for any given board state, using a combination of supervised learning and self-play. The network is then used to guide the search during gameplay.","block_group":"c6034400c0f04709911ec899d95341cd"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"5183e45249f64288ba413c81964ba92a","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Each of these approaches has its own strengths and weaknesses, and the best choice depends on the specific requirements of the task at hand. For example, if computation time is limited, then MCTS with a policy network might be the best choice, while if accurate evaluation of all possible moves is critical, then minimax with alpha-beta pruning might be the way to go.","block_group":"5183e45249f64288ba413c81964ba92a"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"1e02923ce6e8482f93ed05983a84878e","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Kai: tycker vi k칬r p친 MCTS (eventuellt med till칛gget av en neural network policy) d친 den kr칛ver mindre ber칛kningtid/kraft, skalar b칛ttre och 칛r en lite coolare implementation som anv칛nds f칬r modeller av shack eller go 游뱔 MCTS har ocks친 f칬rm친gan att utforska nya strategier som kanske inte 칛r uppenbara med traditionella s칬kmetoder som minimax med alfa-beta-sortering. ->P친 bekostnad av att den inte garanterar b칛sta valet. Just f칬r 3x3 br칛det s친 hade vi likv칛l kunnat garantera b칛sta valet.   ","block_group":"1e02923ce6e8482f93ed05983a84878e"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"a1e8ac4c82364912ac47338c9eb722df","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":294,"fromCodePoint":240},{"type":"marks","marks":{"bold":true},"toCodePoint":537,"fromCodePoint":477}],"deepnote_cell_type":"text-cell-p"},"source":"[3.353794135372892, 3.353794135372892, 3.353794135372892]\n[3.853786566466944, 3.853786566466944, 3.853786566466944]\n[3.8537789971517844, 3.8537789971517844, 1.015257290950553]\n[3.8537744553662487, 1.0152574282663316]\n[0.015267855851645546]\n[3.35379564910498, 3.35379564910498, 3.35379564910498]\n[3.853788080280869, 3.853788080280869, 3.853788080280869]\n[3.8537805110475554, 3.8537805110475554, 1.0152572188482294]\n[3.853775969311132, 1.0152573561619553]\n[0.015267783638920469]\n[3.3537971628207015, 3.3537971628207015, 3.3537971628207015]\nIOPub data rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_data_rate_limit`.","block_group":"a1e8ac4c82364912ac47338c9eb722df"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"ede67093-0b76-4226-a045-2d0f2db9f960","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Current values:\nNotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\nNotebookApp.rate_limit_window=3.0 (secs)","block_group":"ede67093-0b76-4226-a045-2d0f2db9f960"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"be5b86b3-1485-407b-87e7-6f11caa18b37","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"-------------\n| -1 | 0 | 0 |\n-------------\n| 1 | 1 | 1 |\n-------------\n| 0 | -1 | 0 |\n-------------\nPlayer wins!","block_group":"be5b86b3-1485-407b-87e7-6f11caa18b37"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"6d2999c3bbea49ffb813fefce26adf6b","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"","block_group":"6d2999c3bbea49ffb813fefce26adf6b"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"112459bfabd54294b195badc1531eec2","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## Experimentation with exploration constant","block_group":"112459bfabd54294b195badc1531eec2"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"b79206e9dab24926a57874e90227f4e5","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Try different exploration constants and maximum iterations to see how they affect the performance of the algorithm.","block_group":"b79206e9dab24926a57874e90227f4e5"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"f19e3a6ab05b4ae2bc40b8fcf86f0db8","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# Evaluation","block_group":"f19e3a6ab05b4ae2bc40b8fcf86f0db8"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"93b0d030406749808ff50ed13c30bb9b","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Evaluate your algorithm and comment on its pros and cons. For example, is it fast? Is it sample efficient? Is the learned policy competitive? Does it lose? Would you, as a human, beat it? Would it scale well to larger grids such as 4x4 or 5x5?","block_group":"93b0d030406749808ff50ed13c30bb9b"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"cb43260e0dcc4657b0e66e5460fe5c4c","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# Next gen TTT AI","block_group":"cb43260e0dcc4657b0e66e5460fe5c4c"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"8ba9d223b71e470bac23bb0710d35d71","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"We could try using a neural network to estimate the value of each state instead of simulating games to the end. This could speed up the search and improve the performance of the algorithm.","block_group":"8ba9d223b71e470bac23bb0710d35d71"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=1262dda2-abb7-4af7-a1b6-72164064af5a' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"8c21d045db344827b7a1dbedd0027f96","deepnote_persisted_session":{"createdAt":"2023-02-26T23:02:01.028Z"},"deepnote_execution_queue":[]}}
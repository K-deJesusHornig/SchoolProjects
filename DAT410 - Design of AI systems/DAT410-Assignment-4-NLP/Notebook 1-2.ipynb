{"cells":[{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"3b968b0955d4483d8c7d32d9855b2385","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"In the implementation part of the module, you will in some cases be told exactly what to do, and in other cases you will have to come up with your own solution to a tricky problem. In the latter cases, it is crucial that you describe the choices you made and your reasoning behind them. ","block_group":"3b968b0955d4483d8c7d32d9855b2385"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"8cfd8f211bf94243b0f96c70cb4fed09","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# A) Warm up","block_group":"8cfd8f211bf94243b0f96c70cb4fed09"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"f736af44d9d147b3be927b81b15a7dad","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Let's assume that we pick a word completely randomly from the European parliament proceedings. According to your estimate, what is the probability that it is speaker? What is the probability that it is zebra?     ","block_group":"f736af44d9d147b3be927b81b15a7dad"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"8cbc5e2814b74fcb97950977fe613665","source_hash":"90a72fe5","execution_start":1676119688282,"execution_millis":92,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"from collections import Counter\n\n# ten most common words for every language\nnot_english = ['europarl-v7.de-en.lc.de', 'europarl-v7.fr-en.lc.fr', 'europarl-v7.sv-en.lc.sv']\n\nfor language in not_english:\n    text = \"\"\n    with open(language) as stream:\n        text = stream.read()\n    words = text.split(\" \") # string to list\n    c= Counter(words) \n    print(\"For \"+ language + \" it's: \")\n    print(c.most_common(10))","block_group":"8cbc5e2814b74fcb97950977fe613665","execution_count":null,"outputs":[{"name":"stdout","text":"For europarl-v7.de-en.lc.de it's: \n[(',', 18549), ('die', 9649), ('der', 9139), ('und', 6920), ('in', 3934), ('zu', 3136), ('den', 2955), ('daß', 2725), ('von', 2448), ('für', 2432)]\nFor europarl-v7.fr-en.lc.fr it's: \n[('&apos;', 16729), (',', 15400), ('de', 14444), ('la', 9239), ('et', 6539), ('l', 6254), ('le', 5733), ('à', 5353), ('les', 5260), ('des', 5195)]\nFor europarl-v7.sv-en.lc.sv it's: \n[('att', 9138), (',', 8875), ('och', 6950), ('i', 5599), ('som', 4958), ('för', 4699), ('det', 4524), ('av', 3979), ('är', 3802), ('en', 3632)]\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"4ac32e0878684a889ec65f328d3c2509","source_hash":"320bb8b6","execution_start":1676119688399,"execution_millis":117,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#merging all english docs\ntot_text = []\nall_english = ['europarl-v7.de-en.lc.en', 'europarl-v7.fr-en.lc.en', 'europarl-v7.sv-en.lc.en']\n\nfor language in all_english:\n    text=\"\"\n    with open(language) as stream:\n        text = stream.read()\n    words = text.split(\" \")\n    tot_text.extend(words)\n\nc= Counter(tot_text)\nprint(\"For English it's: \")\nprint(c.most_common(10))","block_group":"4ac32e0878684a889ec65f328d3c2509","execution_count":null,"outputs":[{"name":"stdout","text":"For English it's: \n[('the', 55362), (',', 42038), ('of', 28281), ('to', 26752), ('and', 21257), ('in', 17040), ('is', 13216), ('a', 12801), ('that', 12729), ('for', 8705)]\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"43d45df06998420cba5e0040dfb8fa76","source_hash":"97530831","execution_start":1676119688522,"execution_millis":259,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"the_parliament = []\nthe_parliament.extend(not_english)\nthe_parliament.extend(all_english)\n\ntot_text =[]\nfor language in the_parliament:\n    text=\"\"\n    with open(language) as stream:\n        text = stream.read()\n    words = text.split(\" \")\n    tot_text.extend(words)\n    \nc= Counter(tot_text)\nprint(\"For the whole parliament there is a: \")\nprint(str(c['speaker']/c.total()*100) + \"% \" + \"prob for 'speaker'and a \")\nprint(str(c['zebra']/c.total()*100) + \"% \" + \"prob for 'zebra'\" )","block_group":"43d45df06998420cba5e0040dfb8fa76","execution_count":null,"outputs":[{"name":"stdout","text":"For the whole parliament there is a: \n0.002003123658893535% prob for 'speaker'and a \n0.0% prob for 'zebra'\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"921af42ae6854f938395f22cf42d3124","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## B) Language Modeling  - bigram model  (loaded with the parliament)","block_group":"921af42ae6854f938395f22cf42d3124"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"7c3ab9f3a4eb447c97c4e4dde283de12","is_collapsed":false,"formattedRanges":[{"url":"https://dev.to/amananandrai/language-model-implementation-bigram-model-22ij","type":"link","ranges":[],"toCodePoint":110,"fromCodePoint":35}],"deepnote_cell_type":"text-cell-p"},"source":"Found a nice inspirational source: https://dev.to/amananandrai/language-model-implementation-bigram-model-22ij","block_group":"7c3ab9f3a4eb447c97c4e4dde283de12"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"694c8fd31b3a4c86b9af1c12ee96f1e1","source_hash":"77b69393","execution_start":1676119688783,"execution_millis":27,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#Generate sentences from all data\nthe_parliament = []\nthe_parliament.extend(not_english)\nthe_parliament.extend(all_english)\n\ncomplete_sentences =[]\nfor language in the_parliament:\n    text=\"\"\n    with open(language) as stream:\n        text = stream.read()\n    words = text.split(\" .\\n\")\n    complete_sentences.extend(words)   ","block_group":"694c8fd31b3a4c86b9af1c12ee96f1e1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"36c9e3bc9425438b9963f6b42480dd36","source_hash":"6cea7628","execution_start":1676119688813,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#read in the data to compare\ndef readData(listOfSents):\n    inData = listOfSents\n    listOfWords=[]\n    for i in range(len(inData)):\n        for word in inData[i].split():\n            if word != \",\":         ## does this work as intended? \n                listOfWords.append(word)\n                \n    return listOfWords","block_group":"36c9e3bc9425438b9963f6b42480dd36","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"11b4b55721aa4b559ae440b4be660acf","source_hash":"dbff3c1a","execution_start":1676119688818,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#creat bigram and unigram together with a list that hold them\ndef createBigram(data):\n   listOfBigrams = []\n   bigramCounts = {}\n   unigramCounts = {}\n   for i in range(len(data)-1):\n      if i < len(data) - 1 and data[i+1].islower():\n\n         listOfBigrams.append((data[i], data[i + 1]))\n\n         if (data[i], data[i+1]) in bigramCounts:\n            bigramCounts[(data[i], data[i + 1])] += 1  \n         else:\n            bigramCounts[(data[i], data[i + 1])] = 1\n\n      if data[i] in unigramCounts:\n         unigramCounts[data[i]] += 1\n      else:\n         unigramCounts[data[i]] = 1\n   return listOfBigrams, unigramCounts, bigramCounts","block_group":"11b4b55721aa4b559ae440b4be660acf","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"58fff3855a774da1b3cd6d6d21d2b0b8","source_hash":"4d3d9f40","execution_start":1676119688824,"execution_millis":42,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#compute the probability of words following each other\ndef calcBigramProb(listOfBigrams, unigramCounts, bigramCounts):\n    listOfProb = {}\n    for bigram in listOfBigrams:\n        word1 = bigram[0]\n        word2 = bigram[1]\n        listOfProb[bigram] = (bigramCounts.get(bigram))/(unigramCounts.get(word1))\n    return listOfProb","block_group":"58fff3855a774da1b3cd6d6d21d2b0b8","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"5c77b34c385e4897bc5881208155ac78","source_hash":"350550b7","execution_start":1676119688867,"execution_millis":2606,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#scripting some nice prints\nif __name__ == '__main__':\n    data = readData(complete_sentences)\n    #print(\"\\n HEY LOOK: \")\n    #print(data[:20])\n    listOfBigrams, unigramCounts, bigramCounts = createBigram(data)\n\n    #print(\"\\n Some possible Bigrams are \")\n    #print(listOfBigrams[0:10])\n\n    #print(\"\\n A Bigram along with its frequency \")\n    #print (str(listOfBigrams[0]) + \": \") \n    #print(bigramCounts.get(listOfBigrams[0]))\n\n    #print(\"\\n And the Unigram for \\\"\" + str(listOfBigrams[0][0]) + \"\\\" along with its frequency of: \")\n    #print(unigramCounts.get(listOfBigrams[0][0]))\n\n    bigramProb = calcBigramProb(listOfBigrams, unigramCounts, bigramCounts)\n\n    #print(\"\\n Bigrams along with their probability \")\n    #print(bigramProb)\n    inputList=\"ich erkläre die am freitag\"\n    splt=inputList.split()\n    outputProb = 1\n    bilist=[]\n    bigrm=[]\n\n    for i in range(len(splt) - 1):\n        if i < len(splt) - 1:\n            bilist.append((splt[i], splt[i + 1]))\n\n    #print(\"\\n The bigrams in given sentence are \")\n    #print(bilist)\n   \n    for i in range(len(bilist)):\n        if bilist[i] in bigramProb:\n            outputProb *= bigramProb[bilist[i]]\n        else:\n            outputProb *= 0\n    print('\\n' + 'Probablility of sentence \\\"' + inputList + '\\\" = ' + str(outputProb))","block_group":"5c77b34c385e4897bc5881208155ac78","execution_count":null,"outputs":[{"name":"stdout","text":"\nProbablility of sentence \"ich erkläre die am freitag\" = 8.839638675847781e-09\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"c2fc8937960c4757a8268d8b24197241","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"What happens if you try to compute the probability of a sentence that contains a word that did not appear in the training texts? ANS: currently the probability is zero for that word and hence the sentence also has prob = 0. ","block_group":"c2fc8937960c4757a8268d8b24197241"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"ea60b9d0a1f04b3ea6b56c984c79e4d8","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"\"Also if an unknown word comes in the sentence then the probability becomes 0. This problem of zero probability can be solved with a method known as Smoothing. In Smoothing, we assign some probability to unknown words also. Two very famous smoothing methods are: Laplace Smoothing & Good Turing\" - Source of boilerplate code ","block_group":"ea60b9d0a1f04b3ea6b56c984c79e4d8"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"8d86ccd4e6b44b5b8e24183d55c6b07c","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"And what happens if your sentence is very long (e.g. 100 words or more)? ANS: many fairly small probabilities become minuscule, in the end practically zero. (fact check)","block_group":"8d86ccd4e6b44b5b8e24183d55c6b07c"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"e7170574586f491da64747a2e799b0cb","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"","block_group":"e7170574586f491da64747a2e799b0cb"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"373d4deed21c44b4af7de7a12d0a3c4c","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## C) Translation modeling","block_group":"373d4deed21c44b4af7de7a12d0a3c4c"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"25364019d8d24ad39c07feb1f4bbfccf","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Self-check: if our goal is to translate from some language into English, why does our conditional probability seem to be written backwards? Why don't we estimate P(e|f) instead? ANS: Using Bayes rule (the proportional version) we can decompose the probability at hand and gain division of the tasks of keeping track of contents and fluency, while it also is easier to train the model. From the division of labor, we hence get one language model and one translation model.  (WE MIGHT WANT TO WRITE IT OUT AS A OBJ FUNC TO OPT)","block_group":"25364019d8d24ad39c07feb1f4bbfccf"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"f42cf0fd5a824ce49e53d1502e136d2a","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Write code that implements the estimation algorithm for IBM model 1. Then print, for either Swedish, German, or French, the 10 words that the English word european is most likely to be translated into, according to your estimate. It can be interesting to look at this list of 10 words and see how it changes during the EM iterations.","block_group":"f42cf0fd5a824ce49e53d1502e136d2a"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"8215d3419cb34780be939a6253749a8d","source_hash":"c21a921c","execution_start":1676282088090,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"from os import linesep\nimport string\nfrom collections import Counter, defaultdict\nimport numpy as np\n\ndataset = {'de-en-de': 'europarl-v7.de-en.lc.de',\n          'de-en-en': 'europarl-v7.de-en.lc.en',\n          'fr-en-fr': 'europarl-v7.fr-en.lc.fr',\n          'fr-en-en': 'europarl-v7.fr-en.lc.en',\n          'sv-en-sv': 'europarl-v7.sv-en.lc.sv',\n          'sv-en-en': 'europarl-v7.sv-en.lc.en',}\n\nclass text_parser:\n    def __init__(self, data = dataset):\n        self.data = data\n        self.sentences = []\n        self.words = {}\n        self.keys = []\n\n    def parse(self, keys):\n        self.keys = keys\n        for key in keys:\n            with open(self.data[key], 'r') as file:\n                sentence = file.readlines()\n                words = []\n                for s in sentence:\n                    words_in_sentence = [word for word in s.split() if word not in string.punctuation]\n                    self.sentences.append(words_in_sentence)\n                    words.extend(words_in_sentence)\n                self.words[key] = words\n        \n    def count(self, n = 10):\n        for key in self.keys:\n            most_common = Counter(self.words[key]).most_common(n)\n            print('\\nFor dataset: '+str(key) +', the most common words are:')\n            print(*most_common, sep = \"\\n\")\n    \n    def get_words(self, all_words = True, unique = False):\n        if all_words: \n            all_words = [word for words in self.words.values() for word in words]\n            \n            if unique: \n                return np.unique(all_words)\n            else: \n                return all_words\n        else: return self.words\n    \n    def get_sent(self, n = -1):\n        if n == -1: return self.sentences\n        else: return self.sentences[0:n]\n  \n    def prob_words(self,word_list):\n        all_words = self.get_words(all_words = True)\n        C = Counter(all_words)\n        for word in word_list:\n            if C[word] == 0: print(word+\" is not used in: \"+ str(self.keys))\n            else: print(\"Probability of: \"+ word +\" in \"+str(self.keys)+ \" is: \"+str(C[word]/C.total()))\n\n   ","block_group":"8215d3419cb34780be939a6253749a8d","execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"47cda3bbb52a47e09f61a7fbfbeda870","source_hash":"3b3c1e0a","execution_start":1676282091362,"execution_millis":301,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"parser = text_parser()\nparser.parse(['de-en-de', 'de-en-en'])\nparser.count()","block_group":"47cda3bbb52a47e09f61a7fbfbeda870","execution_count":9,"outputs":[{"name":"stdout","text":"\nFor dataset: de-en-de, the most common words are:\n('die', 10521)\n('der', 9374)\n('und', 7028)\n('in', 4175)\n('zu', 3168)\n('den', 2976)\n('wir', 2863)\n('daß', 2738)\n('ich', 2670)\n('das', 2669)\n\nFor dataset: de-en-en, the most common words are:\n('the', 19847)\n('of', 9597)\n('to', 9059)\n('and', 7303)\n('in', 6237)\n('is', 4478)\n('that', 4441)\n('a', 4435)\n('we', 3372)\n('this', 3362)\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"2ce3ab0225e3426d9de1a8719ce450d4","source_hash":"1d503b27","execution_start":1676281049390,"execution_millis":337,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"parser2 = text_parser()\nparser2.parse(['fr-en-en', 'de-en-en','sv-en-en'])\nparser2.prob_words(['speaker','zebra'])","block_group":"2ce3ab0225e3426d9de1a8719ce450d4","execution_count":null,"outputs":[{"name":"stdout","text":"Probability of: speaker in ['fr-en-en', 'de-en-en', 'sv-en-en'] is: 4.23327120259538e-05\nzebra is not used in: ['fr-en-en', 'de-en-en', 'sv-en-en']\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"f927aaa0c60d458f9698ba6f14540a34","source_hash":"16acb195","execution_start":1676280367341,"execution_millis":22,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"all_words = parser2.get_words(all_words =True)\nprint(all_words[:10])","block_group":"f927aaa0c60d458f9698ba6f14540a34","execution_count":null,"outputs":[{"name":"stdout","text":"['i', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'european', 'parliament', 'adjourned']\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"3aa7a9356578462a88c4a927413d8639","source_hash":"a639753a","execution_start":1676282095294,"execution_millis":121,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"parser3 = text_parser()\nparser3.parse(['sv-en-sv'])\nparser3.get_sent(n=2)","block_group":"3aa7a9356578462a88c4a927413d8639","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"[['jag',\n  'förklarar',\n  'europaparlamentets',\n  'session',\n  'återupptagen',\n  'efter',\n  'avbrottet',\n  'den',\n  '17',\n  'december',\n  'jag',\n  'vill',\n  'på',\n  'nytt',\n  'önska',\n  'er',\n  'ett',\n  'gott',\n  'nytt',\n  'år',\n  'och',\n  'jag',\n  'hoppas',\n  'att',\n  'ni',\n  'haft',\n  'en',\n  'trevlig',\n  'semester'],\n ['som',\n  'ni',\n  'kunnat',\n  'konstatera',\n  'ägde',\n  '&quot;',\n  'den',\n  'stora',\n  'år',\n  '2000-buggen',\n  '&quot;',\n  'aldrig',\n  'rum',\n  'däremot',\n  'har',\n  'invånarna',\n  'i',\n  'ett',\n  'antal',\n  'av',\n  'våra',\n  'medlemsländer',\n  'drabbats',\n  'av',\n  'naturkatastrofer',\n  'som',\n  'verkligen',\n  'varit',\n  'förskräckliga']]"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"55547ffddcbf4064bcbc74bf40dd13e8","source_hash":"d2f7ebf","execution_start":1676282161627,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"class translationModel: \n    def __init__(self, orig_lang, trans_lang):\n\n        #Initiate \"orignal\" & \"translated\" data\n        orig_parser = text_parser()\n        trans_parser = text_parser()\n\n        #Parse \"orignal\" & \"translated\" data\n        orig_parser.parse(orig_lang)\n        trans_parser.parse(trans_lang)\n\n        self.orig_sent = orig_parser.get_sent()\n        self.trans_sent = trans_parser.get_sent()\n        #self.trans_probs = None\n\n        self.orig_words = orig_parser.get_words(all_words=True, unique = True)\n        self.trans_words = trans_parser.get_words(all_words=True, unique = True)\n        \n    def calculate_translation_probs(self, n_iter=5, small_value = 0.01):\n        #initiate transition probabilities to some small value\n        self.trans_probs = defaultdict(lambda: defaultdict(lambda: small_value))\n      \n        #Define # of EM iterations \n        for i in range(1,n_iter): \n\n            #Set all counts c(o,t) and c(t) to 0\n            ot_count = defaultdict(lambda: defaultdict(lambda: small_value))\n            o_count = defaultdict(lambda: small_value)\n\n            #For each sentence pair\n            for o_words, t_words in zip(self.orig_sent,self.trans_sent):\n\n                #o_words = o_sent.split()\n                #t_words = t_sent.split()\n\n                #Include NULL word to original sentence\n                o_words += [\"NULL\"]\n\n                #For each original word\n                for t_word in t_words:\n\n                    #Get transition probability sum\n                    tp_sum = 0\n\n                    for o_word in o_words:\n                        tp_sum+=self.trans_probs[o_word][t_word]\n\n                    #For each translated word (and null word)\n                    for o_word in o_words:\n\n                        #Compute alignment probability\n                        align_prob = self.trans_probs[o_word][t_word]/tp_sum\n\n                        #Update Pseudocount of c(o,t)\n                        ot_count[o_word][t_word] += align_prob\n\n                        #Update Pseudocount of c(t)\n                        o_count[o_word] += align_prob\n            print('test')\n            # Reestimate transition probabilities\n            for o_word, ot_dict in ot_count.items():\n                for t_word, _ in ot_dict.items():\n                    self.trans_probs[o_word][t_word] = ot_count[o_word][t_word] / o_count[o_word]\n\n    def get_similar_words(self,word, n = 10):\n        word_probs = list(self.trans_probs[word].items())\n        word_probs.sort(key=lambda x: x[1], reverse=True)\n        return word_probs[:n]\n\n","block_group":"55547ffddcbf4064bcbc74bf40dd13e8","execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"f31eb4e48a9b4099974d48f46805bcfb","source_hash":"909f7003","execution_start":1676282164345,"execution_millis":43384,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"model = translationModel(['sv-en-en'],['sv-en-sv'])\nmodel.calculate_translation_probs()\nprint(*model.get_similar_words(word=\"european\"), sep =\"\\n\")","block_group":"f31eb4e48a9b4099974d48f46805bcfb","execution_count":14,"outputs":[{"name":"stdout","text":"test\ntest\ntest\ntest\n('europeiska', 0.6177156544630333)\n('europeisk', 0.04368390926320252)\n('i', 0.033485668270207866)\n('och', 0.024945272891128172)\n('att', 0.02397303257217284)\n('unionen', 0.02331994990234653)\n('europaparlamentet', 0.02172010375981349)\n('den', 0.02129427853973632)\n('en', 0.02012773430033462)\n('för', 0.01662425479044724)\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"66ba17c79f3649f6ad93b339ee1b79b1","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## D) Decoding","block_group":"66ba17c79f3649f6ad93b339ee1b79b1"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=27527141-1c2d-41eb-a7d6-3699f108d4e9' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"74d7e6bdcfec458c9fc5bd4460689dba","deepnote_execution_queue":[]}}
{"cells":[{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"fc31405e4ba348e5839cfca369c1b539","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"In the implementation part of the module, you will in some cases be told exactly what to do, and in other cases you will have to come up with your own solution to a tricky problem. In the latter cases, it is crucial that you describe the choices you made and your reasoning behind them. ","block_group":"fc31405e4ba348e5839cfca369c1b539"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"121e5bc2fb774f27b532d74f96204174","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# 2 - Implementation","block_group":"121e5bc2fb774f27b532d74f96204174"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"bcee6a0f1dde481bb18457ee41fe18ca","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## A) warm up","block_group":"bcee6a0f1dde481bb18457ee41fe18ca"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"c714b7012ba04a52868a3fad7211db5b","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Let's assume that we pick a word completely randomly from the European parliament proceedings. According to your estimate, what is the probability that it is speaker? What is the probability that it is zebra?     ","block_group":"c714b7012ba04a52868a3fad7211db5b"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"b6b12d04c0a044c2b19cb73b8cee1099","source_hash":"90a72fe5","execution_start":1676121795878,"execution_millis":138,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"from collections import Counter\n\n# ten most common words for every language\nnot_english = ['europarl-v7.de-en.lc.de', 'europarl-v7.fr-en.lc.fr', 'europarl-v7.sv-en.lc.sv']\n\nfor language in not_english:\n    text = \"\"\n    with open(language) as stream:\n        text = stream.read()\n    words = text.split(\" \") # string to list\n    c= Counter(words) \n    print(\"For \"+ language + \" it's: \")\n    print(c.most_common(10))","block_group":"b6b12d04c0a044c2b19cb73b8cee1099","execution_count":null,"outputs":[{"name":"stdout","text":"For europarl-v7.de-en.lc.de it's: \n[(',', 18549), ('die', 9649), ('der', 9139), ('und', 6920), ('in', 3934), ('zu', 3136), ('den', 2955), ('daß', 2725), ('von', 2448), ('für', 2432)]\nFor europarl-v7.fr-en.lc.fr it's: \n[('&apos;', 16729), (',', 15400), ('de', 14444), ('la', 9239), ('et', 6539), ('l', 6254), ('le', 5733), ('à', 5353), ('les', 5260), ('des', 5195)]\nFor europarl-v7.sv-en.lc.sv it's: \n[('att', 9138), (',', 8875), ('och', 6950), ('i', 5599), ('som', 4958), ('för', 4699), ('det', 4524), ('av', 3979), ('är', 3802), ('en', 3632)]\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"9853235cde7c421494791c4ac4ff29e9","source_hash":"320bb8b6","execution_start":1676121796019,"execution_millis":156,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#merging all english docs\ntot_text = []\nall_english = ['europarl-v7.de-en.lc.en', 'europarl-v7.fr-en.lc.en', 'europarl-v7.sv-en.lc.en']\n\nfor language in all_english:\n    text=\"\"\n    with open(language) as stream:\n        text = stream.read()\n    words = text.split(\" \")\n    tot_text.extend(words)\n\nc= Counter(tot_text)\nprint(\"For English it's: \")\nprint(c.most_common(10))","block_group":"9853235cde7c421494791c4ac4ff29e9","execution_count":null,"outputs":[{"name":"stdout","text":"For English it's: \n[('the', 55362), (',', 42038), ('of', 28281), ('to', 26752), ('and', 21257), ('in', 17040), ('is', 13216), ('a', 12801), ('that', 12729), ('for', 8705)]\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"bdd5a2b504444bea91e3a08260bdb48b","source_hash":"97530831","execution_start":1676121796181,"execution_millis":301,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"the_parliament = []\nthe_parliament.extend(not_english)\nthe_parliament.extend(all_english)\n\ntot_text =[]\nfor language in the_parliament:\n    text=\"\"\n    with open(language) as stream:\n        text = stream.read()\n    words = text.split(\" \")\n    tot_text.extend(words)\n    \nc= Counter(tot_text)\nprint(\"For the whole parliament there is a: \")\nprint(str(c['speaker']/c.total()*100) + \"% \" + \"prob for 'speaker'and a \")\nprint(str(c['zebra']/c.total()*100) + \"% \" + \"prob for 'zebra'\" )","block_group":"bdd5a2b504444bea91e3a08260bdb48b","execution_count":null,"outputs":[{"name":"stdout","text":"For the whole parliament there is a: \n0.002003123658893535% prob for 'speaker'and a \n0.0% prob for 'zebra'\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"bbb4c5f6b4df46d0bc193e789486e85d","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## B) Language Modeling  - bigram model  (loaded with the parliament)","block_group":"bbb4c5f6b4df46d0bc193e789486e85d"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"070cff7d0e98493da4fd42317e3d8404","is_collapsed":false,"formattedRanges":[{"url":"https://dev.to/amananandrai/language-model-implementation-bigram-model-22ij","type":"link","ranges":[],"toCodePoint":110,"fromCodePoint":35}],"deepnote_cell_type":"text-cell-p"},"source":"Found a nice inspirational source: https://dev.to/amananandrai/language-model-implementation-bigram-model-22ij","block_group":"070cff7d0e98493da4fd42317e3d8404"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"6035311d42ab4297b8b1c97e8e01e437","source_hash":"77b69393","execution_start":1676121796483,"execution_millis":41,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#Generate sentences from all data\nthe_parliament = []\nthe_parliament.extend(not_english)\nthe_parliament.extend(all_english)\n\ncomplete_sentences =[]\nfor language in the_parliament:\n    text=\"\"\n    with open(language) as stream:\n        text = stream.read()\n    words = text.split(\" .\\n\")\n    complete_sentences.extend(words)   ","block_group":"6035311d42ab4297b8b1c97e8e01e437","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"884233f19d48482580b264f56c42865a","source_hash":"6cea7628","execution_start":1676121796529,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#read in the data to compare\ndef readData(listOfSents):\n    inData = listOfSents\n    listOfWords=[]\n    for i in range(len(inData)):\n        for word in inData[i].split():\n            if word != \",\":         ## does this work as intended? \n                listOfWords.append(word)\n                \n    return listOfWords","block_group":"884233f19d48482580b264f56c42865a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"4b4ca4560262425fb4bc07ec4cb76193","source_hash":"dbff3c1a","execution_start":1676121796534,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#creat bigram and unigram together with a list that hold them\ndef createBigram(data):\n   listOfBigrams = []\n   bigramCounts = {}\n   unigramCounts = {}\n   for i in range(len(data)-1):\n      if i < len(data) - 1 and data[i+1].islower():\n\n         listOfBigrams.append((data[i], data[i + 1]))\n\n         if (data[i], data[i+1]) in bigramCounts:\n            bigramCounts[(data[i], data[i + 1])] += 1  \n         else:\n            bigramCounts[(data[i], data[i + 1])] = 1\n\n      if data[i] in unigramCounts:\n         unigramCounts[data[i]] += 1\n      else:\n         unigramCounts[data[i]] = 1\n   return listOfBigrams, unigramCounts, bigramCounts","block_group":"4b4ca4560262425fb4bc07ec4cb76193","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"de24985f643846408e3923e36c6e23b2","source_hash":"4d3d9f40","execution_start":1676121796537,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#compute the probability of words following each other\ndef calcBigramProb(listOfBigrams, unigramCounts, bigramCounts):\n    listOfProb = {}\n    for bigram in listOfBigrams:\n        word1 = bigram[0]\n        word2 = bigram[1]\n        listOfProb[bigram] = (bigramCounts.get(bigram))/(unigramCounts.get(word1))\n    return listOfProb","block_group":"de24985f643846408e3923e36c6e23b2","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"d422d74c436848d99ae432bdb9f18330","source_hash":"350550b7","execution_start":1676121796575,"execution_millis":3366,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#scripting some nice prints\nif __name__ == '__main__':\n    data = readData(complete_sentences)\n    #print(\"\\n HEY LOOK: \")\n    #print(data[:20])\n    listOfBigrams, unigramCounts, bigramCounts = createBigram(data)\n\n    #print(\"\\n Some possible Bigrams are \")\n    #print(listOfBigrams[0:10])\n\n    #print(\"\\n A Bigram along with its frequency \")\n    #print (str(listOfBigrams[0]) + \": \") \n    #print(bigramCounts.get(listOfBigrams[0]))\n\n    #print(\"\\n And the Unigram for \\\"\" + str(listOfBigrams[0][0]) + \"\\\" along with its frequency of: \")\n    #print(unigramCounts.get(listOfBigrams[0][0]))\n\n    bigramProb = calcBigramProb(listOfBigrams, unigramCounts, bigramCounts)\n\n    #print(\"\\n Bigrams along with their probability \")\n    #print(bigramProb)\n    inputList=\"ich erkläre die am freitag\"\n    splt=inputList.split()\n    outputProb = 1\n    bilist=[]\n    bigrm=[]\n\n    for i in range(len(splt) - 1):\n        if i < len(splt) - 1:\n            bilist.append((splt[i], splt[i + 1]))\n\n    #print(\"\\n The bigrams in given sentence are \")\n    #print(bilist)\n   \n    for i in range(len(bilist)):\n        if bilist[i] in bigramProb:\n            outputProb *= bigramProb[bilist[i]]\n        else:\n            outputProb *= 0\n    print('\\n' + 'Probablility of sentence \\\"' + inputList + '\\\" = ' + str(outputProb))","block_group":"d422d74c436848d99ae432bdb9f18330","execution_count":null,"outputs":[{"name":"stdout","text":"\nProbablility of sentence \"ich erkläre die am freitag\" = 8.839638675847781e-09\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"ba127840f3d7436b8fd90164842c3729","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"What happens if you try to compute the probability of a sentence that contains a word that did not appear in the training texts? ANS: currently the probability is zero for that word and hence the sentence also has prob = 0. ","block_group":"ba127840f3d7436b8fd90164842c3729"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"63f853768ec34ef7a17e7194ff27c322","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"\"Also if an unknown word comes in the sentence then the probability becomes 0. This problem of zero probability can be solved with a method known as Smoothing. In Smoothing, we assign some probability to unknown words also. Two very famous smoothing methods are: Laplace Smoothing & Good Turing\" - Source of boilerplate code ","block_group":"63f853768ec34ef7a17e7194ff27c322"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"d6d6ae4ef7794f338b5c52ff25246d1e","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"And what happens if your sentence is very long (e.g. 100 words or more)? ANS: many fairly small probabilities become minuscule, in the end practically zero. (fact check)","block_group":"d6d6ae4ef7794f338b5c52ff25246d1e"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"21faead8879843d4accbbe4a71a23262","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"","block_group":"21faead8879843d4accbbe4a71a23262"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"c9d59c930c254c548ab6a63fdc2f59eb","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## C) Translation modeling","block_group":"c9d59c930c254c548ab6a63fdc2f59eb"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"e4be7bf9cad143f291b9c8c707da11db","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Self-check: if our goal is to translate from some language into English, why does our conditional probability seem to be written backwards? Why don't we estimate P(e|f) instead? ","block_group":"e4be7bf9cad143f291b9c8c707da11db"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"304ce05e-7920-4b8f-a751-9dc5bb73e590","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"ANS: Using Bayes rule we get that we can decompose the probability at hand to something that is proportional to it. The gain here is dividing tasks of keeping track of contents in the translation with P(F|E) and the task of getting grammar and fluency right with P(E). This also makes it easier to train the model since only English is required. I.e. this is division of labor, we get one language model and one translation model.  ","block_group":"304ce05e-7920-4b8f-a751-9dc5bb73e590"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"ca179c2acff34da1a81e4925757e178f","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"TASK: Write code that implements the estimation algorithm for IBM model 1. Then print, for either Swedish, German, or French, the 10 words that the English word european is most likely to be translated into, according to your estimate. It can be interesting to look at this list of 10 words and see how it changes during the EM iterations.","block_group":"ca179c2acff34da1a81e4925757e178f"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"3f429f7042aa48f8b0ba3b17040e8610","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"},"source":"### textParser class","block_group":"3f429f7042aa48f8b0ba3b17040e8610"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"0a90f724175f497bb0522bfbe328ec49","source_hash":"51a3a117","execution_start":1676121799967,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"from os import linesep\nimport string\nfrom collections import Counter, defaultdict\nimport numpy as np\n\ndataset = {'de-en-de': 'europarl-v7.de-en.lc.de',\n          'de-en-en': 'europarl-v7.de-en.lc.en',\n          'fr-en-fr': 'europarl-v7.fr-en.lc.fr',\n          'fr-en-en': 'europarl-v7.fr-en.lc.en',\n          'sv-en-sv': 'europarl-v7.sv-en.lc.sv',\n          'sv-en-en': 'europarl-v7.sv-en.lc.en',}\n\nclass text_parser:\n    def __init__(self, data = dataset):\n        self.data = data\n        self.sentences = []\n        self.words = {}\n        self.keys = []\n\n    def parse(self, keys):\n        self.keys = keys\n        for key in keys:\n            with open(self.data[key], 'r') as file:\n                s = file.read()\n                self.sentences = s.split(\" .\\n\")\n                \"\"\"\n                sents = s.split(\" .\\n\")\n                for sent in sents:                # no need for looping through list and appending --> selfsentences = sents\n                    self.sentences.append(sent)\n                \"\"\" \n                words =  s.split(\" \")\n                self.words[key] = [word for word in words if word not in string.punctuation]\n        \n    def count(self, n = 10):\n        for key in self.keys:\n            most_common = Counter(self.words[key]).most_common(n)\n            print('\\nFor dataset: '+str(key) +', the most common words are:')\n            print(*most_common, sep = \"\\n\")\n    \n    def get_words(self, all_words = True, unique = False):\n        if all_words: \n            all_words = [word for words in self.words.values() for word in words]\n            if unique: return np.unique(all_words)\n            else: return all_words\n        else: return self.words\n    \n    def get_sent(self, n = -1):\n        if n == -1: return self.sentences\n        else: return self.sentences[:n]\n  \n    def prob_words(self,word_list):\n        all_words = self.get_words(all_words = True)\n        C = Counter(all_words)\n        for word in word_list:\n            if C[word] == 0: print(word+\" is not used in: \"+ str(self.keys))\n            else: print(\"Probability of: \"+ word +\" in \"+str(self.keys)+ \" is: \"+str(C[word]/C.total()))\n\n   ","block_group":"0a90f724175f497bb0522bfbe328ec49","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"40291095d23c47408e1fabdece46bc7e","source_hash":"3b3c1e0a","execution_start":1676121799972,"execution_millis":169,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"parser = text_parser()\nparser.parse(['de-en-de', 'de-en-en'])\nparser.count()","block_group":"40291095d23c47408e1fabdece46bc7e","execution_count":null,"outputs":[{"name":"stdout","text":"\nFor dataset: de-en-de, the most common words are:\n('die', 9649)\n('der', 9139)\n('und', 6920)\n('in', 3934)\n('zu', 3136)\n('den', 2955)\n('daß', 2725)\n('von', 2448)\n('für', 2432)\n('ist', 2259)\n\nFor dataset: de-en-en, the most common words are:\n('the', 18696)\n('of', 9553)\n('to', 9029)\n('and', 7230)\n('in', 5762)\n('is', 4441)\n('a', 4337)\n('that', 4272)\n('for', 2939)\n('this', 2832)\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"d15328d0a3c1416aa7cc52056d00636d","source_hash":"1d503b27","execution_start":1676121800163,"execution_millis":199,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"parser2 = text_parser()\nparser2.parse(['fr-en-en', 'de-en-en','sv-en-en'])\nparser2.prob_words(['speaker','zebra'])","block_group":"d15328d0a3c1416aa7cc52056d00636d","execution_count":null,"outputs":[{"name":"stdout","text":"Probability of: speaker in ['fr-en-en', 'de-en-en', 'sv-en-en'] is: 4.2331408750800126e-05\nzebra is not used in: ['fr-en-en', 'de-en-en', 'sv-en-en']\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"52a10322c8764391bb01ba69730ba51c","source_hash":"5a6561e4","execution_start":1676121800406,"execution_millis":153,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"all_words = parser2.get_words(all_words =True)\nprint(all_words[:10])\n\nall_words = parser2.get_words(all_words =False)\nprint(all_words)","block_group":"52a10322c8764391bb01ba69730ba51c","execution_count":null,"outputs":[{"text":"IOPub data rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_data_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"95f3a35e7e4447ed95e7e6778f963188","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"},"source":"### translationModel class","block_group":"95f3a35e7e4447ed95e7e6778f963188"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"e5b5adf83349421eb884315705282afc","source_hash":"74c5874c","execution_start":1676122052566,"execution_millis":7,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"import sys\n\nMIN_PROB = sys.float_info.min\nMIN_PROB\n","block_group":"e5b5adf83349421eb884315705282afc","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"2.2250738585072014e-308"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"91483fc15f75498aa065fa8fae2a6ce4","source_hash":"84802d61","execution_start":1676125877019,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"class translationModel: \n    def __init__(self, orig_lang, trans_lang):\n        \n        #Initiate \"orignal\" & \"translated\" data\n        orig_parser = text_parser()\n        trans_parser = text_parser()\n\n        #Parse \"orignal\" & \"translated\" data\n        orig_parser.parse(orig_lang)\n        trans_parser.parse(trans_lang)\n\n        #Initiate sentences \n        self.orig_sent = orig_parser.get_sent()\n        self.trans_sent = trans_parser.get_sent()\n        \"\"\"\n        Are these below used at all? \n        #self.trans_probs = None\n        self.orig_words = orig_parser.get_words(all_words=True, unique = True)\n        self.trans_words = trans_parser.get_words(all_words=True, unique = True)\n        \"\"\"\n    \n        \n    def calculate_translation_probs(self, n_iter=5, small_value = MIN_PROB, word=None):\n        #initiate transition probabilities to some small value & word of interest\n        self.trans_probs = defaultdict(lambda: defaultdict(lambda: small_value))\n        word_of_interest = word\n        if  (word_of_interest != None): \n            print(\"Using: \" + word_of_interest + \"\\n\")\n        else: print(\"Error: Specify a word\")\n        \"\"\"\n        def get_similar_words(self, n = 10):\n            word_probs = list(self.trans_probs[word_of_interest].items())\n            word_probs.sort(key=lambda x: x[1], reverse=True)\n            return word_probs[:10]\n        \"\"\"\n\n\n        # print for each iteration \n        word_probs = list(self.trans_probs[word_of_interest].items())\n        word_probs.sort(key=lambda x: x[1], reverse=True)\n        print(\"\\nRound: [\" + str(0) + \"/\" + str(n_iter) + \"]\")\n        print(*word_probs[:10], sep= \"\\n\") \n\n\n\n        #Define # of EM iterations \n        for i in range(1,n_iter+1): # change to 0 - n_iter?   \n\n            #Set all counts c(o,t) and c(t) to 0\n            ot_count = defaultdict(lambda: defaultdict(lambda: small_value))\n            o_count = defaultdict(lambda: small_value)\n\n            #For each sentence pair\n            for o_sent, t_sent in zip(self.orig_sent,self.trans_sent):\n\n                o_words = o_sent.split()\n                t_words = t_sent.split()\n\n                #Include NULL word to original sentence\n                o_words += [\"NULL\"]\n\n                #For each original word (but here \"t_word\" due to bayes??)\n                for t_word in t_words:\n\n                    #Get (initilize?) transition probability sum\n                    tp_sum = 0\n\n                    for o_word in o_words:         # load sum\n                        tp_sum+=self.trans_probs[o_word][t_word]\n\n                    #For each translated word (and null word)\n                    for o_word in o_words:\n\n                        #Compute alignment probability\n                        align_prob = self.trans_probs[o_word][t_word]/tp_sum\n\n                        #Update Pseudocount of c(o,t)& of c(t)\n                        ot_count[o_word][t_word] += align_prob\n                        o_count[o_word] += align_prob\n\n            # Reestimate transition probabilities\n            for o_word, ot_dict in ot_count.items():\n                for t_word, _ in ot_dict.items():\n                    self.trans_probs[o_word][t_word] = ot_count[o_word][t_word] / o_count[o_word]\n            \n            # print for each iteration \n            word_probs = list(self.trans_probs[word_of_interest].items())\n            word_probs.sort(key=lambda x: x[1], reverse=True)\n            print(\"\\nRound: [\" + str(i) + \"/\" + str(n_iter) + \"]\")\n            print(*word_probs[:10], sep= \"\\n\") \n            \n","block_group":"91483fc15f75498aa065fa8fae2a6ce4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"cfd5453c5cb94ba9bdca452e3cf2eb33","source_hash":"b024b19f","execution_start":1676125881013,"execution_millis":32081,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"model = translationModel(['sv-en-en'],['sv-en-sv'])\nmodel.calculate_translation_probs(word=\"european\")\n","block_group":"cfd5453c5cb94ba9bdca452e3cf2eb33","execution_count":null,"outputs":[{"name":"stdout","text":"Using: european\n\n\nRound: [0/5]\n\n\nRound: [1/5]\n('att', 0.03681418155342775)\n(',', 0.03634172163664544)\n('och', 0.026906798671119368)\n('i', 0.0247574372369237)\n('det', 0.022179298361308157)\n('som', 0.020877887365033796)\n('för', 0.01892124782529525)\n('av', 0.018094315236971116)\n('en', 0.017186765112304034)\n('är', 0.015331287767306873)\n\nRound: [2/5]\n(',', 0.06324136464695101)\n('att', 0.06284287431842107)\n('och', 0.04364733168529026)\n('i', 0.04261401110938812)\n('det', 0.03628207117599448)\n('som', 0.03585449599160646)\n('av', 0.032433891994174055)\n('en', 0.03113832865372013)\n('för', 0.03022247936020297)\n('vi', 0.025753582049213894)\n\nRound: [3/5]\n(',', 0.07156659907184448)\n('att', 0.06940639207958753)\n('i', 0.04869083204289162)\n('och', 0.04637430821202799)\n('som', 0.04107332752682835)\n('av', 0.0397542748487959)\n('det', 0.03951818948130992)\n('en', 0.0387084939374777)\n('för', 0.032329109509809474)\n('vi', 0.031845587458155515)\n\nRound: [4/5]\n(',', 0.07456801556309861)\n('att', 0.07066257106302784)\n('i', 0.051192487893493434)\n('och', 0.045430887311888234)\n('av', 0.0446373298594548)\n('en', 0.04382914172575616)\n('som', 0.043174414895791866)\n('det', 0.03956095405357936)\n('vi', 0.03606436371477041)\n('för', 0.031719289439789924)\n\nRound: [5/5]\n(',', 0.07599214701073342)\n('att', 0.07047683768216799)\n('i', 0.05256665534856924)\n('av', 0.0487210354657634)\n('en', 0.04797412598481332)\n('som', 0.044242698629534796)\n('och', 0.04364966576776109)\n('vi', 0.03953290108960199)\n('det', 0.03870181069305573)\n('för', 0.030370363263130282)\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"27957c8b8c9249d085659e446282fc57","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Fungerar uppebarligen inte. Kolla: så att vår taktik med uniform standard slh fungerar. Kolla slg för ord som \"europeisk\". Jämför beräkningarna och looparna med Git. ","block_group":"27957c8b8c9249d085659e446282fc57"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"5f0bd9d3afe14a0dbf7f50fa9cb80fd7","is_collapsed":false,"formattedRanges":[{"url":"https://github.com/lukaborec/IBM-Model-1/blob/master/IBM1.py","type":"link","ranges":[],"toCodePoint":81,"fromCodePoint":21}],"deepnote_cell_type":"text-cell-p"},"source":"Possible compare to: https://github.com/lukaborec/IBM-Model-1/blob/master/IBM1.py","block_group":"5f0bd9d3afe14a0dbf7f50fa9cb80fd7"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"2ee1cf3347ad41988cb17f37fa4bc370","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## D) Decoding","block_group":"2ee1cf3347ad41988cb17f37fa4bc370"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"6ece0bd35991491cb8f196770b40a265","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Because it requires sifting through a huge search space of potential phrase outputs, the task of locating the sentence with the highest probability in a machine translation decoder model is seen as being algorithmically challenging. It is a combinatorially heavy problem. It can be challenging to discover the ideal solution in a fair amount of time because of the complexity of language and the wide variety of sentence structures and phrasing.","block_group":"6ece0bd35991491cb8f196770b40a265"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"1f4426b4f4f44995b5e25bb181b943ca","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Using a probabilistic model to estimate the likelihood of a sentence given the source sentence, can result in many possible outputs with high probabilities. Hence, finding the sentence with the highest probability is not necessarily a guarantee that is the most accurate and coherent translation. ","block_group":"1f4426b4f4f44995b5e25bb181b943ca"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"d86dd904e4a448839013cf9d122ce96f","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Hence, let's introduce: ","block_group":"d86dd904e4a448839013cf9d122ce96f"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","number":1,"cell_id":"3df8d1b0-505c-483e-bdb8-6d600aecac6a","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"},"source":"- To complete a translation in a timely manner, we only consider the top 20 translations for a word/phrase. I.e. we will satisfy with getting one of the sentences with the highest probability, not necessarily the one and only with the highest probability.  ","block_group":"3df8d1b0-505c-483e-bdb8-6d600aecac6a"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","cell_id":"5b4d2fc8-70fd-4160-812a-286d0090ba70","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"},"source":"- Markov property: the following word only depends on the previous one, allowing us to use a bigram model to gather more contextual information by computing probabilities from adjacent words. This introduces an assumption of independence. ","block_group":"5b4d2fc8-70fd-4160-812a-286d0090ba70"},{"cell_type":"markdown","metadata":{"tags":[],"style":"decimal","cell_id":"9ef15411-b740-451a-a1c6-459a87a067be","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"},"source":"- Caveat: using this model with a certain corpus of language and words limits the translation to words already existing in it.  ","block_group":"9ef15411-b740-451a-a1c6-459a87a067be"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"de63ca18d8bb4c63938a2afb13f8ab07","source_hash":"ddd5e97a","execution_start":1676279500187,"execution_millis":5,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"\"\"\"Given that we pass a word into the get_translation_probs \n--> do this for every word in the source sentence\n\nImplement a connection to adjacent word, remembering the 1-2 last words for better \"contextual understanding\"\n\n\n\"\"\"\n\n\n\n\n\n","block_group":"de63ca18d8bb4c63938a2afb13f8ab07","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"'Given that we pass a word into the get_translation_probs --> do this for every word in the source sentence\\n\\n\\n\\n'"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"a2b1ecc22ca04374bbad8ea29d1207cc","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## ","block_group":"a2b1ecc22ca04374bbad8ea29d1207cc"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"35b63cfade974f2592f43d20318056c1","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# 3 - Discussion ","block_group":"35b63cfade974f2592f43d20318056c1"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"050f86f6a280491c8019bca79683e3b7","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## a) Different evaluation protocols for machine translation systems","block_group":"050f86f6a280491c8019bca79683e3b7"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"c676ca1c-bb4d-4abc-b50a-83746c320add","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"BLEU (Bilingual Evaluation Understudy): A popular metric that examines the n-gram overlap between the predicted sentence and one or more reference translations is called BLEU (Bilingual Evaluation Understudy). The benefit of BLEU is that it can be used to compare various machine translation algorithms and is straightforward and effective to calculate. However, it has the disadvantage of being overly influenced by the length of the predicted sentence and the presence of specific n-grams, and can sometimes give high scores to translations that are semantically incorrect.","block_group":"c676ca1c-bb4d-4abc-b50a-83746c320add"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"3e16b720-36d5-4821-9aab-ba005a6a42d1","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"METEOR (Metric for Evaluation of Translation with Explicit Ordering): This metric assesses semantic similarity more thoroughly than BLEU by taking word-level alignments and synonymy into account. METEOR's benefit is that it offers a more complex assessment of translation quality. However, calculating it costs more computationally than with BLEU.","block_group":"3e16b720-36d5-4821-9aab-ba005a6a42d1"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"b238c2c5-b729-4d4c-b7e8-592f9dbdd697","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Humans in the loop is a third alternative. It basically means that humans are used to evaluating the quality of the translation. The perhaps biggest advantage is the direct measure of quality from the end-user. Although carrying the potential for some really high-quality assessments, in line with the capability of the human at issue, it has some challenges of its own. For example, it is expensive in terms of resources, and not very replicable, further it questionable reliability as the individual and their preferences and biases can easily affect it. This method is the most subjective in its evaluation. ","block_group":"b238c2c5-b729-4d4c-b7e8-592f9dbdd697"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"894d133a-02db-4cd1-9b5d-c437eb6537f3","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"We still have: ROUGE (Recall-Oriented Understudy for Gisting Evaluation), TER (Translation Error Rate):","block_group":"894d133a-02db-4cd1-9b5d-c437eb6537f3"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"fee29d221de24f7a977723ea991486e6","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## b) Translating from Estonian","block_group":"fee29d221de24f7a977723ea991486e6"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"58b01b21-c8ce-413d-9ebd-4d593ea929f4","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"The technical reason why the translation results in a male or female pronoun is that it is related to an overrepresentation of one gender in the respective professional role. E.g. the role of a doctor is probably more often associated with male pronouns such as \"he/him\" in the data that the model builds on, and similar is the role of a nurse more often combined with \"she/her\" in those examples. The underlying reason for this vocabulary is the real-world overrepresentation (current or historical) that one gender is more commonly associated with a certain profession.  We believe that this is a natural bias that exists in the language and it is a historical remnant, despite being based on sometimes rather outdated assumptions of gender roles, the translation itself is still a guess based on the highest probability for how the pronoun is expected to be found. At least according to the data set used. ","block_group":"58b01b21-c8ce-413d-9ebd-4d593ea929f4"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"7b571231-091b-46bb-a550-9a39f0e99699","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Hence, if the data set is big and broad enough to represent the modern language usage of the most recent years, then the qualified guess and decision to choose one pronoun over the other based on the likelihood of it in combination with a certain role should be seen as a feature rather than a bug. Adjusting this by letting the word \"Ta\" be translated to he's/she's with a 50/50 chance instead would most likely not result in any better translations than in the current situation.     ","block_group":"7b571231-091b-46bb-a550-9a39f0e99699"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"7accea3229c845a1b441132ed6eb2233","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## c) Bat VS Bat","block_group":"7accea3229c845a1b441132ed6eb2233"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"8a55973e2b144f458db1da52ec40faab","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"The reason why the first two translations are correct is probably that they are more distinct and unambiguous and hence more in line with one or the other meaning of the homograph \"bat\" the examples found in the word space(farlig användning av space?) that the model builds on. I.e. there are more examples of the word bat being translated into \"slagträ\" in the vicinity of the words like \"hits\" and \"ball\", giving it a higher probability to be correct than \"fladdermus\" in these cases. Similar is the case for the second sentence, where \"bat\" is more likely to be aiming at the animal bat in the context of eating insects. ","block_group":"8a55973e2b144f458db1da52ec40faab"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"28251c6d-6003-4b37-aa6d-efb9ee88dbeb","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"In the third sentence, however, the situation is a bit different. Here the context and adjacent word would suggest that the bat again is an animal since it \"lives\" somewhere. Nevertheless, the model seems not to be able to distinguish this as it has no sense of the actual meaning of words like \"living\". Instead, it completely relies on having seen examples of similar use cases of the word \"bat\" in a sentence and knowing what the word was translated into in that specific setting. The reason why it is a blend of the two cases of the animal and the baseball bat could be explained with the assumption that the bat should refer to the same thing, the same version of \"bat\", as in the previous sentences in one and the same text. But this is more of a speculation from our side than it is a fact, since we don't know exactly how the translation model of google is built. For us, it doesn't make sense to translate the homonym into some morphed version of its different meanings, but the model seems to have done so as a result of not knowing which version was more probable.    ","block_group":"28251c6d-6003-4b37-aa6d-efb9ee88dbeb"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"f79de1d2696d4d01853277f0342f619b","deepnote_cell_type":"code"},"source":" \n\n","block_group":"f79de1d2696d4d01853277f0342f619b","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"106221a25a26491782da10460e868fc0","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# 3 - Discussion - new version","block_group":"106221a25a26491782da10460e868fc0"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"72db9368-167f-4f69-85f9-2471b499b28d","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## a) Different evaluation protocols for machine translation systems","block_group":"72db9368-167f-4f69-85f9-2471b499b28d"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"ea6bf60a-7223-4b1d-8ec6-b05977fbdc19","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"BLEU (Bilingual Evaluation Understudy): A popular metric that examines the n-gram overlap between the predicted sentence and one or more reference translations is called BLEU (Bilingual Evaluation Understudy). The benefit of BLEU is that it can be used to compare various machine translation algorithms and is straightforward and effective to calculate. However, it has the disadvantage of being overly influenced by the length of the predicted sentence and the presence of specific n-grams, and can sometimes give high scores to translations that are semantically incorrect.","block_group":"ea6bf60a-7223-4b1d-8ec6-b05977fbdc19"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"f8ca8286-4408-4b9a-940f-edf40cf142c9","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"METEOR (Metric for Evaluation of Translation with Explicit Ordering): This metric assesses semantic similarity more thoroughly than BLEU by taking word-level alignments and synonymy into account. METEOR's benefit is that it offers a more complex assessment of translation quality. However, calculating it costs more computationally than with BLEU.","block_group":"f8ca8286-4408-4b9a-940f-edf40cf142c9"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"0dc4d60b-d768-46a6-84f7-57915e9516a0","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Humans in the loop is a third alternative. It basically means that humans are used to evaluating the quality of the translation. The perhaps biggest advantage is the direct measure of quality from the end-user. Although carrying the potential for some really high-quality assessments, in line with the capability of the human at issue, it has some challenges of its own. For example, it is expensive in terms of resources, and not very replicable, further it questionable reliability as the individual and their preferences and biases can easily affect it. This method is the most subjective in its evaluation. ","block_group":"0dc4d60b-d768-46a6-84f7-57915e9516a0"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"030117e6-7d4e-4852-be72-d49cfc6e3f47","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true,"underline":true},"toCodePoint":103,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-p"},"source":"We still have: ROUGE (Recall-Oriented Understudy for Gisting Evaluation), TER (Translation Error Rate):","block_group":"030117e6-7d4e-4852-be72-d49cfc6e3f47"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"0ad27d62-f36b-42a1-b227-fc29f268973d","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## b) Translating from Estonian","block_group":"0ad27d62-f36b-42a1-b227-fc29f268973d"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"35efb91a-3be3-4ff9-b538-586f0c03ec55","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"The technical reason why the translation results in a male or female pronoun is that it is related to an overrepresentation of one gender in the respective professional role. E.g. the role of a doctor is probably more often associated with male pronouns such as \"he/him\" in the data that the model builds on, and similar is the role of a nurse more often combined with \"she/her\" in those examples. The underlying reason for this vocabulary is the real-world overrepresentation (current or historical) that one gender is more commonly associated with a certain profession.  We believe that this is a natural bias that exists in the language and it is a historical remnant, despite being based on sometimes rather outdated assumptions of gender roles, the translation itself is still a guess based on the highest probability for how the pronoun is expected to be found. At least according to the data set used. ","block_group":"35efb91a-3be3-4ff9-b538-586f0c03ec55"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"7044a848-bbac-43a4-8b83-9f7314782de4","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"If the data set is big and broad enough to represent the language as a whole and if it reflects the skewed reality that the language is part of, then the decision to predict one pronoun over the other could be seen as a more accurate and helpful way of designing the system. However, there is no way of telling what version of the pronoun should be in any particular translation unless this is somehow captured from the rest of the text. Assuming one gender over the other based on the roles alone is rather discriminating and does not reflect the progressive Europe and western world that Estonia is part of. That way it can also be viewed as a bug. ","block_group":"7044a848-bbac-43a4-8b83-9f7314782de4"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"76989f75-2a3f-46a7-903e-908eca4a3eb2","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## c) Bat VS Bat","block_group":"76989f75-2a3f-46a7-903e-908eca4a3eb2"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"f5d30168-ec38-4f95-84ff-354c8a6b61b1","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"The reason why the first two translations are correct is probably that they are more distinct and unambiguous and hence more in line with one version of the homograph \"bat\" that exists in the \"word space\" of the model. I.e. there are more examples of the word bat being translated into \"slagträ\" in the vicinity of the words like \"hits\" and \"ball\", giving it a higher probability to be correct than \"fladdermus\" in these cases. Similar is the case for the second sentence, where \"bat\" is more likely to be aiming at the animal bat in the context of eating insects. In the third sentence, however, the situation is a bit different. Here the context and adjacent word would suggest that the bat again is an animal since it \"lives\" somewhere. Nevertheless, the model seems not to be able to distinguish this as it has no sense of the actual meaning of words like \"living\". ","block_group":"f5d30168-ec38-4f95-84ff-354c8a6b61b1"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"57a91809-f6c6-4f10-8385-2b354a4e726e","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true,"underline":true},"toCodePoint":23,"fromCodePoint":13}],"deepnote_cell_type":"text-cell-p"},"source":"(Instead, it completely relies on having seen examples of similar use cases of the word \"bat\" in a sentence and knowing what the word was translated into in that specific setting.) The reason why it is a blend of the two cases of the animal and the baseball bat could be explained by the model assuming that the bat should refer to the same thing, the same version of \"bat\", as in the previous sentences in one and the same text/paragraph. Or google could have some sort of loss function that tries to interpolate a new third version of the previously seen two versions to optimize the translation. The model could use some error measures like edit distance and have concluded that this new string had the least error. Anyway, different from our model which only can translate into words from its vocabulary, this system seems to be able to create new words. Maybe as part of some adaptive learning.  But all this is speculation from our side, since we don't know exactly how the translation model of google is built. For us, it doesn't make sense to translate the homonym into some morphed version of its different meanings.    ","block_group":"57a91809-f6c6-4f10-8385-2b354a4e726e"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"c3e7a2a5-145a-4987-a5a6-da97e65f9f46","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Contrast to our model --> could only us words in the data. ","block_group":"c3e7a2a5-145a-4987-a5a6-da97e65f9f46"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"53609df4-30bb-4dcb-a81d-c3ffac91aa62","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Google's model might use some sort of deep learning where the maximized score for the model is this morphed version. ","block_group":"53609df4-30bb-4dcb-a81d-c3ffac91aa62"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"e0eb14e0-b6c0-414a-aaca-b276ed3a69d4","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Error measurement: comparing strings with \"edit distance\". ","block_group":"e0eb14e0-b6c0-414a-aaca-b276ed3a69d4"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"d4343cca-15d8-429c-8cf6-0377cbb44efe","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Unsupervised learningwe wwwwww. Pros (adaptive for new language, possible to catch up), cons(creating nonsensical words) --> reviews good in the sense to human, ","block_group":"d4343cca-15d8-429c-8cf6-0377cbb44efe"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"ebd91d98-0abc-4831-826b-d640c0f875c3","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"Kan vara interpolation: --> e.g mnist, kombinera två handskrivna variabler --> skicka in i encoder --> decoder skickar ut \"syntetisk\" siffra","block_group":"ebd91d98-0abc-4831-826b-d640c0f875c3"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=27527141-1c2d-41eb-a7d6-3699f108d4e9' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"780a9f17d38746d882af24b244924c70","deepnote_execution_queue":[]}}
{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"c75d94b36d3744ed9908b385953374a7","deepnote_cell_type":"markdown"},"source":"# Bandit Assignment\n\nThis assignment should be done in groups of 3 and consists of a number of implementation and theory problems based on the topics discussed in the lectures and the course literature (specifically, **version 5** on arXiv):\n\n[Bandits] *Aleksandrs Slivkins, [Introduction to Multi-Armed Bandits](https://arxiv.org/pdf/1904.07272v5.pdf), Found. Trends Mach. Learn. 12(1-2): 1-286 (2019)*\n\nIn the implementation problems **(1, 2, 3 and 5)**, you will implement multi-armed bandit algorithms from the [Bandits] book and use them in a provided multi-armed bandit environment. These problems will be graded based on the correctness of the code.\n\nIn the theory problems **(4 and 6)**, you will derive some properties of the algorithms. These problems will be graded based on the correctness of the arguments.\n\nYou may use the python libraries imported below (*numpy*, *scipy.stats* and *pandas*).\n\nThe assignment should be handed in as an updated notebook. The entire notebook should be run before it is handed in, so that the plots are visible. Ensure that it is completely runnable, in the case that we want to reproduce the results. ","block_group":"c75d94b36d3744ed9908b385953374a7"},{"cell_type":"markdown","metadata":{"cell_id":"41849f5923fe4cb4b0d1fa83b1ff0c3b","deepnote_cell_type":"markdown"},"source":"## Setup","block_group":"41849f5923fe4cb4b0d1fa83b1ff0c3b"},{"cell_type":"markdown","metadata":{"cell_id":"338c73dbd528473e9d79fcb103636e68","deepnote_cell_type":"markdown"},"source":"The cell below contains imports. It may not be modified!","block_group":"338c73dbd528473e9d79fcb103636e68"},{"cell_type":"code","metadata":{"cell_id":"a0968ca64bb34b41839d922f7b632817","deepnote_cell_type":"code"},"source":"# DO NOT MODIFY\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as st\n\nSEED = 150\nITERATIONS = 5\nK = 100\nT = 10000","block_group":"a0968ca64bb34b41839d922f7b632817","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"3e55c3a4fda14d68ba49866c1cf9f91e","deepnote_cell_type":"markdown"},"source":"The cell below contains the bandit environment and may not be modified!","block_group":"3e55c3a4fda14d68ba49866c1cf9f91e"},{"cell_type":"code","metadata":{"cell_id":"2d99d2a06baf49d290fafc152f59a247","deepnote_cell_type":"code"},"source":"# DO NOT MODIFY\nclass Environment:\n    def __init__(self, K=10, seed=0):\n        self.random_state = np.random.RandomState(seed=seed)\n        self.mu = st.beta.rvs(a=1, b=1, size=K, random_state=self.random_state)\n        \n    def expected_value(self, a):\n        return self.mu[a]\n        \n    def perform_action(self, a):\n        return st.bernoulli.rvs(self.mu[a], random_state=self.random_state)\n        \n    def optimal_action(self):\n        return np.argmax(self.mu)","block_group":"2d99d2a06baf49d290fafc152f59a247","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"305b6118dfaf4ae0b26511c92817c5c3","deepnote_cell_type":"markdown"},"source":"The cell below contains the bandit algorithm base class and may not be modified!","block_group":"305b6118dfaf4ae0b26511c92817c5c3"},{"cell_type":"code","metadata":{"cell_id":"425164d650a647aab65f2e441aa75d1b","deepnote_cell_type":"code"},"source":"# DO NOT MODIFY\nclass BanditAlgorithmBase:\n    def select_action(self):\n        pass\n    \n    def update(self, action, reward):\n        pass","block_group":"425164d650a647aab65f2e441aa75d1b","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"9e5bbcf41f38423997af09ed9e47d2b4","deepnote_cell_type":"markdown"},"source":"The cell below contains the bandit experiment and may not be modified!","block_group":"9e5bbcf41f38423997af09ed9e47d2b4"},{"cell_type":"code","metadata":{"cell_id":"42f4e03d3add44b2b4d3e9567a622c35","deepnote_cell_type":"code"},"source":"# DO NOT MODIFY\nclass Experiment:\n    def __init__(self, environment, bandit_algorithm):\n        self.environment = environment\n        self.bandit_algorithm = bandit_algorithm\n        \n    def run_experiment(self, T=100):\n        instant_regrets = np.zeros(T)\n        for t in range(0, T):\n            action = self.bandit_algorithm.select_action()\n            reward = self.environment.perform_action(action)\n            self.bandit_algorithm.update(action, reward)\n            \n            optimal_action = self.environment.optimal_action()\n            instant_regret = self.environment.expected_value(optimal_action) - self.environment.expected_value(action)\n            instant_regrets[t] = instant_regret\n        cumulative_regrets = np.cumsum(instant_regrets)\n        return (instant_regrets, cumulative_regrets)\n            ","block_group":"42f4e03d3add44b2b4d3e9567a622c35","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"b9e08e88f9c94a63b406b12484c99c4b","deepnote_cell_type":"markdown"},"source":"The cell below contains a function for repeated experiments with a provided bandit algorithm, averaging regret over the runs. It may not be modified!","block_group":"b9e08e88f9c94a63b406b12484c99c4b"},{"cell_type":"code","metadata":{"cell_id":"8f15264deba54eec9f90ab6912877905","deepnote_cell_type":"code"},"source":"# DO NOT MODIFY\ndef run_repeated_experiments(bandit_algorithm_class, seed):\n    instant_regrets = []\n    cumulative_regrets = []\n    for i in range(ITERATIONS):\n        bandit_algorithm = bandit_algorithm_class(T, K)\n        environment = Environment(K, seed+i+1)\n        experiment = Experiment(environment, bandit_algorithm)\n\n        instant_regrets_i, cumulative_regrets_i = experiment.run_experiment(T)\n        instant_regrets.append(instant_regrets_i)\n        cumulative_regrets.append(cumulative_regrets_i)\n    return pd.DataFrame(data={'t': np.arange(1, T+1),\n                             'instant_regret': np.mean(np.vstack(np.array(instant_regrets)), axis=0),\n                             'regret': np.mean(np.vstack(np.array(cumulative_regrets)), axis=0)})\n","block_group":"8f15264deba54eec9f90ab6912877905","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"b76e41d47cf44e708c061cbd85eb07ac","deepnote_cell_type":"markdown"},"source":"## Stochastic Bandits (Chapter 1)","block_group":"b76e41d47cf44e708c061cbd85eb07ac"},{"cell_type":"markdown","metadata":{"cell_id":"d1dbb06e733947689c951ed105b442c3","deepnote_cell_type":"markdown"},"source":"### Problem 1 \n(3 points)\n\nImplement the *Explore-First* algorithm (**Algorithm 1.1** in [Bandits]) within the provided bandit algorithm template below. Use $N = \\left(\\frac{T}{K}\\right)^{2/3} \\cdot \\left( \\log T \\right)^{1/3}$.","block_group":"d1dbb06e733947689c951ed105b442c3"},{"cell_type":"code","metadata":{"cell_id":"61e5c21c21be4d2b89a55f081213abca","deepnote_cell_type":"code"},"source":"class ExploreFirst(BanditAlgorithmBase):\n    def __init__(self, T, K):\n        \"\"\"\n        Constructor of the bandit algorithm\n\n        Parameters\n        ----------\n        T : int\n            Horizon\n        K : int\n            Number of actions\n        \"\"\"\n        \n        # FILL IN CODE HERE\n        pass\n    \n    def select_action(self):\n        \"\"\"\n        Select an action which will be performed in the environment in the \n        current time step\n\n        Returns\n        -------\n        An action index (integer) in [0, K-1]\n        \"\"\"\n        \n        # FILL IN CODE HERE\n        pass\n    \n    def update(self, action, reward):\n        \"\"\"\n        Update the bandit algorithm with the reward received from the \n        environment for the action performed in the current time step\n\n        Parameters\n        ----------\n        action : int\n            An action index (integer) in [0, K-1]\n        reward : int\n            Reward (integer) in {0, 1} (Bernoulli rewards)\n\n        \"\"\"\n        \n        # FILL IN CODE HERE\n        pass","block_group":"61e5c21c21be4d2b89a55f081213abca","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"e8ae35886e114162a32da4d25977893c","deepnote_cell_type":"markdown"},"source":"Run the algorithm in the provided environment using the code below (averaging regret over 5 runs). The exploration and exploitation phases should be clearly visible in the plot.","block_group":"e8ae35886e114162a32da4d25977893c"},{"cell_type":"code","metadata":{"cell_id":"e7e4d24d6dce44ceaeafd960733648e9","deepnote_cell_type":"code"},"source":"# DO NOT MODIFY\nnp.random.seed(SEED)\nef_df = run_repeated_experiments(ExploreFirst, SEED)\nef_df.plot(x='t', y='regret', title='Explore-First')","block_group":"e7e4d24d6dce44ceaeafd960733648e9","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"2e290299365f4a2bb6aef36e5ff09ecc","deepnote_cell_type":"markdown"},"source":"### Problem 2\n(3 points) \n\nImplement the $ \\epsilon_t $-*Greedy* algorithm (**Algorithm 1.2** in [Bandits]) within the provided bandit algorithm template below. Use $\\epsilon_t = \\min \\left\\{1,\\ t^{-1/3} \\cdot (K \\log t)^{1/3}\\right\\} $.","block_group":"2e290299365f4a2bb6aef36e5ff09ecc"},{"cell_type":"code","metadata":{"cell_id":"133a5c33ad0840eda32ba61404edeb4c","deepnote_cell_type":"code"},"source":"class EpsilonTGreedy(BanditAlgorithmBase):\n    def __init__(self, T, K):\n        \"\"\"\n        Constructor of the bandit algorithm\n\n        Parameters\n        ----------\n        T : int\n            Horizon\n        K : int\n            Number of actions\n        \"\"\"\n        \n        # FILL IN CODE HERE\n        pass\n    \n    def select_action(self):\n        \"\"\"\n        Select an action which will be performed in the environment in the \n        current time step\n\n        Returns\n        -------\n        An action index (integer) in [0, K-1]\n        \"\"\"\n        \n        # FILL IN CODE HERE\n        pass\n    \n    def update(self, action, reward):\n        \"\"\"\n        Update the bandit algorithm with the reward received from the \n        environment for the action performed in the current time step\n\n        Parameters\n        ----------\n        action : int\n            An action index (integer) in [0, K-1]\n        reward : int\n            Reward (integer) in {0, 1} (Bernoulli rewards)\n\n        \"\"\"\n        \n        # FILL IN CODE HERE\n        pass","block_group":"133a5c33ad0840eda32ba61404edeb4c","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"16425345031142ab954585b057901fe2","deepnote_cell_type":"markdown"},"source":"Run the algorithm in the provided environment using the code below (averaging regret over 5 runs). The plot should show sublinear regret with respect to $t$.","block_group":"16425345031142ab954585b057901fe2"},{"cell_type":"code","metadata":{"cell_id":"24a4955ef03e43a69d8b4d32cf0b6728","deepnote_cell_type":"code"},"source":"# DO NOT MODIFY\nnp.random.seed(SEED)\neg_df = run_repeated_experiments(EpsilonTGreedy, SEED)\neg_df.plot(x='t', y='regret', title='Epsilon_t-Greedy')","block_group":"24a4955ef03e43a69d8b4d32cf0b6728","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"358c8c826e1d4adeaf49abb69dfb89b4","deepnote_cell_type":"markdown"},"source":"### Problem 3\n(3 points) \n\nImplement the UCB1 algorithm (**Algorithm 1.5** in [Bandits]) within the provided bandit algorithm template below.","block_group":"358c8c826e1d4adeaf49abb69dfb89b4"},{"cell_type":"code","metadata":{"cell_id":"0f33f7d5f1224bc5b7d64d766d2e5657","deepnote_cell_type":"code"},"source":"class UCB1(BanditAlgorithmBase):\n    def __init__(self, T, K):\n        \"\"\"\n        Constructor of the bandit algorithm\n\n        Parameters\n        ----------\n        T : int\n            Horizon\n        K : int\n            Number of actions\n        \"\"\"\n        \n        # FILL IN CODE HERE\n        pass\n    \n    def select_action(self):\n        \"\"\"\n        Select an action which will be performed in the environment in the \n        current time step\n\n        Returns\n        -------\n        An action index (integer) in [0, K-1]\n        \"\"\"\n        \n        # FILL IN CODE HERE\n        pass\n    \n    def update(self, action, reward):\n        \"\"\"\n        Update the bandit algorithm with the reward received from the \n        environment for the action performed in the current time step\n\n        Parameters\n        ----------\n        action : int\n            An action index (integer) in [0, K-1]\n        reward : int\n            Reward (integer) in {0, 1} (Bernoulli rewards)\n\n        \"\"\"\n        \n        # FILL IN CODE HERE\n        pass","block_group":"0f33f7d5f1224bc5b7d64d766d2e5657","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"04266480a85d487e9dc4eed66a3a6e4c","deepnote_cell_type":"markdown"},"source":"Run the algorithm in the provided environment using the code below (averaging regret over 5 runs). The plot should show sublinear regret with respect to $t$.","block_group":"04266480a85d487e9dc4eed66a3a6e4c"},{"cell_type":"code","metadata":{"cell_id":"83da3b50c4ad45d0be586200c18f3f17","deepnote_cell_type":"code"},"source":"# DO NOT MODIFY\nnp.random.seed(SEED)\nucb1_df = run_repeated_experiments(UCB1, SEED)\nucb1_df.plot(x='t', y='regret', title='UCB1')","block_group":"83da3b50c4ad45d0be586200c18f3f17","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"8073df14680647daa600a2b9a7381461","deepnote_cell_type":"markdown"},"source":"### Problem 4\n(6 points) \n\nThis theory problem is based on **Exercise 1.1** in [Bandits]. The proofs in **Chapter 1** consider environments where the rewards are in the interval $[0,1]$. Consider the case when we have additional knowledge about about the problem and that we know that the rewards for each action are in the interval $\\left[\\frac{1}{2}, \\frac{1}{2} + \\epsilon\\right]$ for some fixed $\\epsilon \\in \\left(0, \\frac{1}{2}\\right)$. \n\nConsider a version of $\\text{UCB1}$ modified to utilize this knowledge (you do not need to specify the algorithm completely, just define the new confidence radius $r_t(a)$). For this algorithm and problem setting, prove that:\n\n$\\mathbb{E}\\left[R(t)\\right] \\leq \\frac{2 t}{T^2} + 2 \\epsilon \\sqrt{2 K t \\log T}$\n\n**Instructions:** Use a version of Hoeffding Inequality with ranges (**Theorem A.2** in the [Bandits] book) to modify the confidence radius $r_t(a)$. Subsequently follow the steps of the analysis leading up to **Theorem 1.14** in [Bandits] to derive the regret bound, though show the actual constants instead of using big O notation:\n\n1. Define the clean event, like in **Section 1.3.1**, and bound the probability of the event.\n2. Start with the definition of the regret $\\mathbb{E}\\left[R(t)\\right]$, and perform a regret decomposition like on **Page 11** of **Section 1.3.2**.\n3. Bound the *gap* $\\Delta (a)$, like in **Section 1.3.3**.\n4. Complete the proof using the technique on **Page 12** of **Section 1.3.2**.\n\n*Write the solution in the Markdown cell below (use LaTeX-math mode for equations, etc.).*","block_group":"8073df14680647daa600a2b9a7381461"},{"cell_type":"markdown","metadata":{"cell_id":"c7d64524c7eb49569a76a9efa071b4f3","deepnote_cell_type":"markdown"},"source":"#### Solution","block_group":"c7d64524c7eb49569a76a9efa071b4f3"},{"cell_type":"markdown","metadata":{"cell_id":"5a53f635456d494a8e9bd6b7342efdd6","deepnote_cell_type":"markdown"},"source":"WRITE SOLUTION HERE","block_group":"5a53f635456d494a8e9bd6b7342efdd6"},{"cell_type":"markdown","metadata":{"cell_id":"a11b542910694f3b935ecd3a52f3f82c","deepnote_cell_type":"markdown"},"source":"## Bayesian Bandits (Chapter 3)","block_group":"a11b542910694f3b935ecd3a52f3f82c"},{"cell_type":"markdown","metadata":{"cell_id":"9b01503cc95a428ab71c6b7394f627bf","deepnote_cell_type":"markdown"},"source":"### Problem 5\n(3 points)\n\nImplement the *Thompson Sampling* algorithm (**Algorithm 3.3** in [Bandits]) within the provided bandit algorithm template below. Assume independent priors and that the prior is $\\mathbb{P} = \\text{Beta}(\\alpha_0, \\beta_0)$ with $\\alpha_0 = 1$ and $\\beta_0 = 1$ (i.e. the **Beta-Bernoulli** setting, on **page 35** in [Bandits]).\n\n**Note:** There is a typo in the expression for the posterior $\\mathbb{P}_H$ in [Bandits]. It should be $\\text{Beta}(\\alpha_0 + \\text{REW}_H,\\ \\beta_0 + t - \\text{REW}_H)$.","block_group":"9b01503cc95a428ab71c6b7394f627bf"},{"cell_type":"code","metadata":{"cell_id":"2b30b6cb5fd24dd4867fab34abea9baf","deepnote_cell_type":"code"},"source":"class ThompsonSampling(BanditAlgorithmBase):\n    def __init__(self, T, K):\n        \"\"\"\n        Constructor of the bandit algorithm\n\n        Parameters\n        ----------\n        T : int\n            Horizon\n        K : int\n            Number of actions\n        \"\"\"\n        \n        # FILL IN CODE HERE\n        pass\n    \n    def select_action(self):\n        \"\"\"\n        Select an action which will be performed in the environment in the \n        current time step\n\n        Returns\n        -------\n        An action index (integer) in [0, K-1]\n        \"\"\"\n        \n        # FILL IN CODE HERE\n        pass\n    \n    def update(self, action, reward):\n        \"\"\"\n        Update the bandit algorithm with the reward received from the \n        environment for the action performed in the current time step\n\n        Parameters\n        ----------\n        action : int\n            An action index (integer) in [0, K-1]\n        reward : int\n            Reward (integer) in {0, 1} (Bernoulli rewards)\n\n        \"\"\"\n        \n        # FILL IN CODE HERE\n        pass","block_group":"2b30b6cb5fd24dd4867fab34abea9baf","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"07cd3182cfb6450bb00622c30a59c3ac","deepnote_cell_type":"markdown"},"source":"Run the algorithm in the provided environment using the code below (averaging regret over 5 runs). The plot should show sublinear regret with respect to $t$.","block_group":"07cd3182cfb6450bb00622c30a59c3ac"},{"cell_type":"code","metadata":{"cell_id":"84e92f8550634a72b6842eda4438ff36","deepnote_cell_type":"code"},"source":"# DO NOT MODIFY\nnp.random.seed(SEED)\nts_df = run_repeated_experiments(ThompsonSampling, SEED)\nts_df.plot(x='t', y='regret', title='Thompson Sampling')","block_group":"84e92f8550634a72b6842eda4438ff36","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"6a19bfa4cd9a440ca4f91262c6a98261","deepnote_cell_type":"markdown"},"source":"### Problem 6\n(6 points)\n\nIn this theory problem, you will show an intermediary step in the proof for the Bayesian regret bound of *Thompson Sampling* in the [Bandits] book.\n\nYou are given a $K$-armed bandit problem with rewards in the interval $[0, 1]$. You can assume that $K \\leq T$, where $T$ is the horizon. Additionally, you can assume that **Lemma 1.5** holds (i.e., for this assignment we define $r_t (a) := \\sqrt{\\frac{2  \\log T}{ n_t (a)}}$, and then it holds that $\\text{Pr}\\left\\{ \\mathcal{E} \\right\\} \\geq 1 - \\frac{2}{T^2}$ with $\\mathcal{E} := \\left\\{ \\forall a \\forall t \\;\\; \\vert \\bar{\\mu}_t (a) - \\mu (a) \\vert \\leq r_t (a) \\right\\}$). Then, with $\\text{UCB}_t (a) := \\bar{\\mu}_t (a) + r_t (a)$, show that $\\mathbb{E}\\left[ \\left[ \\text{UCB}_t (a) - \\mu (a) \\right]^{-} \\right] \\leq \\frac{2}{TK}$ (i.e., show that Equation 3.14 in [Bandits], with $\\gamma = 2$, holds for all arms $a$ and rounds $t$).\n\n**Note:** $[x]^{-}$ is the negative portion of $x$, i.e., $[x]^{-} = 0$ if $x \\geq 0$ and $[x]^{-} = \\vert x \\vert$ otherwise.\n\n**Hint:** Remember that, given a random variable $X$, an event $\\mathcal{E}$ (subset of the sample space) and its complement $\\mathcal{E}^c$, by the tower rule, $\\mathbb{E}\\left[ X \\right] = \\mathbb{E}\\left[ X \\;\\vert\\; \\mathcal{E} \\right] \\cdot \\text{Pr}\\left\\{ \\mathcal{E} \\right\\} + \\mathbb{E}\\left[ X \\;\\vert\\; \\mathcal{E}^c \\right] \\cdot \\text{Pr}\\left\\{ \\mathcal{E}^c \\right\\}$.\n\n*Write the solution in the Markdown cell below (use LaTeX-math mode for equations, etc.).*","block_group":"6a19bfa4cd9a440ca4f91262c6a98261"},{"cell_type":"markdown","metadata":{"cell_id":"dde8487c34994f80929a61787f0fb78f","deepnote_cell_type":"markdown"},"source":"#### Solution","block_group":"dde8487c34994f80929a61787f0fb78f"},{"cell_type":"markdown","metadata":{"cell_id":"8e1ca1e2c9c44d3b976aa24934961760","deepnote_cell_type":"markdown"},"source":"WRITE SOLUTION HERE","block_group":"8e1ca1e2c9c44d3b976aa24934961760"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=15c42e9f-e6bb-4427-b594-fb21b448c014' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3 (ipykernel)"},"language_info":{"name":"python","version":"3.9.12","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"deepnote_notebook_id":"ffe2d3d7c42743019d2f1176c2a95bdc","deepnote_execution_queue":[]}}